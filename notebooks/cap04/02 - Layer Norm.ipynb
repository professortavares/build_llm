{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8df91d-17f2-4d52-bf62-4519928f70f0",
   "metadata": {},
   "source": [
    "# Layer Norm\n",
    "\n",
    "**Descrição**  \n",
    "Layer Normalization (LayerNorm) é uma técnica de normalização que ajusta as ativações de uma rede para ficarem mais estáveis durante o treinamento. Em vez de depender do tamanho do batch (como no BatchNorm), o LayerNorm normaliza **por amostra** (e, em Transformers, tipicamente **por token**), ao longo da dimensão do embedding.\n",
    "\n",
    "**Objetivo**  \n",
    "- Aumentar a **estabilidade** do treinamento (reduzindo problemas de gradiente explodindo/sumindo).  \n",
    "- Ajudar redes profundas (ex.: **Transformers**) a aprenderem com mais consistência.  \n",
    "- Manter as ativações em uma escala mais controlada, facilitando a otimização.\n",
    "\n",
    "**Funcionamento**  \n",
    "Dado um tensor `x` com shape `(..., emb_dim)`, o LayerNorm normaliza os valores ao longo da última dimensão (`emb_dim`):\n",
    "\n",
    "1. Calcula a média `μ` e variância `σ²` ao longo de `emb_dim`  \n",
    "2. Normaliza:\n",
    "  <img src=\"https://latex.codecogs.com/svg.image?\\text{norm}(x)=\\frac{x-\\mu}{\\sqrt{\\sigma^2&plus;\\epsilon}}\" />\n",
    "2. Aplica uma transformação afim aprendível:\n",
    "<img src=\"https://latex.codecogs.com/svg.image?y=\\gamma\\cdot\\text{norm}(x)&plus;\\beta&space;\" />\n",
    "\n",
    "Onde:\n",
    "- `γ` (`scale`) e `β` (`shift`) são parâmetros treináveis com shape `(emb_dim,)`\n",
    "- `ε` é um valor pequeno para evitar divisão por zero\n",
    "\n",
    "Em um tensor `(batch, seq_len, emb_dim)`, isso significa que **cada token** (cada posição em `seq_len`) é normalizado de forma independente, usando apenas seus próprios valores do embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58424dcc-57e3-4f78-9e30-f30ba5e77338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fbcab-c0d6-4e56-903f-c76ac7b8649d",
   "metadata": {},
   "source": [
    "## Pra que serve o `LayerNorm` (Layer Normalization)?\n",
    "\n",
    "O **LayerNorm** é uma técnica de normalização usada para **estabilizar e acelerar o treinamento** de redes profundas (muito comum em **Transformers/LLMs**).\n",
    "\n",
    "### O que ele faz?\n",
    "\n",
    "Para cada vetor de embedding (por exemplo, **cada token** em uma sequência), ele:\n",
    "\n",
    "1. **Calcula a média** dos valores na dimensão do embedding  \n",
    "2. **Calcula a variância** nessa mesma dimensão  \n",
    "3. **Normaliza** os valores para ficarem com média ~0 e variância ~1  \n",
    "4. Aplica uma transformação aprendível:\n",
    "   - `scale` (γ): ajusta a escala final\n",
    "   - `shift` (β): ajusta o deslocamento final\n",
    "\n",
    "A ideia é:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.image?\\[\\text{norm}(x)=\\frac{x-\\mu}{\\sqrt{\\sigma^2&plus;\\epsilon}},\\quad&space;y=\\gamma\\cdot\\text{norm}(x)&plus;\\beta\\]\" />\n",
    "\n",
    "\n",
    "### Por que isso ajuda?\n",
    "\n",
    "- **Deixa o treino mais estável**: evita ativações com escalas muito diferentes entre camadas.\n",
    "- **Melhora o fluxo de gradiente**: ajuda a treinar arquiteturas profundas com menos problemas de gradiente explodindo/sumindo.\n",
    "- **Não depende do batch**: diferente de BatchNorm, o LayerNorm normaliza **por amostra/token**, então funciona bem com batch pequeno e com sequências variáveis.\n",
    "\n",
    "### Qual dimensão é normalizada no seu código?\n",
    "\n",
    "No seu `forward`:\n",
    "\n",
    "```python\n",
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "````\n",
    "\n",
    "Isso normaliza **ao longo da última dimensão** (`dim=-1`), que normalmente é `emb_dim`.\n",
    "\n",
    "Se `x` tem shape `(batch, seq_len, emb_dim)`, então:\n",
    "\n",
    "* para cada **exemplo do batch**\n",
    "* para cada **posição (token) em `seq_len`**\n",
    "* normaliza os **valores do embedding** daquele token\n",
    "\n",
    "### Resumo\n",
    "\n",
    "**LayerNorm serve para manter as ativações em uma escala mais controlada por token, deixando o treinamento de Transformers mais estável e eficiente.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c31175-c67c-4e5f-8144-5999659e5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação de Layer Normalization (normalização por token, ao longo da última dimensão).\n",
    "\n",
    "    Esta camada normaliza as ativações ao longo da dimensão de embedding (dim=-1),\n",
    "    e então aplica uma transformação afim aprendível:\n",
    "\n",
    "        y = scale * (x - mean) / sqrt(var + eps) + shift\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    emb_dim : int\n",
    "        Dimensão do embedding (tamanho do último eixo de `x`).\n",
    "    eps : float, default = 1e-5\n",
    "        Constante pequena para evitar divisão por zero.\n",
    "\n",
    "    Entrada:\n",
    "    -------\n",
    "    x : torch.Tensor\n",
    "        Tensor com shape (..., emb_dim). Exemplos comuns:\n",
    "        - (batch, seq_len, emb_dim)\n",
    "        - (seq_len, emb_dim)\n",
    "\n",
    "    Saída:\n",
    "    -----\n",
    "    torch.Tensor\n",
    "        Tensor normalizado com o mesmo shape de `x`.\n",
    "\n",
    "    Observações:\n",
    "    -----------\n",
    "    - `unbiased=False` em `var` corresponde ao comportamento típico em LN.\n",
    "    - `scale` e `shift` têm shape (emb_dim,) e são broadcastados para o shape de `x`.\n",
    "\n",
    "    Referência:\n",
    "    ----------\n",
    "    Seção 4.2 — \"Normalizing activations with layer normalization\". :contentReference[oaicite:0]{index=0}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim: int, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if not isinstance(emb_dim, int) or emb_dim <= 0:\n",
    "            raise ValueError(\"emb_dim deve ser um int positivo.\")\n",
    "\n",
    "        self.eps = float(eps)\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Executa a normalização por camada (LayerNorm) ao longo do último eixo.\n",
    "\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Tensor de entrada com shape (..., emb_dim).\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Tensor normalizado com o mesmo shape de `x`.\n",
    "        \"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838db629-a2ca-453b-ad44-a4ec6b37d29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada (batch_example):\n",
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n",
      "\n",
      "Média por linha (antes): tensor([-0.3596, -0.2606])\n",
      "Desvio padrão por linha (antes): tensor([0.4489, 0.5170])\n",
      "\n",
      "Saída após LayerNorm:\n",
      "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Média por linha (depois): tensor([-2.9802e-08,  0.0000e+00], grad_fn=<MeanBackward1>)\n",
      "Desvio padrão por linha (depois): tensor([1.0000, 1.0000], grad_fn=<StdBackward0>)\n",
      "\n",
      "Checagem (aprox.):\n",
      "Max |mean - 0|: 2.9802322387695312e-08\n",
      "Max |std  - 1|: 2.47955322265625e-05\n",
      "Média ~ 0 ? True\n",
      "Std ~ 1   ? True\n"
     ]
    }
   ],
   "source": [
    "def media_desvio_por_linha(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Calcula média e desvio padrão ao longo da última dimensão (dim=-1),\n",
    "    retornando 1 valor por linha (ou por token, se for o caso).\n",
    "    \"\"\"\n",
    "    mean = x.mean(dim=-1)\n",
    "    std = x.std(dim=-1, unbiased=False)  # condizente com var(..., unbiased=False)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Exemplo de uso\n",
    "# ------------------------------------------------------------\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "mean_before, std_before = media_desvio_por_linha(batch_example)\n",
    "\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out = ln(batch_example)\n",
    "\n",
    "mean_after, std_after = media_desvio_por_linha(out)\n",
    "\n",
    "print(\"Entrada (batch_example):\")\n",
    "print(batch_example)\n",
    "print(\"\\nMédia por linha (antes):\", mean_before)\n",
    "print(\"Desvio padrão por linha (antes):\", std_before)\n",
    "\n",
    "print(\"\\nSaída após LayerNorm:\")\n",
    "print(out)\n",
    "print(\"\\nMédia por linha (depois):\", mean_after)\n",
    "print(\"Desvio padrão por linha (depois):\", std_after)\n",
    "\n",
    "print(\"\\nChecagem (aprox.):\")\n",
    "# erro absoluto máximo da média e do std em relação a (0 e 1)\n",
    "max_mean_err = (mean_after).abs().max().item()\n",
    "max_std_err = (std_after - 1).abs().max().item()\n",
    "\n",
    "print(\"Max |mean - 0|:\", max_mean_err)\n",
    "print(\"Max |std  - 1|:\", max_std_err)\n",
    "\n",
    "print(\n",
    "    \"Média ~ 0 ?\", torch.allclose(mean_after, torch.zeros_like(mean_after), atol=1e-6)\n",
    ")\n",
    "print(\n",
    "    \"Std ~ 1   ?\", torch.allclose(std_after, torch.ones_like(std_after), atol=1e-4)\n",
    ")  # atol mais realista"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
