{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85be0de-5220-4f8c-a0c4-f2027f45f7cd",
   "metadata": {},
   "source": [
    "# Bloco transformer\n",
    "\n",
    "**Descrição:**\n",
    "Implementação do *Transformer Block* (estilo GPT) — o principal “tijolo” arquitetural de modelos *decoder-only*.  \n",
    "Ele combina **atenção multi-cabeças com máscara causal**, **rede feed-forward (MLP)**, **normalização** e **conexões residuais**, preservando o formato do tensor ao longo do bloco (mesmo `batch`, `seq_len` e `emb_dim`).\n",
    "\n",
    "**Objetivo:**\n",
    "Receber uma sequência de embeddings e **produzir embeddings contextualizados**, isto é, representações em que cada token passa a carregar informação relevante dos tokens anteriores (dependências de longo alcance), ao mesmo tempo em que refina a informação local via MLP. \n",
    "\n",
    "**Funcionamento:**\n",
    "1. **Pré-normalização (Pre-LN) + Atenção Causal Multi-Head**  \n",
    "   - Aplica *LayerNorm* na entrada do bloco.  \n",
    "   - Calcula `Q, K, V`, aplica **máscara causal** (impede “olhar o futuro”), obtém pesos de atenção e combina os valores.  \n",
    "   - Aplica *dropout* e projeta a saída.\n",
    "2. **Atalho residual (skip connection)**  \n",
    "   - Soma a saída da atenção com a entrada original do bloco.\n",
    "3. **Pré-normalização (Pre-LN) + Feed-Forward (MLP)**  \n",
    "   - Aplica *LayerNorm* novamente.  \n",
    "   - Passa por um MLP (tipicamente `Linear -> GELU -> Linear`) para transformar cada posição de forma independente.\n",
    "4. **Atalho residual final**  \n",
    "   - Soma a saída do MLP com o tensor após o primeiro residual.  \n",
    "\n",
    "Resultado: um bloco que **mantém a mesma dimensionalidade de entrada e saída**, mas “re-encoda” cada token com contexto + não-linearidades, pronto para ser empilhado várias vezes numa arquitetura GPT. \n",
    "\n",
    "![Bloco transformer](../../imagens/cap04/04_bloco_transformer.png)\n",
    "\n",
    "*Baseado na figura do livro, Capítulo 4, página 114*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9698b6c-1cb8-4389-887a-95b51970f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from build_llm.activation import GELU\n",
    "from build_llm.attention import MultiHeadAttention\n",
    "from build_llm.layer import LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ef8ab-ccf7-4f9b-9f12-d281ee9550cc",
   "metadata": {},
   "source": [
    "### Verificando a presença da GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d363bfbf-def5-472d-a6c6-4a9f4e051fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cpu\n",
      "False\n",
      "Usando: cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # Versão do torch\n",
    "print(torch.cuda.is_available())  # Verificação de GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e636c-11b6-41f6-a8f6-d436edff8382",
   "metadata": {},
   "source": [
    "## Configuração do modelo\n",
    "\n",
    "As mesmas utilizadas no notebook [01 - LLM architecture](./01%20-%20LLM%20architecture.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41fe282e-652e-471e-9ba3-74495786e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50257,  # Tamanho do vocabulário\n",
    "    \"context_length\": 1024,  # Comprimento do contexto\n",
    "    \"emb_dim\": 768,  # Dimensão do embedding\n",
    "    \"n_heads\": 12,  # Número de cabeças de atenção\n",
    "    \"n_layers\": 12,  # Número de camadas\n",
    "    \"drop_rate\": 0.1,  # Taxa de dropout\n",
    "    \"qkv_bias\": False,  # Viés em Query-Key-Value (QKV)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edd82b-dd58-4c65-a28d-ca8eb2ed9dd4",
   "metadata": {},
   "source": [
    "## Classe `FeedForward` (MLP do Transformer)\n",
    "\n",
    "A classe **`FeedForward`** implementa a **rede feed-forward** (também chamada de **MLP** ou *position-wise feed-forward network*) que aparece dentro de cada **bloco Transformer** (no estilo GPT).\n",
    "\n",
    "Ela é composta por **duas camadas lineares** com uma **ativação não-linear (GELU)** no meio:\n",
    "\n",
    "- **Expansão**: `emb_dim -> 4 * emb_dim`  \n",
    "- **Não-linearidade**: `GELU()`  \n",
    "- **Projeção de volta**: `4 * emb_dim -> emb_dim`\n",
    "\n",
    "### Para que ela serve?\n",
    "\n",
    "Dentro do Transformer, existem dois “tipos” principais de processamento:\n",
    "\n",
    "1. **Self-Attention (atenção)**: permite que cada token “olhe” para outros tokens e misture informações do contexto.\n",
    "2. **FeedForward / MLP (esta classe)**: aplica uma transformação **não-linear** em **cada token individualmente**, de forma independente (posição a posição).\n",
    "\n",
    "Ou seja, depois que a atenção combinou informações entre tokens, o `FeedForward` atua como um “processador” por token, ajudando o modelo a:\n",
    "\n",
    "- aumentar a **capacidade de representação** (aprendendo combinações mais complexas),\n",
    "- introduzir **não-linearidades** (o que torna o modelo muito mais expressivo),\n",
    "- refinar e re-encodar os vetores de embedding mantendo o mesmo tamanho final (`emb_dim`).\n",
    "\n",
    "### Por que usar `4 * emb_dim`?\n",
    "\n",
    "O fator `4` é uma convenção comum em arquiteturas do tipo GPT/Transformer:\n",
    "\n",
    "- ao **expandir** a dimensão interna, o modelo ganha um “espaço” maior para computações intermediárias;\n",
    "- ao **reduzir** de volta para `emb_dim`, o bloco mantém compatibilidade com o residual e com o restante da rede.\n",
    "\n",
    "### Entrada e saída\n",
    "\n",
    "- **Entrada**: tensor com shape `(..., emb_dim)` (por exemplo, `[batch, tokens, emb_dim]`)\n",
    "- **Saída**: tensor com o **mesmo shape** da entrada `(..., emb_dim)`\n",
    "\n",
    "Isso é importante porque o `FeedForward` normalmente é usado em conjunto com **conexões residuais** no `TransformerBlock`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184e71c1-7895-49e2-b1c5-9303a70196cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Retorna o device apropriado (CUDA se disponível, caso contrário CPU).\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c437f157-7dd8-40ea-95f3-21124112fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Rede feed-forward (MLP) usada dentro de um bloco Transformer (estilo GPT).\n",
    "\n",
    "    Estrutura:\n",
    "    ----------\n",
    "    Linear(emb_dim -> 4*emb_dim) -> GELU -> Linear(4*emb_dim -> emb_dim)\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    cfg : dict\n",
    "        Dicionário de configuração contendo:\n",
    "        - \"emb_dim\" (int): dimensão do embedding / hidden size do modelo.\n",
    "\n",
    "    Exceções:\n",
    "    --------\n",
    "    Levanta KeyError se \"emb_dim\" não existir em cfg.\n",
    "    Levanta TypeError / ValueError se \"emb_dim\" não puder ser convertido para int\n",
    "    ou não for um inteiro positivo.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: dict) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if \"emb_dim\" not in cfg:\n",
    "            raise KeyError('cfg deve conter a chave \"emb_dim\".')\n",
    "\n",
    "        try:\n",
    "            emb_dim = int(cfg[\"emb_dim\"])\n",
    "        except (TypeError, ValueError) as e:\n",
    "            raise TypeError(\n",
    "                '\"emb_dim\" deve ser um inteiro (ou conversível para int).'\n",
    "            ) from e\n",
    "\n",
    "        if emb_dim <= 0:\n",
    "            raise ValueError('\"emb_dim\" deve ser um inteiro positivo.')\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Executa o forward pass do feed-forward.\n",
    "\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Tensor de entrada com shape (..., emb_dim).\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Tensor de saída com o mesmo shape de entrada (..., emb_dim).\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc39b7f-f4d3-4152-9c9c-5af78efb2eb9",
   "metadata": {},
   "source": [
    "## Classe `TransformerBlock` (Bloco principal do Transformer estilo GPT)\n",
    "\n",
    "A classe **`TransformerBlock`** implementa um **bloco Transformer completo**, que é a unidade básica repetida várias vezes para formar um modelo do tipo GPT.\n",
    "\n",
    "Ela combina quatro ideias fundamentais:\n",
    "\n",
    "1. **Multi-Head Self-Attention (MHA)**  \n",
    "   Permite que cada token “preste atenção” em outros tokens da sequência, misturando informações do contexto.\n",
    "\n",
    "2. **FeedForward / MLP (FFN)**  \n",
    "   Processa cada token individualmente com camadas densas e não-linearidade, aumentando a capacidade do modelo.\n",
    "\n",
    "3. **Pre-LayerNorm (LayerNorm antes de cada sub-bloco)**  \n",
    "   Normaliza a entrada de cada sub-bloco (atenção e MLP), ajudando a estabilizar o treinamento (muito comum em modelos GPT modernos).\n",
    "\n",
    "4. **Conexões residuais + Dropout**  \n",
    "   Usa *skip connections* (atalhos) para facilitar o fluxo de gradientes e aplica dropout para regularização.\n",
    "\n",
    "### Fluxo interno do bloco\n",
    "\n",
    "O bloco é composto por **dois sub-blocos**, cada um com:\n",
    "- normalização,\n",
    "- operação principal,\n",
    "- dropout,\n",
    "- soma residual.\n",
    "\n",
    "#### 1) Sub-bloco de Atenção\n",
    "```\n",
    "shortcut = x\n",
    "x = LayerNorm(x)\n",
    "x = MultiHeadAttention(x)\n",
    "x = Dropout(x)\n",
    "x = x + shortcut\n",
    "```\n",
    "\n",
    "Aqui, a atenção mistura informação entre tokens, e o residual garante que o sinal original possa “passar direto” caso a atenção não precise alterá-lo muito.\n",
    "\n",
    "#### 2) Sub-bloco FeedForward (MLP)\n",
    "```\n",
    "shortcut = x\n",
    "x = LayerNorm(x)\n",
    "x = FeedForward(x)\n",
    "x = Dropout(x)\n",
    "x = x + shortcut\n",
    "```\n",
    "\n",
    "Aqui, o MLP refina a representação **token a token** (posição a posição), adicionando expressividade e não-linearidade.\n",
    "\n",
    "### Para que serve esse bloco?\n",
    "\n",
    "O `TransformerBlock` é responsável por transformar embeddings iniciais em representações cada vez mais ricas, combinando:\n",
    "\n",
    "- **contexto** (via self-attention),\n",
    "- **capacidade não-linear** (via MLP),\n",
    "- **estabilidade e eficiência no treinamento** (via LayerNorm + residual).\n",
    "\n",
    "Empilhando vários `TransformerBlock`s, o modelo vai construindo camadas de entendimento:  \n",
    "camadas mais baixas capturam padrões locais/sintáticos, enquanto camadas mais altas tendem a capturar relações mais abstratas (dependendo do tamanho/dados do modelo).\n",
    "\n",
    "### Entrada e saída\n",
    "\n",
    "- **Entrada**: `x` com shape **`[batch_size, num_tokens, emb_dim]`**\n",
    "- **Saída**: mesmo shape **`[batch_size, num_tokens, emb_dim]`**\n",
    "\n",
    "Isso é essencial para:\n",
    "- empilhar vários blocos em sequência,\n",
    "- manter compatibilidade com conexões residuais,\n",
    "- manter a mesma “largura” do modelo ao longo das camadas.\n",
    "\n",
    "### Papel dos principais parâmetros (`cfg`)\n",
    "\n",
    "- **`emb_dim`**: tamanho do vetor de embedding (largura do modelo)\n",
    "- **`context_length`**: limite máximo de tokens (tamanho do contexto)\n",
    "- **`n_heads`**: número de cabeças de atenção (divide `emb_dim` em partes)\n",
    "- **`drop_rate`**: intensidade do dropout (regularização)\n",
    "- **`qkv_bias`**: se a atenção usa bias nas projeções de Q/K/V\n",
    "\n",
    "> Observação: o código valida que `emb_dim % n_heads == 0` para garantir que cada head tenha a mesma dimensão interna.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9959a440-feab-4e73-a7b0-58847ee267b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco Transformer (estilo GPT) com:\n",
    "    - Multi-Head Self-Attention\n",
    "    - MLP / FeedForward\n",
    "    - Pre-LayerNorm (LayerNorm antes de cada sub-bloco)\n",
    "    - Conexões residuais (shortcut) e dropout nas saídas dos sub-blocos\n",
    "\n",
    "    Fluxo (simplificado):\n",
    "    ---------------------\n",
    "    x -> LN -> MHA -> Dropout -> +residual\n",
    "      -> LN -> FFN -> Dropout -> +residual\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    cfg : dict\n",
    "        Dicionário de configuração contendo:\n",
    "        - \"emb_dim\" (int): dimensão do embedding (d_model).\n",
    "        - \"context_length\" (int): tamanho máximo de contexto (n_tokens).\n",
    "        - \"n_heads\" (int): número de cabeças de atenção.\n",
    "        - \"drop_rate\" (float): taxa de dropout (0.0 a 1.0).\n",
    "        - \"qkv_bias\" (bool): se deve usar bias em QKV no attention.\n",
    "\n",
    "    Exceções:\n",
    "    --------\n",
    "    Levanta KeyError se alguma chave obrigatória não existir em cfg.\n",
    "    Levanta TypeError/ValueError se algum valor não puder ser convertido\n",
    "    para o tipo esperado ou se estiver fora de um intervalo válido.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: dict) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        required_keys = [\n",
    "            \"emb_dim\",\n",
    "            \"context_length\",\n",
    "            \"n_heads\",\n",
    "            \"drop_rate\",\n",
    "            \"qkv_bias\",\n",
    "        ]\n",
    "        missing = [k for k in required_keys if k not in cfg]\n",
    "        if missing:\n",
    "            raise KeyError(f\"cfg está faltando as chaves obrigatórias: {missing}\")\n",
    "\n",
    "        # Validações / conversões defensivas\n",
    "        try:\n",
    "            emb_dim = int(cfg[\"emb_dim\"])\n",
    "            context_length = int(cfg[\"context_length\"])\n",
    "            n_heads = int(cfg[\"n_heads\"])\n",
    "            drop_rate = float(cfg[\"drop_rate\"])\n",
    "            qkv_bias = bool(cfg[\"qkv_bias\"])\n",
    "        except (TypeError, ValueError) as e:\n",
    "            raise TypeError(\n",
    "                \"cfg contém valores em formato inválido para o TransformerBlock.\"\n",
    "            ) from e\n",
    "\n",
    "        if emb_dim <= 0:\n",
    "            raise ValueError('\"emb_dim\" deve ser um inteiro positivo.')\n",
    "        if context_length <= 0:\n",
    "            raise ValueError('\"context_length\" deve ser um inteiro positivo.')\n",
    "        if n_heads <= 0:\n",
    "            raise ValueError('\"n_heads\" deve ser um inteiro positivo.')\n",
    "        if emb_dim % n_heads != 0:\n",
    "            raise ValueError('\"emb_dim\" deve ser divisível por \"n_heads\".')\n",
    "        if not (0.0 <= drop_rate <= 1.0):\n",
    "            raise ValueError('\"drop_rate\" deve estar entre 0.0 e 1.0.')\n",
    "\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=emb_dim,\n",
    "            d_out=emb_dim,\n",
    "            context_length=context_length,\n",
    "            num_heads=n_heads,\n",
    "            dropout=drop_rate,\n",
    "            qkv_bias=qkv_bias,\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        self.norm1 = LayerNorm(emb_dim)\n",
    "        self.norm2 = LayerNorm(emb_dim)\n",
    "\n",
    "        # Dropout aplicado na saída de cada sub-bloco antes de somar o residual\n",
    "        self.drop_shortcut = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Executa o forward pass do bloco Transformer.\n",
    "\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Tensor de entrada com shape [batch_size, num_tokens, emb_dim].\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Tensor de saída com o mesmo shape [batch_size, num_tokens, emb_dim].\n",
    "        \"\"\"\n",
    "        # Residual + Attention\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # [batch, tokens, emb_dim]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Residual + FeedForward\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)  # [batch, tokens, emb_dim]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a789becf-f1a2-4f47-895e-8f0b30606cdb",
   "metadata": {},
   "source": [
    "Esse trecho faz um **teste rápido de “forma” (shape check)** do `TransformerBlock`:\n",
    "\n",
    "* `block = TransformerBlock(GPT_CONFIG_124M)`: cria um bloco Transformer com as configurações do GPT 124M (por exemplo, `emb_dim = 768`, número de heads, dropout, etc.).\n",
    "* `x = torch.randn(2, 16, 768)`: cria uma entrada aleatória simulando **2 exemplos (batch=2)**, com **16 tokens** cada, e cada token representado por um vetor de **768** dimensões.\n",
    "* `y = block(x)`: passa `x` pelo bloco (LayerNorm + atenção + residual + LayerNorm + MLP + residual).\n",
    "\n",
    "O resultado `torch.Size([2, 16, 768]) -> torch.Size([2, 16, 768])` acontece porque o bloco **transforma as representações internamente**, mas **sempre retorna a mesma dimensão `emb_dim`** para poder empilhar vários blocos e manter as conexões residuais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4fb32d0-00aa-4c73-9ec1-74a29fb44be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: cpu\n",
      "torch.Size([2, 16, 768]) -> torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(\"Usando:\", device)\n",
    "\n",
    "# Envia o bloco para o device (CUDA se existir)\n",
    "block = TransformerBlock(GPT_CONFIG_124M).to(device)\n",
    "\n",
    "x = torch.randn(2, 16, GPT_CONFIG_124M[\"emb_dim\"])\n",
    "y = block(x)\n",
    "print(x.shape, \"->\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d38425-7be4-4573-a75e-7b38314421ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
