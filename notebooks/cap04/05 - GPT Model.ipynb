{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0e3959-e61b-4ebd-8806-c539e2ecfaba",
   "metadata": {},
   "source": [
    "# GPT Model\n",
    "\n",
    "**Descrição:**\n",
    "Notebook dedicado à implementação e exploração de um **GPT decoder-only** (`GPTModel`) em PyTorch: combina *token embeddings* e *positional embeddings*, empilha blocos Transformer com **atenção causal (masked self-attention)** e finaliza com uma projeção linear para produzir **logits sobre o vocabulário** em cada posição da sequência.\n",
    "\n",
    "**Objetivo:**\n",
    "Consolidar o entendimento prático de como um GPT gera texto via **previsão do próximo token**, validando o fluxo do `forward`, checando *shapes* dos tensores, conferindo reprodutibilidade (seed), avaliando recursos (GPU) e inspecionando métricas como **número de parâmetros treináveis**, além de executar testes simples de geração.\n",
    "\n",
    "**Funcionamento:**\n",
    "1. **Entrada**: `in_idx` com shape **(B, T)** (IDs inteiros de tokens).  \n",
    "2. **Embeddings**: `tok_emb(in_idx)` → **(B, T, C)** e `pos_emb(0..T-1)` → **(T, C)**; soma e aplica *dropout*.  \n",
    "3. **Transformer blocks (`trf_blocks`)**: processam o contexto com **atenção causal** (cada posição atende apenas `≤ t`), MLP/FFN, conexões residuais e normalização.  \n",
    "4. **Saída**: `final_norm` e `out_head` projetam para **logits** com shape **(B, T, V)** (probabilidades só após `softmax`, tipicamente na geração).\n",
    "\n",
    "\n",
    "![Bloco transformer](../../imagens/cap04/05_gpt_model.png)\n",
    "\n",
    "*Baseado na figura do livro, Capítulo 4, página 118*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5c05a2-2eec-4187-81ae-268ad26892f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from build_llm.layer import LayerNorm\n",
    "from build_llm.transformer import TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9b55a4-b618-4ce1-bc94-e83ef8fc3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824158b-c381-40a3-8bb3-2b4688aebab5",
   "metadata": {},
   "source": [
    "### Verifica a presença da GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abaa0796-7d73-40c6-9372-ab763178cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cpu\n",
      "False\n",
      "Usando: cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # Versão do torch\n",
    "print(torch.cuda.is_available())  # Verificação de GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9509da-737f-4c5a-9715-1f4e885f2952",
   "metadata": {},
   "source": [
    "## Fixando o seed para reproducibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0fb197-0340-4e43-9213-9f360d935d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Fixa seeds para reprodutibilidade em Python, NumPy e PyTorch.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Garante determinismo (pode afetar performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe7204-a6ec-4106-bc46-acec8eb0dac2",
   "metadata": {},
   "source": [
    "## Configuração do modelo\n",
    "\n",
    "As mesmas utilizadas no notebook [01 - LLM architecture](./01%20-%20LLM%20architecture.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8889e598-d537-49c6-b5d3-46010d65e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50257,  # Tamanho do vocabulário\n",
    "    \"context_length\": 1024,  # Comprimento do contexto\n",
    "    \"emb_dim\": 768,  # Dimensão do embedding\n",
    "    \"n_heads\": 12,  # Número de cabeças de atenção\n",
    "    \"n_layers\": 12,  # Número de camadas\n",
    "    \"drop_rate\": 0.1,  # Taxa de dropout\n",
    "    \"qkv_bias\": False,  # Viés em Query-Key-Value (QKV)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805fb45-e56e-46d3-80fd-241d4c684eea",
   "metadata": {},
   "source": [
    "# Como o `GPTModel` funciona (passo a passo)\n",
    "\n",
    "A classe `GPTModel` implementa um **Transformer decoder-only** (estilo GPT), treinado para **predizer o próximo token** a partir de um contexto à esquerda. Em termos práticos: para cada posição `t` da sequência, o modelo produz uma distribuição (logits) sobre o vocabulário para o “próximo token provável”. Esse é o mecanismo central do GPT descrito no capítulo de implementação do modelo (cap. 4).\n",
    "\n",
    "## Intuição: o que entra e o que sai\n",
    "\n",
    "- **Entrada:** `in_idx` com shape **(B, T)**  \n",
    "  `B = batch_size`, `T = seq_len`  \n",
    "  Cada elemento é um **ID inteiro** de token (ex.: 50256, 123, 99...).\n",
    "\n",
    "- **Saída:** `logits` com shape **(B, T, V)**  \n",
    "  `V = vocab_size`  \n",
    "  Para cada posição da sequência, temos um vetor de logits com tamanho do vocabulário (um “score” para cada token possível).\n",
    "\n",
    "## Por que embeddings?\n",
    "\n",
    "Redes neurais não operam diretamente em IDs discretos. Então o GPT faz duas codificações iniciais e as soma:\n",
    "\n",
    "### 1) `tok_emb`: Token Embedding\n",
    "Transforma IDs em vetores densos:\n",
    "- `tok_embeds = tok_emb(in_idx)`  \n",
    "- shape: **(B, T, C)**, onde `C = emb_dim`\n",
    "\n",
    "Cada token vira um vetor “semântico” treinável.\n",
    "\n",
    "### 2) `pos_emb`: Positional Embedding\n",
    "Sem posição, o modelo veria a sequência como um “saco de tokens”.\n",
    "- cria `pos_ids = [0, 1, 2, ..., T-1]`\n",
    "- `pos_embeds = pos_emb(pos_ids)`  \n",
    "- shape: **(T, C)** (depois é broadcast para (B, T, C))\n",
    "\n",
    "Cada posição ganha um vetor treinável que informa “onde” o token está.\n",
    "\n",
    "### Soma + Dropout\n",
    "- `x = tok_embeds + pos_embeds`  → **(B, T, C)**\n",
    "- `x = drop_emb(x)`\n",
    "\n",
    "A soma combina “o que é” (token) com “onde está” (posição). O dropout ajuda na regularização.\n",
    "\n",
    "## O coração do modelo: `trf_blocks` (TransformerBlock empilhados)\n",
    "\n",
    "```\n",
    "x = self.trf_blocks(x)\n",
    "```\n",
    "\n",
    "Aqui acontece o processamento contextual: cada token pode “olhar” para outros tokens do contexto e ajustar sua representação.\n",
    "\n",
    "Embora o seu `GPTModel` não mostre o conteúdo de `TransformerBlock`, um bloco GPT típico contém:\n",
    "\n",
    "1. **Atenção causal (masked self-attention)**  \n",
    "   - Cada posição `t` só pode atender **posições ≤ t**  \n",
    "   - Isso impede “ver o futuro” e torna o modelo autoregressivo (essencial para previsão do próximo token). :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "2. **MLP / Feed-Forward**  \n",
    "   - Uma rede totalmente conectada aplicada por token para refinar representações.\n",
    "\n",
    "3. **Residuais + Normalização**  \n",
    "   - Conexões de atalho (skip connections) preservam informação e estabilizam gradientes.\n",
    "   - LayerNorm ajuda a estabilizar o treino.\n",
    "\n",
    "Como você empilha `n_layers`, o modelo aprende dependências mais complexas e de longo alcance.\n",
    "\n",
    "## Finalização: normalizar e projetar para o vocabulário\n",
    "\n",
    "### `final_norm`\n",
    "\n",
    "```\n",
    "x = self.final_norm(x)\n",
    "```\n",
    "Uma última normalização antes de converter para logits. Em GPTs isso costuma melhorar a estabilidade e a qualidade das saídas.\n",
    "\n",
    "### `out_head`: Linear para logits\n",
    "```\n",
    "logits = self.out_head(x)\n",
    "```\n",
    "- Entrada: **(B, T, C)**\n",
    "- Saída: **(B, T, V)**\n",
    "\n",
    "Essa projeção “transforma” cada vetor oculto em um vetor de scores para cada token do vocabulário.\n",
    "\n",
    "> Importante: **logits não são probabilidades**. Para obter probabilidades você aplicaria `softmax` no eixo do vocabulário (geralmente só na hora de gerar texto).\n",
    "\n",
    "## O que o modelo aprende no treino (visão operacional)\n",
    "\n",
    "No treino de *next-token prediction*, você normalmente:\n",
    "- fornece uma sequência de entrada\n",
    "- pede ao modelo para prever o próximo token em cada posição\n",
    "\n",
    "Exemplo conceitual:\n",
    "- entrada:  `[\"Eu\", \"gosto\", \"de\"]`\n",
    "- alvo:     `[\"gosto\", \"de\", \"café\"]`\n",
    "\n",
    "O modelo produz logits para cada posição e a loss (ex.: cross-entropy) compara com os alvos deslocados.\n",
    "\n",
    "Essa lógica é o fundamento do GPT: uma tarefa simples (prever próximo token) que, com dados e escala, leva a comportamento linguístico complexo. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "## Checagem de shapes (resumo)\n",
    "\n",
    "Considere `B=batch_size`, `T=seq_len`, `C=emb_dim`, `V=vocab_size`:\n",
    "\n",
    "| Etapa | Tensor | Shape |\n",
    "|------|--------|-------|\n",
    "| entrada | `in_idx` | (B, T) |\n",
    "| token emb | `tok_embeds` | (B, T, C) |\n",
    "| pos emb | `pos_embeds` | (T, C) |\n",
    "| soma | `x` | (B, T, C) |\n",
    "| blocos | `x` | (B, T, C) |\n",
    "| norm final | `x` | (B, T, C) |\n",
    "| cabeça | `logits` | (B, T, V) |\n",
    "\n",
    "## Por que o `forward` valida entrada?\n",
    "\n",
    "O método valida:\n",
    "- se `in_idx` é `torch.Tensor`\n",
    "- se é 2D (B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3dc7741-ebdf-432d-9828-7ffbab39f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação de um modelo GPT (decoder-only Transformer).\n",
    "\n",
    "    Componentes:\n",
    "    -----------\n",
    "    - Embedding de tokens (tok_emb): converte IDs de tokens em vetores.\n",
    "    - Embedding posicional (pos_emb): adiciona informação de posição na sequência.\n",
    "    - Dropout (drop_emb): regularização nos embeddings somados.\n",
    "    - Blocos Transformer (trf_blocks): pilha de TransformerBlock.\n",
    "    - Normalização final (final_norm): LayerNorm antes da projeção final.\n",
    "    - Cabeça de saída (out_head): projeção para logits no vocabulário.\n",
    "\n",
    "    Parâmetros esperados em `cfg`:\n",
    "    ------------------------------\n",
    "    vocab_size : int\n",
    "        Tamanho do vocabulário (número de tokens possíveis).\n",
    "    emb_dim : int\n",
    "        Dimensão dos embeddings / hidden size.\n",
    "    context_length : int\n",
    "        Comprimento máximo de contexto (seq_len máximo).\n",
    "    drop_rate : float\n",
    "        Taxa de dropout aplicada após a soma tok_emb + pos_emb.\n",
    "    n_layers : int\n",
    "        Número de blocos Transformer.\n",
    "\n",
    "    Retorno do forward:\n",
    "    -------------------\n",
    "    torch.Tensor\n",
    "        Logits com shape (batch_size, seq_len, vocab_size).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Executa o forward pass do GPT.\n",
    "\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        in_idx : torch.Tensor\n",
    "            Tensor de IDs de tokens com shape (batch_size, seq_len) e dtype inteiro.\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Logits com shape (batch_size, seq_len, vocab_size).\n",
    "\n",
    "        Exceções:\n",
    "        --------\n",
    "        Levanta ValueError se `in_idx` não for um tensor 2D.\n",
    "        \"\"\"\n",
    "        if not isinstance(in_idx, torch.Tensor):\n",
    "            raise TypeError(\"`in_idx` deve ser um torch.Tensor.\")\n",
    "        if in_idx.ndim != 2:\n",
    "            raise ValueError(\"`in_idx` deve ter shape (batch_size, seq_len).\")\n",
    "\n",
    "        batch_size, seq_len = in_idx.shape  # batch_size não é usado diretamente aqui\n",
    "\n",
    "        tok_embeds = self.tok_emb(in_idx)  # (B, T, C)\n",
    "\n",
    "        pos_ids = torch.arange(seq_len, device=in_idx.device)\n",
    "        pos_embeds = self.pos_emb(pos_ids)  # (T, C) -> broadcast p/ (B, T, C)\n",
    "\n",
    "        x = tok_embeds + pos_embeds  # (B, T, C)\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)  # (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4bc49-cd1a-4f4c-8e6f-0449d6209d7b",
   "metadata": {},
   "source": [
    "## Teste do GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857c380f-5906-40cc-bd74-8af294deebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   46,   308,  5549, 18586,    68,   645]])\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "txt1 = \"O gato dorme no\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b790f0-69db-42fb-9e0e-7296c67b6df1",
   "metadata": {},
   "source": [
    "### Verificando os tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf47b79-9106-480e-9689-fa4f39ae5535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[   46,   308,  5549, 18586,    68,   645]])\n",
      "\n",
      "Output shape: torch.Size([1, 6, 50257])\n",
      "tensor([[[-0.0545, -0.7861,  0.9106,  ...,  0.6177,  0.0392, -0.4710],\n",
      "         [ 0.2858, -0.3389,  0.0768,  ..., -0.7126, -0.2354, -0.1229],\n",
      "         [ 0.8791, -0.1072,  0.6407,  ..., -1.0492, -0.9322, -0.8358],\n",
      "         [-0.5154,  0.2904,  0.4959,  ..., -0.8074, -0.2613, -0.7128],\n",
      "         [ 0.5876,  0.0875,  0.1256,  ..., -1.2970, -0.0646, -0.7104],\n",
      "         [ 0.1734,  0.3456,  0.5032,  ..., -0.7875,  1.3982, -1.1401]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1001e-df8a-4f88-a95c-2151a740f1f6",
   "metadata": {},
   "source": [
    "### Número total de parâmetros treináveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80be86b-9271-4356-99f0-83df96b52cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de parâmetros: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Número total de parâmetros: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "503bb4bc-8299-435d-a62c-6429544a805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato da camada de embeddings de tokens: torch.Size([50257, 768])\n",
      "Formato da camada de saída: torch.Size([50257, 768])\n",
      "Tamanho total do modelo: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Formato da camada de embeddings de tokens:\", model.tok_emb.weight.shape)\n",
    "print(\"Formato da camada de saída:\", model.out_head.weight.shape)\n",
    "\n",
    "# Calcula o tamanho total em bytes (assumindo float32, 4 bytes por parâmetro)\n",
    "tamanho_total_bytes = total_params * 4\n",
    "\n",
    "# Converte para megabytes\n",
    "tamanho_total_mb = tamanho_total_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Tamanho total do modelo: {tamanho_total_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a29ce9-2a62-4e25-9958-6a78aa6e9ebc",
   "metadata": {},
   "source": [
    "## Geração dos próximos tokens a partir de pequeno texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b2c3b0-5101-4151-87a2-842598839088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(\n",
    "    model: nn.Module,\n",
    "    idx: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    context_size: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gera texto de forma simples (greedy decoding), adicionando tokens um a um.\n",
    "\n",
    "    A cada iteração:\n",
    "    - Recorta o contexto para no máximo `context_size` tokens (janela deslizante).\n",
    "    - Calcula os logits do modelo.\n",
    "    - Usa apenas o último passo de tempo (último token) para decidir o próximo token.\n",
    "    - Aplica softmax e seleciona o token de maior probabilidade (argmax).\n",
    "    - Concatena o token escolhido à sequência.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Modelo que recebe um tensor de IDs (batch_size, seq_len) e retorna logits\n",
    "        (batch_size, seq_len, vocab_size).\n",
    "    idx : torch.Tensor\n",
    "        Tensor de IDs de tokens do contexto atual, com shape (batch_size, n_tokens).\n",
    "        Deve ser inteiro (tipicamente torch.long).\n",
    "    max_new_tokens : int\n",
    "        Número máximo de novos tokens a serem gerados.\n",
    "    context_size : int\n",
    "        Tamanho máximo do contexto suportado (janela usada como entrada do modelo).\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Tensor com a sequência estendida, shape (batch_size, n_tokens + max_new_tokens).\n",
    "\n",
    "    Exceções:\n",
    "    --------\n",
    "    Levanta TypeError/ValueError para entradas inválidas (tipos, shapes, valores).\n",
    "    \"\"\"\n",
    "    if not isinstance(model, nn.Module):\n",
    "        raise TypeError(\"`model` deve ser uma instância de torch.nn.Module.\")\n",
    "    if not isinstance(idx, torch.Tensor):\n",
    "        raise TypeError(\"`idx` deve ser um torch.Tensor.\")\n",
    "    if idx.ndim != 2:\n",
    "        raise ValueError(\"`idx` deve ter shape (batch_size, n_tokens).\")\n",
    "    if not isinstance(max_new_tokens, int) or max_new_tokens < 0:\n",
    "        raise ValueError(\"`max_new_tokens` deve ser um inteiro >= 0.\")\n",
    "    if not isinstance(context_size, int) or context_size <= 0:\n",
    "        raise ValueError(\"`context_size` deve ser um inteiro > 0.\")\n",
    "\n",
    "    # Garante que idx esteja em um dtype inteiro comum para embeddings\n",
    "    if idx.dtype not in (\n",
    "        torch.int64,\n",
    "        torch.int32,\n",
    "        torch.int16,\n",
    "        torch.int8,\n",
    "        torch.uint8,\n",
    "    ):\n",
    "        raise TypeError(\"`idx` deve ser um tensor de inteiros (ex.: torch.long).\")\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Recorta o contexto para caber no tamanho máximo suportado\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Predição sem gradientes\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Usa apenas o último passo de tempo: (B, T, V) -> (B, V)\n",
    "        logits_last = logits[:, -1, :]\n",
    "\n",
    "        # Probabilidades do próximo token: (B, V)\n",
    "        probas = torch.softmax(logits_last, dim=-1)\n",
    "\n",
    "        # Escolha greedy: token com maior probabilidade (B, 1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        # Concatena o próximo token à sequência: (B, T) -> (B, T+1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc33adea-a688-4f1e-beab-3d578e676fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [46, 308, 5549, 523, 1350, 645]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"O gato sobe no\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecaa8a0-400f-4d51-af14-477864175774",
   "metadata": {},
   "source": [
    "### Verificando os tensores de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd1d9562-7933-44a4-93a0-e3b37d2cc892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[   46,   308,  5549,   523,  1350,   645, 34381, 37516, 12434, 42030,\n",
      "         37383,   678]])\n",
      "Output length: 12\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,  # 6 tokens além dos que já foram enviados\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198fae24-f651-4f38-8d31-ec2af19c636a",
   "metadata": {},
   "source": [
    "### O texto saiu aleatório pois ainda não houve treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12fcaff8-58ee-4251-8dff-d5a9985853be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O gato sobe no Dice eighty Driverfoundland Garner 19\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
