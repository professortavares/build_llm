{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee21ee2-60b2-4070-b2c3-9bac1c26d6ec",
   "metadata": {},
   "source": [
    "# Coding an LLM architecture\n",
    "\n",
    "**Descrição**  \n",
    "Este notebook monta o **esqueleto de uma arquitetura GPT (LLM decoder-only)** em PyTorch, com foco no **fluxo de dados e nos formatos (shapes)**. Para isso, ele centraliza os hiperparâmetros em um dicionário de configuração, valida essa configuração com uma função utilitária e implementa versões *placeholder* (dummy) de componentes essenciais (bloco Transformer e LayerNorm). A ideia é deixar o pipeline completo pronto para, depois, substituir os “dummies” pelas implementações reais. :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}\n",
    "\n",
    "**Objetivo**  \n",
    "- Definir um **contrato claro de configuração** (hiperparâmetros obrigatórios e coerência).  \n",
    "- Garantir que a arquitetura esteja **estruturada como um GPT** (embeddings → blocos Transformer → normalização → projeção para o vocabulário), mesmo antes da atenção/MLP reais.  \n",
    "- Permitir testar cedo o `forward` e os shapes de entrada/saída, reduzindo bugs quando os módulos verdadeiros forem adicionados. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "**Funcionamento**  \n",
    "1. **Configuração (`GPT_CONFIG_124M`)**: define `vocab_size`, `context_length`, `emb_dim`, `n_heads`, `n_layers`, `drop_rate`, `qkv_bias`.  \n",
    "2. **Validação (`validate_gpt_config`)**: checa presença de chaves, tipos e valores aceitáveis antes de construir o modelo.  \n",
    "3. **Componentes dummy**:  \n",
    "   - `DummyTransformerBlock`: mantém a interface, mas retorna a entrada sem mudanças.  \n",
    "   - `DummyLayerNorm`: mantém a interface, mas não normaliza (retorna a entrada).  \n",
    "4. **Modelo (`DummyGPTModel`)**:  \n",
    "   - Recebe `in_idx` com shape **(batch_size, seq_len)** e valida `seq_len <= context_length`.  \n",
    "   - Calcula **token embeddings** e **positional embeddings**, soma e aplica dropout.  \n",
    "   - Passa pela pilha de “blocos Transformer” (dummy) e por uma normalização final (dummy).  \n",
    "   - Projeta para logits via `out_head`, retornando **(batch_size, seq_len, vocab_size)**. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "\n",
    "![O gato sobe no tapete](../../imagens/cap04/01_building_blocks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9188051-e9e7-4375-8cf2-4ff4a7352b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a293d-8938-4ca1-a357-9d2af992fe74",
   "metadata": {},
   "source": [
    "## Configuração do modelo (hiperparâmetros)\n",
    "\n",
    "Este dicionário reúne os **hiperparâmetros** que definem o “tamanho” e o comportamento básico do GPT (arquitetura e regularização). A ideia é centralizar tudo em um único lugar para que o resto do código (embeddings, blocos Transformer, etc.) consiga ler esses valores de forma consistente.\n",
    "\n",
    "* **`vocab_size`**: quantidade de tokens possíveis no vocabulário (define o tamanho da tabela de embeddings e a dimensão de saída da *head* de logits).\n",
    "* **`context_length`**: tamanho máximo de sequência (quantos tokens o modelo consegue “enxergar” de uma vez), usado no embedding posicional.\n",
    "* **`emb_dim`**: dimensão dos vetores de embedding (também chamada de `d_model`), que é a “largura” das representações internas.\n",
    "* **`n_heads`**: número de cabeças de atenção em *multi-head attention* (em implementações reais, `emb_dim` costuma ser divisível por `n_heads`).\n",
    "* **`n_layers`**: número de blocos Transformer empilhados (profundidade do modelo).\n",
    "* **`drop_rate`**: taxa de dropout para regularização (ajuda a reduzir overfitting durante o treino).\n",
    "* **`qkv_bias`**: indica se as projeções lineares de **Query/Key/Value** usam termo de viés (bias) ou não.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a8b978-3495-4f1a-aa29-0b99d9dc091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50257,  # Tamanho do vocabulário\n",
    "    \"context_length\": 1024,  # Comprimento do contexto\n",
    "    \"emb_dim\": 768,  # Dimensão do embedding\n",
    "    \"n_heads\": 12,  # Número de cabeças de atenção\n",
    "    \"n_layers\": 12,  # Número de camadas\n",
    "    \"drop_rate\": 0.1,  # Taxa de dropout\n",
    "    \"qkv_bias\": False,  # Viés em Query-Key-Value (QKV)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55d76f-f241-4655-9ace-1173215b215a",
   "metadata": {},
   "source": [
    "## Função de validação da configuração (`validate_gpt_config`)\n",
    "\n",
    "Este trecho cria uma função utilitária para **verificar se o dicionário `cfg` está completo e coerente** antes de construir o modelo. Isso evita erros difíceis de debugar mais adiante (por exemplo, `KeyError` no meio do `forward` ou shapes incompatíveis) e garante que hiperparâmetros essenciais estejam presentes, com tipos e faixas aceitáveis.\n",
    "\n",
    "* **Assinatura e objetivo**\n",
    "\n",
    "  * `cfg: Mapping[str, Any]`: aceita qualquer “dicionário-like” (dict, `Mapping`, configs imutáveis etc.).\n",
    "  * Retorna `None`: a função apenas **valida**; se algo estiver errado, ela **interrompe** com exceção.\n",
    "\n",
    "* **`required_keys`: contrato do que o modelo espera**\n",
    "\n",
    "  * Define um “schema” mínimo: cada chave obrigatória e o(s) tipo(s) aceito(s).\n",
    "  * `drop_rate` aceita `(float, int)` porque é comum o usuário passar `0` ou `1` como inteiros.\n",
    "\n",
    "* **Checagem de chaves ausentes**\n",
    "\n",
    "  * `missing = [...]` coleta tudo que falta.\n",
    "  * Se houver faltas, levanta `KeyError` com uma mensagem clara, indicando exatamente quais chaves precisam ser adicionadas.\n",
    "\n",
    "* **Checagem de tipos**\n",
    "\n",
    "  * Itera por `required_keys.items()` e valida com `isinstance`.\n",
    "  * Se algum tipo estiver incorreto, levanta `TypeError` dizendo:\n",
    "\n",
    "    * qual chave falhou,\n",
    "    * qual tipo era esperado,\n",
    "    * qual tipo foi recebido.\n",
    "\n",
    "* **Validações de domínio (valores válidos)**\n",
    "\n",
    "  * Garante que parâmetros que definem dimensões e contagens sejam **positivos**:\n",
    "\n",
    "    * `vocab_size`, `context_length`, `emb_dim`, `n_heads`, `n_layers` > 0\n",
    "  * Restringe `drop_rate` ao intervalo **[0.0, 1.0]**, já que dropout é uma probabilidade/taxa.\n",
    "  * Cada regra levanta `ValueError` com uma mensagem direta, facilitando a correção.\n",
    "\n",
    "Em resumo, essa função funciona como uma “barreira de segurança”: **se a configuração passar por aqui, o restante do código pode assumir que `cfg` tem o formato esperado**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7426231d-beb0-475f-a037-2d87dba1f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_gpt_config(cfg: Mapping[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Valida um dicionário de configuração para um GPT-like model.\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    cfg : Mapping[str, Any]\n",
    "        Dicionário (ou mapping) com as chaves esperadas pelo modelo.\n",
    "\n",
    "    Retorno\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Exceções\n",
    "    --------\n",
    "    Levanta KeyError se faltar alguma chave obrigatória.\n",
    "    Levanta TypeError/ValueError se algum valor for inválido.\n",
    "    \"\"\"\n",
    "    required_keys = {\n",
    "        \"vocab_size\": int,\n",
    "        \"context_length\": int,\n",
    "        \"emb_dim\": int,\n",
    "        \"n_heads\": int,\n",
    "        \"n_layers\": int,\n",
    "        \"drop_rate\": (float, int),\n",
    "        \"qkv_bias\": bool,\n",
    "    }\n",
    "\n",
    "    missing = [k for k in required_keys if k not in cfg]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Config incompleta. Faltando chaves: {missing}\")\n",
    "\n",
    "    for k, expected_type in required_keys.items():\n",
    "        if not isinstance(cfg[k], expected_type):\n",
    "            raise TypeError(\n",
    "                f\"cfg['{k}'] deve ser do tipo {expected_type}, mas recebeu {type(cfg[k])}.\"\n",
    "            )\n",
    "\n",
    "    if int(cfg[\"vocab_size\"]) <= 0:\n",
    "        raise ValueError(\"cfg['vocab_size'] deve ser > 0.\")\n",
    "    if int(cfg[\"context_length\"]) <= 0:\n",
    "        raise ValueError(\"cfg['context_length'] deve ser > 0.\")\n",
    "    if int(cfg[\"emb_dim\"]) <= 0:\n",
    "        raise ValueError(\"cfg['emb_dim'] deve ser > 0.\")\n",
    "    if int(cfg[\"n_heads\"]) <= 0:\n",
    "        raise ValueError(\"cfg['n_heads'] deve ser > 0.\")\n",
    "    if int(cfg[\"n_layers\"]) <= 0:\n",
    "        raise ValueError(\"cfg['n_layers'] deve ser > 0.\")\n",
    "    if not (0.0 <= float(cfg[\"drop_rate\"]) <= 1.0):\n",
    "        raise ValueError(\"cfg['drop_rate'] deve estar no intervalo [0.0, 1.0].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d737db-e212-4717-bbdf-552ac4c1ad5c",
   "metadata": {},
   "source": [
    "## Classe `DummyTransformerBlock` (bloco Transformer “falso”)\n",
    "\n",
    "Este trecho define um **bloco Transformer placeholder** que mantém a *mesma interface* (construtor + `forward`) de um bloco real, mas **não aplica nenhuma transformação** nos dados. Ele é útil para montar a “carcaça” da arquitetura do GPT (empilhamento de blocos, fluxo do `forward`, shapes esperados) antes de implementar atenção, MLP, residuais e normalizações de verdade.\n",
    "\n",
    "* **Por que existe?**\n",
    "\n",
    "  * Permite testar o pipeline completo do modelo (embeddings → blocos → head de saída) sem depender das partes complexas ainda não implementadas.\n",
    "  * Ajuda a garantir que os tensores tenham os formatos corretos e que o código esteja bem organizado para substituição posterior.\n",
    "\n",
    "* **`__init__(cfg)`**\n",
    "\n",
    "  * Chama `super().__init__()` para inicializar corretamente o `nn.Module`.\n",
    "  * Executa `validate_gpt_config(cfg)` para garantir que a configuração possui as chaves e valores esperados, evitando erros silenciosos ou inconsistências.\n",
    "\n",
    "* **`forward(x)`**\n",
    "\n",
    "  * Recebe `x` no formato típico **(batch, seq_len, emb_dim)**.\n",
    "  * Retorna **exatamente o mesmo tensor** (`return x`), sem atenção, sem MLP, sem dropout, sem conexões residuais.\n",
    "  * Mantém a assinatura e o comportamento mínimo para que `nn.Sequential` consiga encadear vários “blocos” de forma idêntica a um Transformer real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce7f4d9-cef9-422f-a6a9-27c901e4095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Placeholder de um TransformerBlock.\n",
    "\n",
    "    Observação:\n",
    "    ----------\n",
    "    Esta implementação não faz nada além de retornar a entrada.\n",
    "    Ela existe apenas para manter a estrutura do modelo enquanto\n",
    "    os blocos reais são implementados nas próximas seções.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Mapping[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "        validate_gpt_config(cfg)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Retorna a entrada inalterada.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        x : Tensor\n",
    "            Tensor de entrada no formato (batch, seq_len, emb_dim).\n",
    "\n",
    "        Retorno\n",
    "        -------\n",
    "        Tensor\n",
    "            O mesmo tensor de entrada.\n",
    "        \"\"\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ce473-41d1-463a-afa9-d43ec5be34e7",
   "metadata": {},
   "source": [
    "## Classe `DummyLayerNorm` (LayerNorm “falso”)\n",
    "\n",
    "Este trecho define uma **versão placeholder de `LayerNorm`**: ela mantém a mesma estrutura e parâmetros esperados por uma normalização real, mas **não realiza nenhuma normalização** (apenas devolve a entrada). O objetivo é permitir que a arquitetura completa do GPT seja montada e testada desde cedo, deixando a implementação correta da normalização para uma etapa posterior.\n",
    "\n",
    "* **Por que mimetizar o `LayerNorm`?**\n",
    "\n",
    "  * Em um Transformer real, `LayerNorm` é parte fundamental da estabilidade do treinamento.\n",
    "  * Aqui, a classe existe para preservar o “formato” do código (mesma API), facilitando substituir `DummyLayerNorm` por uma implementação real sem mudar o restante do modelo.\n",
    "\n",
    "* **`__init__(normalized_shape, eps=1e-5)`**\n",
    "\n",
    "  * Chama `super().__init__()` para inicializar corretamente o `nn.Module`.\n",
    "  * Valida `normalized_shape`:\n",
    "\n",
    "    * precisa ser `int` e **maior que 0**, pois representa a dimensão que seria normalizada (tipicamente `emb_dim`).\n",
    "  * Armazena:\n",
    "\n",
    "    * `self.normalized_shape` (metadado/compatibilidade)\n",
    "    * `self.eps` como `float` (em LayerNorm real, `eps` evita divisão por zero).\n",
    "\n",
    "* **`forward(x)`**\n",
    "\n",
    "  * Recebe um tensor `x` (sem impor shape específico aqui).\n",
    "  * Retorna `x` **inalterado**, sem calcular média, variância ou aplicar escala/viés.\n",
    "  * Mantém a assinatura para que o restante do pipeline funcione como se houvesse uma normalização real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25542346-07fc-47bf-ab12-f971e5c6cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyLayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Placeholder de LayerNorm.\n",
    "\n",
    "    Observação:\n",
    "    ----------\n",
    "    Esta implementação não normaliza; apenas retorna a entrada.\n",
    "    Ela mimetiza a interface para facilitar a troca posterior.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape: int, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        if not isinstance(normalized_shape, int) or normalized_shape <= 0:\n",
    "            raise ValueError(\"normalized_shape deve ser um int > 0.\")\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = float(eps)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Retorna a entrada inalterada.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        x : Tensor\n",
    "            Tensor de entrada.\n",
    "\n",
    "        Retorno\n",
    "        -------\n",
    "        Tensor\n",
    "            O mesmo tensor de entrada.\n",
    "        \"\"\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae0426-e826-4c7f-9657-451fd2fd8ab0",
   "metadata": {},
   "source": [
    "## Classe `DummyGPTModel` (esqueleto de um GPT)\n",
    "\n",
    "Este trecho implementa um **modelo GPT simplificado**, montando a estrutura principal do pipeline (embeddings → blocos Transformer → normalização → projeção para vocabulário), mas usando **componentes “dummy”** (`DummyTransformerBlock` e `DummyLayerNorm`) que ainda não fazem o trabalho real. A intenção é ter uma arquitetura funcional em termos de *shapes* e fluxo de dados, pronta para receber as implementações completas nas próximas seções.\n",
    "\n",
    "---\n",
    "\n",
    "### Visão geral dos componentes\n",
    "\n",
    "* **Token Embedding (`tok_emb`)**\n",
    "  Converte cada índice de token em um vetor denso de dimensão `emb_dim`.\n",
    "  Saída típica: `(B, T, C)`.\n",
    "\n",
    "* **Positional Embedding (`pos_emb`)**\n",
    "  Cria embeddings para posições `0..T-1`, permitindo ao modelo diferenciar a ordem dos tokens.\n",
    "  Saída típica: `(T, C)`.\n",
    "\n",
    "* **Dropout (`drop_emb`)**\n",
    "  Regularização aplicada após somar token + posição (com taxa `drop_rate`).\n",
    "\n",
    "* **Pilha de blocos Transformer (`trf_blocks`)**\n",
    "  Aqui é uma sequência de `n_layers` blocos *placeholder*, construída com `nn.Sequential`, mantendo a mesma estrutura de um GPT real.\n",
    "\n",
    "* **Normalização final (`final_norm`)**\n",
    "  Placeholder de LayerNorm aplicado antes da camada de saída (em GPTs reais isso ajuda estabilidade).\n",
    "\n",
    "* **Cabeça de saída (`out_head`)**\n",
    "  Projeção linear de `emb_dim` para `vocab_size`, gerando **logits** para cada token do vocabulário em cada posição da sequência.\n",
    "\n",
    "---\n",
    "\n",
    "#### `__init__(cfg)`: construção do modelo a partir da configuração\n",
    "\n",
    "* **Validação do `cfg`**\n",
    "\n",
    "  * `validate_gpt_config(cfg)` garante que as chaves existem, tipos estão corretos e valores fazem sentido.\n",
    "\n",
    "* **Leitura e normalização dos hiperparâmetros**\n",
    "\n",
    "  * Converte explicitamente para `int/float` para evitar surpresas com tipos.\n",
    "\n",
    "* **Criação das camadas**\n",
    "\n",
    "  * `nn.Embedding(vocab_size, emb_dim)`: tabela de embeddings de tokens.\n",
    "  * `nn.Embedding(context_length, emb_dim)`: tabela de embeddings posicionais até o máximo de contexto.\n",
    "  * `nn.Dropout(drop_rate)`: dropout aplicado no embedding somado.\n",
    "  * `nn.Sequential(*[...])`: empilha `n_layers` blocos.\n",
    "  * `nn.Linear(emb_dim, vocab_size, bias=False)`: transforma vetores internos em logits do vocabulário.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward(in_idx)`: fluxo do dado no modelo\n",
    "\n",
    "* **Validações de entrada**\n",
    "\n",
    "  * Garante que `in_idx` é um `Tensor`.\n",
    "  * Exige que seja 2D: `(batch_size, seq_len)`.\n",
    "  * Verifica se `seq_len` não excede `context_length` (porque o embedding posicional foi criado com esse limite).\n",
    "\n",
    "* **Embeddings**\n",
    "\n",
    "  * `tok_embeds = tok_emb(in_idx)` → `(B, T, C)`\n",
    "    Cada token vira um vetor.\n",
    "  * `pos_ids = arange(seq_len)` e `pos_embeds = pos_emb(pos_ids)` → `(T, C)`\n",
    "    Cada posição recebe um vetor.\n",
    "\n",
    "* **Combinação token + posição**\n",
    "\n",
    "  * `x = tok_embeds + pos_embeds`\n",
    "    O PyTorch faz *broadcasting*: `(B, T, C) + (T, C) → (B, T, C)`.\n",
    "    Isso injeta informação de ordem na representação.\n",
    "\n",
    "* **Aplicação das “partes do Transformer”**\n",
    "\n",
    "  * `x = drop_emb(x)` aplica dropout.\n",
    "  * `x = trf_blocks(x)` passa por `n_layers` blocos (aqui não alteram nada, mas preservam o pipeline).\n",
    "  * `x = final_norm(x)` normalização final (placeholder).\n",
    "\n",
    "* **Geração dos logits**\n",
    "\n",
    "  * `logits = out_head(x)` → `(B, T, vocab_size)`\n",
    "    Para cada posição `T` e item do batch `B`, o modelo produz uma distribuição (logits) sobre todos os tokens do vocabulário.\n",
    "\n",
    "---\n",
    "\n",
    "#### Resultado final\n",
    "\n",
    "Ao final, a classe entrega um modelo que **já produz logits com o shape correto** para geração de texto e treino (cross-entropy), mesmo antes da implementação real dos blocos Transformer e da LayerNorm. Isso facilita construir e testar a arquitetura por etapas, substituindo os placeholders gradualmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4deeee7f-47c6-4cdf-9a41-4a8ae0b106e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Arquitetura GPT simplificada (com blocos e LayerNorm 'dummy').\n",
    "\n",
    "    Componentes:\n",
    "    -----------\n",
    "    - Token embedding\n",
    "    - Positional embedding\n",
    "    - Dropout\n",
    "    - Pilha de blocos Transformer (placeholder)\n",
    "    - LayerNorm final (placeholder)\n",
    "    - Cabeça linear para logits no vocabulário\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Mapping[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "        validate_gpt_config(cfg)\n",
    "\n",
    "        vocab_size = int(cfg[\"vocab_size\"])\n",
    "        emb_dim = int(cfg[\"emb_dim\"])\n",
    "        context_length = int(cfg[\"context_length\"])\n",
    "        drop_rate = float(cfg[\"drop_rate\"])\n",
    "        n_layers = int(cfg[\"n_layers\"])\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(context_length, emb_dim)\n",
    "        self.drop_emb = nn.Dropout(drop_rate)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(emb_dim)\n",
    "        self.out_head = nn.Linear(emb_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, in_idx: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Executa o forward pass.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        in_idx : Tensor\n",
    "            Tensor de índices de tokens no formato (batch_size, seq_len),\n",
    "            tipicamente dtype torch.long.\n",
    "\n",
    "        Retorno\n",
    "        -------\n",
    "        Tensor\n",
    "            Logits no formato (batch_size, seq_len, vocab_size).\n",
    "\n",
    "        Exceções\n",
    "        --------\n",
    "        Levanta ValueError se seq_len > cfg[\"context_length\"].\n",
    "        Levanta TypeError se o input não for um Tensor 2D.\n",
    "        \"\"\"\n",
    "        if not isinstance(in_idx, Tensor):\n",
    "            raise TypeError(\"in_idx deve ser um torch.Tensor.\")\n",
    "        if in_idx.ndim != 2:\n",
    "            raise TypeError(\"in_idx deve ter 2 dimensões: (batch_size, seq_len).\")\n",
    "\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        if seq_len > self.context_length:\n",
    "            raise ValueError(\n",
    "                f\"seq_len ({seq_len}) excede context_length ({self.context_length}).\"\n",
    "            )\n",
    "\n",
    "        tok_embeds = self.tok_emb(in_idx)  # (B, T, C)\n",
    "        pos_ids = torch.arange(seq_len, device=in_idx.device)\n",
    "        pos_embeds = self.pos_emb(pos_ids)  # (T, C)\n",
    "\n",
    "        x = tok_embeds + pos_embeds  # broadcast: (B, T, C) + (T, C)\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)  # (B, T, vocab_size)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
