{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694fe7b3-3092-4c1e-a365-7fdb9acb27ad",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE)\n",
    "\n",
    "**Descrição:**\n",
    "Este notebook apresenta o funcionamento do **Byte Pair Encoding (BPE)**, um método de tokenização baseado em subpalavras amplamente utilizado em modelos de linguagem modernos. Diferentemente da tokenização por palavras, o BPE constrói tokens a partir de **padrões frequentes de caracteres**, permitindo representar palavras raras ou desconhecidas por meio da combinação de unidades menores.\n",
    "\n",
    "**Objetivo:**\n",
    "Demonstrar, de forma didática e passo a passo, como o algoritmo de Byte Pair Encoding:\n",
    "\n",
    "* Aprende um vocabulário de subpalavras a partir de um corpus\n",
    "* Reduz o problema de palavras fora do vocabulário (OOV)\n",
    "* Cria uma representação compacta e eficiente do texto\n",
    "* Converte texto em sequências de tokens subword (encode)\n",
    "* Reconstrói o texto a partir desses tokens (decode)\n",
    "\n",
    "O objetivo é entender por que o BPE é uma peça fundamental na preparação de texto para o treinamento de LLMs.\n",
    "\n",
    "**Funcionamento:**\n",
    "\n",
    "![Byte pair encoder](../../imagens/04_byte_pair_encode.png)\n",
    "\n",
    "O funcionamento do BPE segue um processo iterativo de aprendizado e aplicação:\n",
    "\n",
    "1. **Inicialização do vocabulário:**\n",
    "   O texto do corpus é inicialmente representado como sequências de caracteres individuais, geralmente com um marcador de fim de palavra.\n",
    "\n",
    "2. **Aprendizado por fusões (merges):**\n",
    "   O algoritmo identifica o par de símbolos adjacentes mais frequente no corpus e os funde em um novo token.\n",
    "   Esse processo é repetido iterativamente, expandindo gradualmente o vocabulário com subpalavras mais frequentes.\n",
    "\n",
    "3. **Encode (tokenização):**\n",
    "   Um novo texto é decomposto em subpalavras usando as regras de fusão aprendidas, garantindo que qualquer palavra possa ser representada, mesmo que nunca tenha aparecido no corpus original.\n",
    "\n",
    "4. **Decode (reconstrução):**\n",
    "   As sequências de subpalavras são combinadas para reconstruir o texto original, preservando a estrutura das palavras.\n",
    "\n",
    "Esse mecanismo permite que modelos de linguagem equilibrem **tamanho de vocabulário**, **expressividade** e **generalização**, sendo a base de tokenizadores como os usados em GPT, BERT e outros LLMs.\n",
    "\n",
    "**Tokenizadores reais:**\n",
    "\n",
    "* **Open AI** https://platform.openai.com/tokenizer\n",
    "* **Tiktokenizer** https://tiktokenizer.vercel.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf23d1-0f7e-4e11-88e6-515a1197c454",
   "metadata": {},
   "source": [
    "## Implementação de um BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc052f9-bcfa-496d-8046-338c2d02c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from collections.abc import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BPEResult:\n",
    "    \"\"\"Armazena resultados do treinamento do BPE (didático).\"\"\"\n",
    "\n",
    "    vocab: dict[\n",
    "        tuple[str, ...], int\n",
    "    ]  # representação do corpus como \"palavra tokenizada\" -> contagem\n",
    "    merges: list[tuple[str, str]]  # lista ordenada de merges aprendidos\n",
    "    token_to_id: dict[str, int]  # vocabulário final (tokens) -> id\n",
    "    id_to_token: dict[int, str]  # id -> token\n",
    "\n",
    "\n",
    "class BytePairEncoderDecoder:\n",
    "    \"\"\"\n",
    "    Byte Pair Encoding (BPE) do zero, com passo a passo (didático).\n",
    "\n",
    "    Ideia central (BPE clássico para subpalavras):\n",
    "    - Representamos cada palavra como caracteres + marcador de fim de palavra (</w>)\n",
    "    - Contamos pares adjacentes mais frequentes (bigramas de símbolos)\n",
    "    - Iterativamente \"fundimos\" (merge) o par mais frequente em um novo símbolo\n",
    "    - Guardamos a sequência de merges para tokenizar (encode) textos novos\n",
    "\n",
    "    Observação:\n",
    "    - Este é o BPE \"clássico\" baseado em palavras, suficiente para entender o mecanismo.\n",
    "    - Em tokenizadores modernos (ex.: GPT-2), há detalhes adicionais (bytes, etc.),\n",
    "      mas o coração do BPE (aprender merges frequentes) é o mesmo.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, end_of_word: str = \"</w>\") -> None:\n",
    "        \"\"\"\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        end_of_word : str, default=\"</w>\"\n",
    "            Marcador de fim de palavra (ajuda o BPE a não \"colar\" palavras entre si).\n",
    "        \"\"\"\n",
    "        if not isinstance(end_of_word, str) or not end_of_word:\n",
    "            raise ValueError(\"`end_of_word` deve ser uma string não vazia.\")\n",
    "        self.end_of_word = end_of_word\n",
    "\n",
    "        # aprendidos após treino\n",
    "        self.merges: list[tuple[str, str]] = []\n",
    "        self.token_to_id: dict[str, int] = {}\n",
    "        self.id_to_token: dict[int, str] = {}\n",
    "\n",
    "    # ======================================================\n",
    "    # Utilitários de pré-processamento (didáticos e simples)\n",
    "    # ======================================================\n",
    "    @staticmethod\n",
    "    def _basic_word_tokenize(text: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Tokeniza em \"palavras\" de forma simples (didática).\n",
    "        - Mantém letras e números como parte das palavras\n",
    "        - Trata pontuação como separador\n",
    "\n",
    "        Ex.: \"O gato, sobe.\" -> [\"O\", \"gato\", \"sobe\"]\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(\"O parâmetro `text` deve ser uma string.\")\n",
    "        words = re.findall(r\"[A-Za-zÀ-ÿ0-9]+\", text)\n",
    "        return words\n",
    "\n",
    "    def _word_to_symbols(self, word: str) -> tuple[str, ...]:\n",
    "        \"\"\"\n",
    "        Converte uma palavra em uma tupla de símbolos (caracteres + </w>).\n",
    "        \"\"\"\n",
    "        if not isinstance(word, str) or not word:\n",
    "            raise ValueError(\"`word` deve ser uma string não vazia.\")\n",
    "        return tuple(list(word) + [self.end_of_word])\n",
    "\n",
    "    # =========================================\n",
    "    # PASSO A PASSO: construção do \"vocab\" BPE\n",
    "    # =========================================\n",
    "    def build_initial_vocab(\n",
    "        self, corpus: Iterable[str], verbose: bool = True\n",
    "    ) -> dict[tuple[str, ...], int]:\n",
    "        \"\"\"\n",
    "        Constrói o vocabulário inicial do BPE:\n",
    "        - Cada palavra vira uma sequência de caracteres + </w>\n",
    "        - Contabilizamos frequência de cada \"palavra tokenizada\"\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        dict[tuple[str, ...], int]\n",
    "            Mapeia (símbolos da palavra) -> contagem\n",
    "        \"\"\"\n",
    "        if corpus is None:\n",
    "            raise TypeError(\"`corpus` não pode ser None.\")\n",
    "\n",
    "        corpus = list(corpus)\n",
    "        if not corpus:\n",
    "            raise ValueError(\"`corpus` precisa conter ao menos 1 texto/frase.\")\n",
    "\n",
    "        vocab: dict[tuple[str, ...], int] = defaultdict(int)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=== PASSO 1: Corpus bruto ===\")\n",
    "            for i, line in enumerate(corpus, start=1):\n",
    "                print(f\"{i}. {line}\")\n",
    "            print()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=== PASSO 2: Quebrar corpus em palavras (tokenização simples) ===\")\n",
    "\n",
    "        all_words: list[str] = []\n",
    "        for line in corpus:\n",
    "            words = self._basic_word_tokenize(line)\n",
    "            all_words.extend(words)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Palavras extraídas:\", all_words)\n",
    "            print()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=== PASSO 3: Vocabulário inicial (caracteres + </w>) ===\")\n",
    "\n",
    "        for w in all_words:\n",
    "            sym = self._word_to_symbols(w)\n",
    "            vocab[sym] += 1\n",
    "\n",
    "        if verbose:\n",
    "            for sym, cnt in sorted(vocab.items(), key=lambda x: (-x[1], x[0])):\n",
    "                print(f\"{' '.join(sym)}  ->  {cnt}\")\n",
    "            print()\n",
    "\n",
    "        return dict(vocab)\n",
    "\n",
    "    # =========================================\n",
    "    # PASSO A PASSO: contar pares e aplicar merge\n",
    "    # =========================================\n",
    "    @staticmethod\n",
    "    def _get_pair_frequencies(\n",
    "        vocab: dict[tuple[str, ...], int],\n",
    "    ) -> Counter[tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Conta a frequência de pares adjacentes em todo o vocabulário.\n",
    "        \"\"\"\n",
    "        pairs: Counter[tuple[str, str]] = Counter()\n",
    "        for word_symbols, freq in vocab.items():\n",
    "            # ex.: (\"l\",\"o\",\"w\",\"</w>\") -> pares: (\"l\",\"o\"), (\"o\",\"w\"), (\"w\",\"</w>\")\n",
    "            for i in range(len(word_symbols) - 1):\n",
    "                pairs[(word_symbols[i], word_symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_pair_in_word(\n",
    "        word_symbols: tuple[str, ...], pair: tuple[str, str]\n",
    "    ) -> tuple[str, ...]:\n",
    "        \"\"\"\n",
    "        Aplica um merge (a,b) dentro de uma palavra tokenizada:\n",
    "        substitui ocorrências adjacentes de a b por \"ab\".\n",
    "        \"\"\"\n",
    "        a, b = pair\n",
    "        merged_symbol = a + b\n",
    "\n",
    "        new_symbols: list[str] = []\n",
    "        i = 0\n",
    "        while i < len(word_symbols):\n",
    "            # se encontramos a,b adjacentes, funde\n",
    "            if (\n",
    "                i < len(word_symbols) - 1\n",
    "                and word_symbols[i] == a\n",
    "                and word_symbols[i + 1] == b\n",
    "            ):\n",
    "                new_symbols.append(merged_symbol)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(word_symbols[i])\n",
    "                i += 1\n",
    "\n",
    "        return tuple(new_symbols)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        corpus: Iterable[str],\n",
    "        num_merges: int = 30,\n",
    "        verbose: bool = True,\n",
    "        top_pairs_to_show: int = 10,\n",
    "    ) -> BPEResult:\n",
    "        \"\"\"\n",
    "        Treina (aprende) merges BPE a partir do corpus.\n",
    "\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        corpus : Iterable[str]\n",
    "            Textos/frases para treinar o BPE.\n",
    "        num_merges : int, default=30\n",
    "            Número de merges (iterações) a aprender.\n",
    "        verbose : bool, default=True\n",
    "            Se True, imprime passo a passo.\n",
    "        top_pairs_to_show : int, default=10\n",
    "            Quantos pares mais frequentes exibir por iteração (didático).\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        BPEResult\n",
    "            Estruturas do BPE treinado (vocab final, merges e mapeamentos).\n",
    "        \"\"\"\n",
    "        if not isinstance(num_merges, int) or num_merges <= 0:\n",
    "            raise ValueError(\"`num_merges` deve ser um inteiro > 0.\")\n",
    "\n",
    "        vocab = self.build_initial_vocab(corpus, verbose=verbose)\n",
    "\n",
    "        self.merges = []\n",
    "\n",
    "        for step in range(1, num_merges + 1):\n",
    "            pairs = self._get_pair_frequencies(vocab)\n",
    "\n",
    "            if not pairs:\n",
    "                if verbose:\n",
    "                    print(\"Nenhum par restante para fundir. Encerrando.\")\n",
    "                break\n",
    "\n",
    "            best_pair, best_freq = pairs.most_common(1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"=== PASSO 4.{step}: Contagem de pares (top {top_pairs_to_show}) ===\"\n",
    "                )\n",
    "                for p, f in pairs.most_common(top_pairs_to_show):\n",
    "                    print(f\"{p} -> {f}\")\n",
    "                print()\n",
    "                print(f\"=== PASSO 5.{step}: Melhor par para merge ===\")\n",
    "                print(f\"Par escolhido: {best_pair} (freq={best_freq})\")\n",
    "                print(f\"Novo símbolo: '{best_pair[0] + best_pair[1]}'\")\n",
    "                print()\n",
    "\n",
    "            # aplica merge ao vocab inteiro\n",
    "            new_vocab: dict[tuple[str, ...], int] = defaultdict(int)\n",
    "            for word_symbols, freq in vocab.items():\n",
    "                merged = self._merge_pair_in_word(word_symbols, best_pair)\n",
    "                new_vocab[merged] += freq\n",
    "\n",
    "            vocab = dict(new_vocab)\n",
    "            self.merges.append(best_pair)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"=== PASSO 6.{step}: Vocabulário após merge ===\")\n",
    "                # imprime um resumo do vocab (palavras tokenizadas + contagem)\n",
    "                for sym, cnt in sorted(vocab.items(), key=lambda x: (-x[1], x[0]))[:15]:\n",
    "                    print(f\"{' '.join(sym)}  ->  {cnt}\")\n",
    "                if len(vocab) > 15:\n",
    "                    print(\"... (mostrando apenas as 15 primeiras entradas)\")\n",
    "                print()\n",
    "\n",
    "        # Montar \"vocab final\" de tokens: todos os símbolos que aparecem no vocab após merges\n",
    "        final_tokens: set[str] = set()\n",
    "        for word_symbols in vocab.keys():\n",
    "            final_tokens.update(word_symbols)\n",
    "\n",
    "        # Ordem estável (didática): ordena alfabeticamente\n",
    "        final_tokens_sorted = sorted(final_tokens)\n",
    "        self.token_to_id = {tok: i for i, tok in enumerate(final_tokens_sorted)}\n",
    "        self.id_to_token = {i: tok for tok, i in self.token_to_id.items()}\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=== PASSO 7: Tokens finais (vocabulário de subpalavras) ===\")\n",
    "            print(final_tokens_sorted)\n",
    "            print()\n",
    "            print(\"=== PASSO 8: token_to_id (amostra) ===\")\n",
    "            for tok in final_tokens_sorted[:30]:\n",
    "                print(f\"{tok:10s} -> {self.token_to_id[tok]}\")\n",
    "            if len(final_tokens_sorted) > 30:\n",
    "                print(\"... (mostrando apenas 30 tokens)\")\n",
    "            print()\n",
    "\n",
    "        return BPEResult(\n",
    "            vocab=vocab,\n",
    "            merges=self.merges.copy(),\n",
    "            token_to_id=self.token_to_id.copy(),\n",
    "            id_to_token=self.id_to_token.copy(),\n",
    "        )\n",
    "\n",
    "    # =========================================\n",
    "    # Encode / Decode com passo a passo\n",
    "    # =========================================\n",
    "    def _apply_merges_to_symbols(\n",
    "        self, symbols: tuple[str, ...], verbose: bool = True\n",
    "    ) -> tuple[str, ...]:\n",
    "        \"\"\"\n",
    "        Aplica os merges aprendidos, na ordem, a uma sequência de símbolos.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Símbolos iniciais:\", symbols)\n",
    "\n",
    "        for i, pair in enumerate(self.merges, start=1):\n",
    "            new_symbols = self._merge_pair_in_word(symbols, pair)\n",
    "            if new_symbols != symbols and verbose:\n",
    "                print(f\"Merge {i}: {pair} -> '{pair[0] + pair[1]}'\")\n",
    "                print(\"Antes:\", symbols)\n",
    "                print(\"Depois:\", new_symbols)\n",
    "                print()\n",
    "            symbols = new_symbols\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Símbolos finais (após merges):\", symbols)\n",
    "            print()\n",
    "\n",
    "        return symbols\n",
    "\n",
    "    def encode(self, text: str, verbose: bool = True) -> list[int]:\n",
    "        \"\"\"\n",
    "        Tokeniza (BPE) e converte em IDs.\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        list[int]\n",
    "            IDs dos tokens BPE.\n",
    "        \"\"\"\n",
    "        if not self.token_to_id or not self.merges:\n",
    "            raise ValueError(\"BPE não treinado. Execute `train()` antes de `encode()`.\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=== ENCODE (BPE) ===\")\n",
    "            print(\"Texto:\", text)\n",
    "            print()\n",
    "\n",
    "        words = self._basic_word_tokenize(text)\n",
    "        if verbose:\n",
    "            print(\"Palavras:\", words)\n",
    "            print()\n",
    "\n",
    "        all_ids: list[int] = []\n",
    "        for w in words:\n",
    "            if verbose:\n",
    "                print(f\"--- Palavra: '{w}' ---\")\n",
    "            symbols = self._word_to_symbols(w)\n",
    "            final_symbols = self._apply_merges_to_symbols(symbols, verbose=verbose)\n",
    "\n",
    "            # converte símbolos finais em ids (todos devem existir no vocab final)\n",
    "            ids = []\n",
    "            for s in final_symbols:\n",
    "                if s not in self.token_to_id:\n",
    "                    raise ValueError(\n",
    "                        f\"Token '{s}' não existe no vocabulário final. (Isso não deveria ocorrer neste BPE.)\"\n",
    "                    )\n",
    "                ids.append(self.token_to_id[s])\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Tokens BPE:\", final_symbols)\n",
    "                print(\"IDs:\", ids)\n",
    "                print()\n",
    "\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=== SEQUÊNCIA FINAL DE IDS ===\")\n",
    "            print(all_ids)\n",
    "            print()\n",
    "\n",
    "        return all_ids\n",
    "\n",
    "    def decode(self, ids: list[int], verbose: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Converte IDs -> tokens e reconstrói texto.\n",
    "\n",
    "        Regra:\n",
    "        - Junta tokens\n",
    "        - Substitui </w> por espaço de palavra\n",
    "        \"\"\"\n",
    "        if not isinstance(ids, list) or any(not isinstance(i, int) for i in ids):\n",
    "            raise TypeError(\"`ids` deve ser uma lista de inteiros (list[int]).\")\n",
    "        if not self.id_to_token:\n",
    "            raise ValueError(\"BPE não treinado. Execute `train()` antes de `decode()`.\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=== DECODE (BPE) ===\")\n",
    "            print(\"IDs:\", ids)\n",
    "            print()\n",
    "\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            if i not in self.id_to_token:\n",
    "                raise ValueError(f\"ID inválido: {i}\")\n",
    "            tokens.append(self.id_to_token[i])\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Tokens:\", tokens)\n",
    "            print()\n",
    "\n",
    "        # Reconstrução:\n",
    "        # - removemos o marcador </w> trocando por espaço\n",
    "        # - depois fazemos strip para remover espaço final\n",
    "        text = \"\"\n",
    "        for t in tokens:\n",
    "            if t == self.end_of_word:\n",
    "                text += \" \"\n",
    "            else:\n",
    "                text += t\n",
    "\n",
    "        text = text.strip()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Texto reconstruído:\", text)\n",
    "            print()\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bff797-9ea9-4b42-ba0e-445c253b3d46",
   "metadata": {},
   "source": [
    "## Geração do corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4cba5f-4d8c-4b84-b2de-15a4ba1a0d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASSO 1: Corpus bruto ===\n",
      "1. O gato sobe no tapete.\n",
      "2. O cachorro sobe na mesa.\n",
      "3. A aranha desce a parede.\n",
      "4. O gato desce da mesa.\n",
      "\n",
      "=== PASSO 2: Quebrar corpus em palavras (tokenização simples) ===\n",
      "Palavras extraídas: ['O', 'gato', 'sobe', 'no', 'tapete', 'O', 'cachorro', 'sobe', 'na', 'mesa', 'A', 'aranha', 'desce', 'a', 'parede', 'O', 'gato', 'desce', 'da', 'mesa']\n",
      "\n",
      "=== PASSO 3: Vocabulário inicial (caracteres + </w>) ===\n",
      "O </w>  ->  3\n",
      "d e s c e </w>  ->  2\n",
      "g a t o </w>  ->  2\n",
      "m e s a </w>  ->  2\n",
      "s o b e </w>  ->  2\n",
      "A </w>  ->  1\n",
      "a </w>  ->  1\n",
      "a r a n h a </w>  ->  1\n",
      "c a c h o r r o </w>  ->  1\n",
      "d a </w>  ->  1\n",
      "n a </w>  ->  1\n",
      "n o </w>  ->  1\n",
      "p a r e d e </w>  ->  1\n",
      "t a p e t e </w>  ->  1\n",
      "\n",
      "=== PASSO 4.1: Contagem de pares (top 10) ===\n",
      "('e', '</w>') -> 6\n",
      "('a', '</w>') -> 6\n",
      "('o', '</w>') -> 4\n",
      "('e', 's') -> 4\n",
      "('O', '</w>') -> 3\n",
      "('d', 'e') -> 3\n",
      "('g', 'a') -> 2\n",
      "('a', 't') -> 2\n",
      "('t', 'o') -> 2\n",
      "('s', 'o') -> 2\n",
      "\n",
      "=== PASSO 5.1: Melhor par para merge ===\n",
      "Par escolhido: ('e', '</w>') (freq=6)\n",
      "Novo símbolo: 'e</w>'\n",
      "\n",
      "=== PASSO 6.1: Vocabulário após merge ===\n",
      "O </w>  ->  3\n",
      "d e s c e</w>  ->  2\n",
      "g a t o </w>  ->  2\n",
      "m e s a </w>  ->  2\n",
      "s o b e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a </w>  ->  1\n",
      "a r a n h a </w>  ->  1\n",
      "c a c h o r r o </w>  ->  1\n",
      "d a </w>  ->  1\n",
      "n a </w>  ->  1\n",
      "n o </w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 4.2: Contagem de pares (top 10) ===\n",
      "('a', '</w>') -> 6\n",
      "('o', '</w>') -> 4\n",
      "('e', 's') -> 4\n",
      "('O', '</w>') -> 3\n",
      "('g', 'a') -> 2\n",
      "('a', 't') -> 2\n",
      "('t', 'o') -> 2\n",
      "('s', 'o') -> 2\n",
      "('o', 'b') -> 2\n",
      "('b', 'e</w>') -> 2\n",
      "\n",
      "=== PASSO 5.2: Melhor par para merge ===\n",
      "Par escolhido: ('a', '</w>') (freq=6)\n",
      "Novo símbolo: 'a</w>'\n",
      "\n",
      "=== PASSO 6.2: Vocabulário após merge ===\n",
      "O </w>  ->  3\n",
      "d e s c e</w>  ->  2\n",
      "g a t o </w>  ->  2\n",
      "m e s a</w>  ->  2\n",
      "s o b e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a r a n h a</w>  ->  1\n",
      "a</w>  ->  1\n",
      "c a c h o r r o </w>  ->  1\n",
      "d a</w>  ->  1\n",
      "n a</w>  ->  1\n",
      "n o </w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 4.3: Contagem de pares (top 10) ===\n",
      "('o', '</w>') -> 4\n",
      "('e', 's') -> 4\n",
      "('O', '</w>') -> 3\n",
      "('g', 'a') -> 2\n",
      "('a', 't') -> 2\n",
      "('t', 'o') -> 2\n",
      "('s', 'o') -> 2\n",
      "('o', 'b') -> 2\n",
      "('b', 'e</w>') -> 2\n",
      "('m', 'e') -> 2\n",
      "\n",
      "=== PASSO 5.3: Melhor par para merge ===\n",
      "Par escolhido: ('o', '</w>') (freq=4)\n",
      "Novo símbolo: 'o</w>'\n",
      "\n",
      "=== PASSO 6.3: Vocabulário após merge ===\n",
      "O </w>  ->  3\n",
      "d e s c e</w>  ->  2\n",
      "g a t o</w>  ->  2\n",
      "m e s a</w>  ->  2\n",
      "s o b e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a r a n h a</w>  ->  1\n",
      "a</w>  ->  1\n",
      "c a c h o r r o</w>  ->  1\n",
      "d a</w>  ->  1\n",
      "n a</w>  ->  1\n",
      "n o</w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 4.4: Contagem de pares (top 10) ===\n",
      "('e', 's') -> 4\n",
      "('O', '</w>') -> 3\n",
      "('g', 'a') -> 2\n",
      "('a', 't') -> 2\n",
      "('t', 'o</w>') -> 2\n",
      "('s', 'o') -> 2\n",
      "('o', 'b') -> 2\n",
      "('b', 'e</w>') -> 2\n",
      "('m', 'e') -> 2\n",
      "('s', 'a</w>') -> 2\n",
      "\n",
      "=== PASSO 5.4: Melhor par para merge ===\n",
      "Par escolhido: ('e', 's') (freq=4)\n",
      "Novo símbolo: 'es'\n",
      "\n",
      "=== PASSO 6.4: Vocabulário após merge ===\n",
      "O </w>  ->  3\n",
      "d es c e</w>  ->  2\n",
      "g a t o</w>  ->  2\n",
      "m es a</w>  ->  2\n",
      "s o b e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a r a n h a</w>  ->  1\n",
      "a</w>  ->  1\n",
      "c a c h o r r o</w>  ->  1\n",
      "d a</w>  ->  1\n",
      "n a</w>  ->  1\n",
      "n o</w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 4.5: Contagem de pares (top 10) ===\n",
      "('O', '</w>') -> 3\n",
      "('g', 'a') -> 2\n",
      "('a', 't') -> 2\n",
      "('t', 'o</w>') -> 2\n",
      "('s', 'o') -> 2\n",
      "('o', 'b') -> 2\n",
      "('b', 'e</w>') -> 2\n",
      "('m', 'es') -> 2\n",
      "('es', 'a</w>') -> 2\n",
      "('a', 'r') -> 2\n",
      "\n",
      "=== PASSO 5.5: Melhor par para merge ===\n",
      "Par escolhido: ('O', '</w>') (freq=3)\n",
      "Novo símbolo: 'O</w>'\n",
      "\n",
      "=== PASSO 6.5: Vocabulário após merge ===\n",
      "O</w>  ->  3\n",
      "d es c e</w>  ->  2\n",
      "g a t o</w>  ->  2\n",
      "m es a</w>  ->  2\n",
      "s o b e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a r a n h a</w>  ->  1\n",
      "a</w>  ->  1\n",
      "c a c h o r r o</w>  ->  1\n",
      "d a</w>  ->  1\n",
      "n a</w>  ->  1\n",
      "n o</w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 4.6: Contagem de pares (top 10) ===\n",
      "('g', 'a') -> 2\n",
      "('a', 't') -> 2\n",
      "('t', 'o</w>') -> 2\n",
      "('s', 'o') -> 2\n",
      "('o', 'b') -> 2\n",
      "('b', 'e</w>') -> 2\n",
      "('m', 'es') -> 2\n",
      "('es', 'a</w>') -> 2\n",
      "('a', 'r') -> 2\n",
      "('d', 'es') -> 2\n",
      "\n",
      "=== PASSO 5.6: Melhor par para merge ===\n",
      "Par escolhido: ('g', 'a') (freq=2)\n",
      "Novo símbolo: 'ga'\n",
      "\n",
      "=== PASSO 6.6: Vocabulário após merge ===\n",
      "O</w>  ->  3\n",
      "d es c e</w>  ->  2\n",
      "ga t o</w>  ->  2\n",
      "m es a</w>  ->  2\n",
      "s o b e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a r a n h a</w>  ->  1\n",
      "a</w>  ->  1\n",
      "c a c h o r r o</w>  ->  1\n",
      "d a</w>  ->  1\n",
      "n a</w>  ->  1\n",
      "n o</w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 4.7: Contagem de pares (top 10) ===\n",
      "('ga', 't') -> 2\n",
      "('t', 'o</w>') -> 2\n",
      "('s', 'o') -> 2\n",
      "('o', 'b') -> 2\n",
      "('b', 'e</w>') -> 2\n",
      "('m', 'es') -> 2\n",
      "('es', 'a</w>') -> 2\n",
      "('a', 'r') -> 2\n",
      "('d', 'es') -> 2\n",
      "('es', 'c') -> 2\n",
      "\n",
      "=== PASSO 5.7: Melhor par para merge ===\n",
      "Par escolhido: ('ga', 't') (freq=2)\n",
      "Novo símbolo: 'gat'\n",
      "\n",
      "=== PASSO 6.7: Vocabulário após merge ===\n",
      "O</w>  ->  3\n",
      "d es c e</w>  ->  2\n",
      "gat o</w>  ->  2\n",
      "m es a</w>  ->  2\n",
      "s o b e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a r a n h a</w>  ->  1\n",
      "a</w>  ->  1\n",
      "c a c h o r r o</w>  ->  1\n",
      "d a</w>  ->  1\n",
      "n a</w>  ->  1\n",
      "n o</w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 4.8: Contagem de pares (top 10) ===\n",
      "('gat', 'o</w>') -> 2\n",
      "('s', 'o') -> 2\n",
      "('o', 'b') -> 2\n",
      "('b', 'e</w>') -> 2\n",
      "('m', 'es') -> 2\n",
      "('es', 'a</w>') -> 2\n",
      "('a', 'r') -> 2\n",
      "('d', 'es') -> 2\n",
      "('es', 'c') -> 2\n",
      "('c', 'e</w>') -> 2\n",
      "\n",
      "=== PASSO 5.8: Melhor par para merge ===\n",
      "Par escolhido: ('gat', 'o</w>') (freq=2)\n",
      "Novo símbolo: 'gato</w>'\n",
      "\n",
      "=== PASSO 6.8: Vocabulário após merge ===\n",
      "O</w>  ->  3\n",
      "d es c e</w>  ->  2\n",
      "gato</w>  ->  2\n",
      "m es a</w>  ->  2\n",
      "s o b e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a r a n h a</w>  ->  1\n",
      "a</w>  ->  1\n",
      "c a c h o r r o</w>  ->  1\n",
      "d a</w>  ->  1\n",
      "n a</w>  ->  1\n",
      "n o</w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 4.9: Contagem de pares (top 10) ===\n",
      "('s', 'o') -> 2\n",
      "('o', 'b') -> 2\n",
      "('b', 'e</w>') -> 2\n",
      "('m', 'es') -> 2\n",
      "('es', 'a</w>') -> 2\n",
      "('a', 'r') -> 2\n",
      "('d', 'es') -> 2\n",
      "('es', 'c') -> 2\n",
      "('c', 'e</w>') -> 2\n",
      "('n', 'o</w>') -> 1\n",
      "\n",
      "=== PASSO 5.9: Melhor par para merge ===\n",
      "Par escolhido: ('s', 'o') (freq=2)\n",
      "Novo símbolo: 'so'\n",
      "\n",
      "=== PASSO 6.9: Vocabulário após merge ===\n",
      "O</w>  ->  3\n",
      "d es c e</w>  ->  2\n",
      "gato</w>  ->  2\n",
      "m es a</w>  ->  2\n",
      "so b e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a r a n h a</w>  ->  1\n",
      "a</w>  ->  1\n",
      "c a c h o r r o</w>  ->  1\n",
      "d a</w>  ->  1\n",
      "n a</w>  ->  1\n",
      "n o</w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 4.10: Contagem de pares (top 10) ===\n",
      "('so', 'b') -> 2\n",
      "('b', 'e</w>') -> 2\n",
      "('m', 'es') -> 2\n",
      "('es', 'a</w>') -> 2\n",
      "('a', 'r') -> 2\n",
      "('d', 'es') -> 2\n",
      "('es', 'c') -> 2\n",
      "('c', 'e</w>') -> 2\n",
      "('n', 'o</w>') -> 1\n",
      "('t', 'a') -> 1\n",
      "\n",
      "=== PASSO 5.10: Melhor par para merge ===\n",
      "Par escolhido: ('so', 'b') (freq=2)\n",
      "Novo símbolo: 'sob'\n",
      "\n",
      "=== PASSO 6.10: Vocabulário após merge ===\n",
      "O</w>  ->  3\n",
      "d es c e</w>  ->  2\n",
      "gato</w>  ->  2\n",
      "m es a</w>  ->  2\n",
      "sob e</w>  ->  2\n",
      "A </w>  ->  1\n",
      "a r a n h a</w>  ->  1\n",
      "a</w>  ->  1\n",
      "c a c h o r r o</w>  ->  1\n",
      "d a</w>  ->  1\n",
      "n a</w>  ->  1\n",
      "n o</w>  ->  1\n",
      "p a r e d e</w>  ->  1\n",
      "t a p e t e</w>  ->  1\n",
      "\n",
      "=== PASSO 7: Tokens finais (vocabulário de subpalavras) ===\n",
      "['</w>', 'A', 'O</w>', 'a', 'a</w>', 'c', 'd', 'e', 'e</w>', 'es', 'gato</w>', 'h', 'm', 'n', 'o', 'o</w>', 'p', 'r', 'sob', 't']\n",
      "\n",
      "=== PASSO 8: token_to_id (amostra) ===\n",
      "</w>       -> 0\n",
      "A          -> 1\n",
      "O</w>      -> 2\n",
      "a          -> 3\n",
      "a</w>      -> 4\n",
      "c          -> 5\n",
      "d          -> 6\n",
      "e          -> 7\n",
      "e</w>      -> 8\n",
      "es         -> 9\n",
      "gato</w>   -> 10\n",
      "h          -> 11\n",
      "m          -> 12\n",
      "n          -> 13\n",
      "o          -> 14\n",
      "o</w>      -> 15\n",
      "p          -> 16\n",
      "r          -> 17\n",
      "sob        -> 18\n",
      "t          -> 19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"O gato sobe no tapete.\",\n",
    "    \"O cachorro sobe na mesa.\",\n",
    "    \"A aranha desce a parede.\",\n",
    "    \"O gato desce da mesa.\",\n",
    "]\n",
    "\n",
    "bpe = BytePairEncoderDecoder(end_of_word=\"</w>\")\n",
    "\n",
    "# Treinar com poucas merges para ficar bem visível no passo a passo\n",
    "resultado = bpe.train(corpus=corpus, num_merges=10, verbose=True, top_pairs_to_show=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa2f29-2838-44a8-9f6a-f51a9ef9b445",
   "metadata": {},
   "source": [
    "## Exemplo de uso do encode (uma frase ainda não utilizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd08a99-666c-4fb2-9fcc-d750cde30a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENCODE (BPE) ===\n",
      "Texto: A aranha sobe no gato.\n",
      "\n",
      "Palavras: ['A', 'aranha', 'sobe', 'no', 'gato']\n",
      "\n",
      "--- Palavra: 'A' ---\n",
      "Símbolos iniciais: ('A', '</w>')\n",
      "Símbolos finais (após merges): ('A', '</w>')\n",
      "\n",
      "Tokens BPE: ('A', '</w>')\n",
      "IDs: [1, 0]\n",
      "\n",
      "--- Palavra: 'aranha' ---\n",
      "Símbolos iniciais: ('a', 'r', 'a', 'n', 'h', 'a', '</w>')\n",
      "Merge 2: ('a', '</w>') -> 'a</w>'\n",
      "Antes: ('a', 'r', 'a', 'n', 'h', 'a', '</w>')\n",
      "Depois: ('a', 'r', 'a', 'n', 'h', 'a</w>')\n",
      "\n",
      "Símbolos finais (após merges): ('a', 'r', 'a', 'n', 'h', 'a</w>')\n",
      "\n",
      "Tokens BPE: ('a', 'r', 'a', 'n', 'h', 'a</w>')\n",
      "IDs: [3, 17, 3, 13, 11, 4]\n",
      "\n",
      "--- Palavra: 'sobe' ---\n",
      "Símbolos iniciais: ('s', 'o', 'b', 'e', '</w>')\n",
      "Merge 1: ('e', '</w>') -> 'e</w>'\n",
      "Antes: ('s', 'o', 'b', 'e', '</w>')\n",
      "Depois: ('s', 'o', 'b', 'e</w>')\n",
      "\n",
      "Merge 9: ('s', 'o') -> 'so'\n",
      "Antes: ('s', 'o', 'b', 'e</w>')\n",
      "Depois: ('so', 'b', 'e</w>')\n",
      "\n",
      "Merge 10: ('so', 'b') -> 'sob'\n",
      "Antes: ('so', 'b', 'e</w>')\n",
      "Depois: ('sob', 'e</w>')\n",
      "\n",
      "Símbolos finais (após merges): ('sob', 'e</w>')\n",
      "\n",
      "Tokens BPE: ('sob', 'e</w>')\n",
      "IDs: [18, 8]\n",
      "\n",
      "--- Palavra: 'no' ---\n",
      "Símbolos iniciais: ('n', 'o', '</w>')\n",
      "Merge 3: ('o', '</w>') -> 'o</w>'\n",
      "Antes: ('n', 'o', '</w>')\n",
      "Depois: ('n', 'o</w>')\n",
      "\n",
      "Símbolos finais (após merges): ('n', 'o</w>')\n",
      "\n",
      "Tokens BPE: ('n', 'o</w>')\n",
      "IDs: [13, 15]\n",
      "\n",
      "--- Palavra: 'gato' ---\n",
      "Símbolos iniciais: ('g', 'a', 't', 'o', '</w>')\n",
      "Merge 3: ('o', '</w>') -> 'o</w>'\n",
      "Antes: ('g', 'a', 't', 'o', '</w>')\n",
      "Depois: ('g', 'a', 't', 'o</w>')\n",
      "\n",
      "Merge 6: ('g', 'a') -> 'ga'\n",
      "Antes: ('g', 'a', 't', 'o</w>')\n",
      "Depois: ('ga', 't', 'o</w>')\n",
      "\n",
      "Merge 7: ('ga', 't') -> 'gat'\n",
      "Antes: ('ga', 't', 'o</w>')\n",
      "Depois: ('gat', 'o</w>')\n",
      "\n",
      "Merge 8: ('gat', 'o</w>') -> 'gato</w>'\n",
      "Antes: ('gat', 'o</w>')\n",
      "Depois: ('gato</w>',)\n",
      "\n",
      "Símbolos finais (após merges): ('gato</w>',)\n",
      "\n",
      "Tokens BPE: ('gato</w>',)\n",
      "IDs: [10]\n",
      "\n",
      "=== SEQUÊNCIA FINAL DE IDS ===\n",
      "[1, 0, 3, 17, 3, 13, 11, 4, 18, 8, 13, 15, 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode: veja como palavras viram subpalavras/bytes (aqui: símbolos BPE)\n",
    "texto_novo = \"A aranha sobe no gato.\"\n",
    "ids = bpe.encode(texto_novo, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd7301-306f-4c8c-a564-5783e725203a",
   "metadata": {},
   "source": [
    "## Exemplo de uso do decode (Uma frase ainda não utilizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645892f6-8576-4c54-88ca-4c0a7e28bdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DECODE (BPE) ===\n",
      "IDs: [1, 0, 3, 17, 3, 13, 11, 4, 18, 8, 13, 15, 10]\n",
      "\n",
      "Tokens: ['A', '</w>', 'a', 'r', 'a', 'n', 'h', 'a</w>', 'sob', 'e</w>', 'n', 'o</w>', 'gato</w>']\n",
      "\n",
      "Texto reconstruído: A aranha</w>sobe</w>no</w>gato</w>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode: volta ao texto\n",
    "texto_reconstruido = bpe.decode(ids, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56366a67-ebd0-436d-8532-5671b8ab6c09",
   "metadata": {},
   "source": [
    "## Este método é naturalmente robusto a palavras desconhecidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3328d858-9934-4ad0-bce3-c390b43f2589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENCODE (BPE) ===\n",
      "Texto: A aranha sobe no rato.\n",
      "\n",
      "Palavras: ['A', 'aranha', 'sobe', 'no', 'rato']\n",
      "\n",
      "--- Palavra: 'A' ---\n",
      "Símbolos iniciais: ('A', '</w>')\n",
      "Símbolos finais (após merges): ('A', '</w>')\n",
      "\n",
      "Tokens BPE: ('A', '</w>')\n",
      "IDs: [1, 0]\n",
      "\n",
      "--- Palavra: 'aranha' ---\n",
      "Símbolos iniciais: ('a', 'r', 'a', 'n', 'h', 'a', '</w>')\n",
      "Merge 2: ('a', '</w>') -> 'a</w>'\n",
      "Antes: ('a', 'r', 'a', 'n', 'h', 'a', '</w>')\n",
      "Depois: ('a', 'r', 'a', 'n', 'h', 'a</w>')\n",
      "\n",
      "Símbolos finais (após merges): ('a', 'r', 'a', 'n', 'h', 'a</w>')\n",
      "\n",
      "Tokens BPE: ('a', 'r', 'a', 'n', 'h', 'a</w>')\n",
      "IDs: [3, 17, 3, 13, 11, 4]\n",
      "\n",
      "--- Palavra: 'sobe' ---\n",
      "Símbolos iniciais: ('s', 'o', 'b', 'e', '</w>')\n",
      "Merge 1: ('e', '</w>') -> 'e</w>'\n",
      "Antes: ('s', 'o', 'b', 'e', '</w>')\n",
      "Depois: ('s', 'o', 'b', 'e</w>')\n",
      "\n",
      "Merge 9: ('s', 'o') -> 'so'\n",
      "Antes: ('s', 'o', 'b', 'e</w>')\n",
      "Depois: ('so', 'b', 'e</w>')\n",
      "\n",
      "Merge 10: ('so', 'b') -> 'sob'\n",
      "Antes: ('so', 'b', 'e</w>')\n",
      "Depois: ('sob', 'e</w>')\n",
      "\n",
      "Símbolos finais (após merges): ('sob', 'e</w>')\n",
      "\n",
      "Tokens BPE: ('sob', 'e</w>')\n",
      "IDs: [18, 8]\n",
      "\n",
      "--- Palavra: 'no' ---\n",
      "Símbolos iniciais: ('n', 'o', '</w>')\n",
      "Merge 3: ('o', '</w>') -> 'o</w>'\n",
      "Antes: ('n', 'o', '</w>')\n",
      "Depois: ('n', 'o</w>')\n",
      "\n",
      "Símbolos finais (após merges): ('n', 'o</w>')\n",
      "\n",
      "Tokens BPE: ('n', 'o</w>')\n",
      "IDs: [13, 15]\n",
      "\n",
      "--- Palavra: 'rato' ---\n",
      "Símbolos iniciais: ('r', 'a', 't', 'o', '</w>')\n",
      "Merge 3: ('o', '</w>') -> 'o</w>'\n",
      "Antes: ('r', 'a', 't', 'o', '</w>')\n",
      "Depois: ('r', 'a', 't', 'o</w>')\n",
      "\n",
      "Símbolos finais (após merges): ('r', 'a', 't', 'o</w>')\n",
      "\n",
      "Tokens BPE: ('r', 'a', 't', 'o</w>')\n",
      "IDs: [17, 3, 19, 15]\n",
      "\n",
      "=== SEQUÊNCIA FINAL DE IDS ===\n",
      "[1, 0, 3, 17, 3, 13, 11, 4, 18, 8, 13, 15, 17, 3, 19, 15]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode: veja como palavras viram subpalavras/bytes (aqui: símbolos BPE)\n",
    "texto_novo = \"A aranha sobe no rato.\"\n",
    "ids = bpe.encode(texto_novo, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8d324-5ff7-4cf5-8543-5ce5c7c7ea6d",
   "metadata": {},
   "source": [
    "## Decode com a palavra desconhecida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b294d3c9-d1e2-4cde-832d-6f1ba29e655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DECODE (BPE) ===\n",
      "IDs: [1, 0, 3, 17, 3, 13, 11, 4, 18, 8, 13, 15, 17, 3, 19, 15]\n",
      "\n",
      "Tokens: ['A', '</w>', 'a', 'r', 'a', 'n', 'h', 'a</w>', 'sob', 'e</w>', 'n', 'o</w>', 'r', 'a', 't', 'o</w>']\n",
      "\n",
      "Texto reconstruído: A aranha</w>sobe</w>no</w>rato</w>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode: volta ao texto\n",
    "texto_reconstruido = bpe.decode(ids, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7774d-88c5-4f14-af28-659384d8450b",
   "metadata": {},
   "source": [
    "## O número de iterações influencia nos tokens gerados\n",
    "\n",
    "### Iterações = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de7e2aa-3e08-4cbe-82d4-3af0299be972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'</w>': 0,\n",
       " 'A': 1,\n",
       " 'O': 2,\n",
       " 'a': 3,\n",
       " 'b': 4,\n",
       " 'c': 5,\n",
       " 'd': 6,\n",
       " 'e': 7,\n",
       " 'e</w>': 8,\n",
       " 'g': 9,\n",
       " 'h': 10,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " 'o': 13,\n",
       " 'p': 14,\n",
       " 'r': 15,\n",
       " 's': 16,\n",
       " 't': 17}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"O gato sobe no tapete.\",\n",
    "    \"O cachorro sobe na mesa.\",\n",
    "    \"A aranha desce a parede.\",\n",
    "    \"O gato desce da mesa.\",\n",
    "]\n",
    "\n",
    "bpe = BytePairEncoderDecoder(end_of_word=\"</w>\")\n",
    "resultado = bpe.train(corpus=corpus, num_merges=1, verbose=False)\n",
    "resultado.token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3eca331-5d01-47f0-a3cc-bd1ebc531a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids=[2, 0, 9, 3, 17, 13, 0, 16, 13, 4, 8, 12, 13, 0, 17, 3, 14, 7, 17, 8, 2, 0, 5, 3, 5, 10, 13, 15, 15, 13, 0, 16, 13, 4, 8, 12, 3, 0, 11, 7, 16, 3, 0], len(ids)=43\n"
     ]
    }
   ],
   "source": [
    "texto_novo = \"O gato sobe no tapete O cachorro sobe na mesa\"\n",
    "ids = bpe.encode(texto_novo, verbose=False)\n",
    "print(f\"{ids=}, {len(ids)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482b868-4de4-4a2d-907f-a769ad6be571",
   "metadata": {},
   "source": [
    "### Iterações = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38e235c-2940-4819-95e1-c56f9c83c3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'</w>': 0,\n",
       " 'A': 1,\n",
       " 'O</w>': 2,\n",
       " 'a': 3,\n",
       " 'a</w>': 4,\n",
       " 'ar': 5,\n",
       " 'c': 6,\n",
       " 'd': 7,\n",
       " 'desce</w>': 8,\n",
       " 'e': 9,\n",
       " 'e</w>': 10,\n",
       " 'gato</w>': 11,\n",
       " 'h': 12,\n",
       " 'mesa</w>': 13,\n",
       " 'n': 14,\n",
       " 'no</w>': 15,\n",
       " 'o': 16,\n",
       " 'o</w>': 17,\n",
       " 'p': 18,\n",
       " 'r': 19,\n",
       " 'sobe</w>': 20,\n",
       " 't': 21,\n",
       " 'tap': 22}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe = BytePairEncoderDecoder(end_of_word=\"</w>\")\n",
    "resultado = bpe.train(corpus=corpus, num_merges=20, verbose=False)\n",
    "resultado.token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7003e641-4f11-46e8-817b-7d69ab3b68c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids=[2, 11, 20, 15, 22, 9, 21, 10, 2, 6, 3, 6, 12, 16, 19, 19, 17, 20, 14, 4, 13], len(ids)=21\n"
     ]
    }
   ],
   "source": [
    "texto_novo = \"O gato sobe no tapete O cachorro sobe na mesa\"\n",
    "ids = bpe.encode(texto_novo, verbose=False)\n",
    "print(f\"{ids=}, {len(ids)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233303a-77f1-422d-8813-6fb50fb256a5",
   "metadata": {},
   "source": [
    "### Iterações = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daaf997f-3a01-4123-9184-01a2718c5d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A</w>': 0,\n",
       " 'O</w>': 1,\n",
       " 'a</w>': 2,\n",
       " 'aranha</w>': 3,\n",
       " 'cachorro</w>': 4,\n",
       " 'd': 5,\n",
       " 'desce</w>': 6,\n",
       " 'gato</w>': 7,\n",
       " 'mesa</w>': 8,\n",
       " 'na</w>': 9,\n",
       " 'no</w>': 10,\n",
       " 'parede</w>': 11,\n",
       " 'sobe</w>': 12,\n",
       " 'tapete</w>': 13}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe = BytePairEncoderDecoder(end_of_word=\"</w>\")\n",
    "resultado = bpe.train(corpus=corpus, num_merges=40, verbose=False)\n",
    "resultado.token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8220f274-33be-4406-8dbf-4cb169c756d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids=[1, 7, 12, 10, 13, 1, 4, 12, 9, 8], len(ids)=10\n"
     ]
    }
   ],
   "source": [
    "texto_novo = \"O gato sobe no tapete O cachorro sobe na mesa\"\n",
    "ids = bpe.encode(texto_novo, verbose=False)\n",
    "print(f\"{ids=}, {len(ids)=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
