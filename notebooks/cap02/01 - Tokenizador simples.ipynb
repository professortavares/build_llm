{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tokenizador simples (Cap 2)\n",
    "\n",
    "**Descrição:**\n",
    "Um tokenizador é o componente responsável por transformar texto bruto em unidades menores chamadas *tokens*. Esses tokens podem ser palavras, subpalavras, caracteres ou símbolos especiais (como pontuação). Como modelos de linguagem não conseguem processar texto diretamente, o tokenizador atua como a ponte entre o texto humano e as representações numéricas que o modelo consegue entender. Esse processo é o primeiro passo fundamental no preparo dos dados para o treinamento e uso de modelos de linguagem de grande escala (LLMs). \n",
    "\n",
    "**Objetivo:**\n",
    "O principal objetivo de um tokenizador é padronizar e converter o texto em uma sequência estruturada de tokens que possa ser posteriormente mapeada para identificadores numéricos (*token IDs*) e, então, para vetores de embeddings. Dessa forma, o tokenizador viabiliza que o modelo aprenda padrões da linguagem, como estrutura, contexto e significado, sendo essencial para tarefas como geração de texto, classificação e compreensão de linguagem natural. \n",
    "\n",
    "**Funcionamento:**\n",
    "\n",
    "![Exemplo de funcionamento de um tokenizador](../../images/01_tokenizador_funcionamento.png)\n",
    "\n",
    "*Figura 1 — Exemplo do funcionamento de um tokenizador: texto → tokens → token IDs.*\n",
    "\n",
    "O tokenizador transforma texto em tokens e tokens em números, permitindo que o modelo de linguagem processe a informação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c04b8b-1320-47ef-a0bd-e33d06df1b98",
   "metadata": {},
   "source": [
    "## Implementação de um tokenizador simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6b829e-969f-4ed3-9d25-b8b2f64b8443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASSO 0: Texto original ===\n",
      "O tokenizador divide o texto.\n",
      "\n",
      "=== PASSO 1: Split bruto (inclui espaços e strings vazias) ===\n",
      "['O', ' ', 'tokenizador', ' ', 'divide', ' ', 'o', ' ', 'texto', '.', '']\n",
      "\n",
      "=== PASSO 2: Limpeza (remove espaços e itens vazios) ===\n",
      "['O', 'tokenizador', 'divide', 'o', 'texto', '.']\n",
      "\n",
      "=== PASSO 3: Construção do vocabulário (token -> id) ===\n",
      "Tokens únicos (ordenados): ['.', 'O', 'divide', 'o', 'texto', 'tokenizador']\n",
      "Vocabulário: {'.': 0, 'O': 1, 'divide': 2, 'o': 3, 'texto': 4, 'tokenizador': 5}\n",
      "\n",
      "=== PASSO 4: Conversão tokens -> token IDs ===\n",
      "Tokens: ['O', 'tokenizador', 'divide', 'o', 'texto', '.']\n",
      "Token IDs: [1, 5, 2, 3, 4, 0]\n",
      "\n",
      "['O', 'tokenizador', 'divide', 'o', 'texto', '.']\n",
      "[1, 5, 2, 3, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class TokenizationResult:\n",
    "    \"\"\"Estrutura para guardar resultados intermediários da tokenização.\"\"\"\n",
    "    raw_text: str\n",
    "    split_raw: List[str]\n",
    "    tokens: List[str]\n",
    "    vocab: Dict[str, int]\n",
    "    token_ids: List[int]\n",
    "\n",
    "\n",
    "def tokenizar_passo_a_passo(texto: str) -> TokenizationResult:\n",
    "    \"\"\"\n",
    "    Tokeniza um texto de forma didática, mostrando o passo a passo.\n",
    "\n",
    "    Estratégia (simples e didática):\n",
    "    - Separar palavras e pontuação com regex\n",
    "    - Remover itens vazios e espaços\n",
    "    - Construir vocabulário (token -> id) ordenando alfabeticamente\n",
    "    - Converter tokens em token IDs\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    texto : str\n",
    "        Texto a ser tokenizado.\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    TokenizationResult\n",
    "        Estrutura contendo todas as saídas intermediárias e finais.\n",
    "\n",
    "    Exceções:\n",
    "    --------\n",
    "    TypeError\n",
    "        Se `texto` não for string.\n",
    "    ValueError\n",
    "        Se `texto` for uma string vazia após remoção de espaços.\n",
    "    \"\"\"\n",
    "    if not isinstance(texto, str):\n",
    "        raise TypeError(\"O parâmetro `texto` deve ser uma string (str).\")\n",
    "\n",
    "    if not texto.strip():\n",
    "        raise ValueError(\"O parâmetro `texto` não pode ser vazio.\")\n",
    "\n",
    "    print(\"=== PASSO 0: Texto original ===\")\n",
    "    print(texto)\n",
    "    print()\n",
    "\n",
    "    # Regex didática:\n",
    "    # - Captura pontuação básica em um grupo separado: [,.!?;:]\n",
    "    # - Captura espaços (\\s) como separadores\n",
    "    padrao = r'([,.!?;:]|\\s)'\n",
    "\n",
    "    print(\"=== PASSO 1: Split bruto (inclui espaços e strings vazias) ===\")\n",
    "    split_raw = re.split(padrao, texto)\n",
    "    print(split_raw)\n",
    "    print()\n",
    "\n",
    "    print(\"=== PASSO 2: Limpeza (remove espaços e itens vazios) ===\")\n",
    "    # Mantemos apenas partes com conteúdo (strip remove espaços)\n",
    "    tokens = [p.strip() for p in split_raw if p.strip()]\n",
    "    print(tokens)\n",
    "    print()\n",
    "\n",
    "    print(\"=== PASSO 3: Construção do vocabulário (token -> id) ===\")\n",
    "    # Ordenar garante reprodutibilidade (didático)\n",
    "    tokens_unicos_ordenados = sorted(set(tokens))\n",
    "    vocab = {tok: i for i, tok in enumerate(tokens_unicos_ordenados)}\n",
    "    print(\"Tokens únicos (ordenados):\", tokens_unicos_ordenados)\n",
    "    print(\"Vocabulário:\", vocab)\n",
    "    print()\n",
    "\n",
    "    print(\"=== PASSO 4: Conversão tokens -> token IDs ===\")\n",
    "    token_ids = [vocab[tok] for tok in tokens]\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", token_ids)\n",
    "    print()\n",
    "\n",
    "    return TokenizationResult(\n",
    "        raw_text=texto,\n",
    "        split_raw=split_raw,\n",
    "        tokens=tokens,\n",
    "        vocab=vocab,\n",
    "        token_ids=token_ids,\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Exemplo de uso (pedido)\n",
    "# =========================\n",
    "texto_exemplo = \"O tokenizador divide o texto.\"\n",
    "resultado = tokenizar_passo_a_passo(texto_exemplo)\n",
    "\n",
    "print(resultado.tokens)\n",
    "print(resultado.token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01532d-9498-4395-9181-3c2526114ccc",
   "metadata": {},
   "source": [
    "### Análise do resultado:\n",
    "\n",
    "\n",
    "#### PASSO 0 — Texto original\n",
    "\n",
    "```text\n",
    "O tokenizador divide o texto.\n",
    "```\n",
    "\n",
    "Neste ponto temos o **texto bruto**, exatamente como escrito por um humano.\n",
    "Modelos de linguagem **não conseguem operar diretamente sobre texto**, então esse formato ainda não é utilizável pelo modelo.\n",
    "\n",
    "---\n",
    "\n",
    "#### PASSO 1 — Split bruto (inclui espaços e strings vazias)\n",
    "\n",
    "```python\n",
    "['O', ' ', 'tokenizador', ' ', 'divide', ' ', 'o', ' ', 'texto', '.', '']\n",
    "```\n",
    "\n",
    "**O que aconteceu aqui:**\n",
    "\n",
    "* O texto foi dividido usando uma expressão regular que separa:\n",
    "\n",
    "  * palavras\n",
    "  * espaços (`' '`)\n",
    "  * pontuação (`'.'`)\n",
    "* Os separadores capturados pela regex **também aparecem na lista**, o que é intencional neste passo didático.\n",
    "\n",
    "**Por que isso é importante?**\n",
    "\n",
    "* Esse resultado mostra claramente que a tokenização inicial é “suja”.\n",
    "* Espaços e strings vazias aparecem naturalmente e **precisam ser tratados**.\n",
    "* Ajuda a entender que tokenização não é apenas um `split()` simples.\n",
    "\n",
    "---\n",
    "\n",
    "#### PASSO 2 — Limpeza (remove espaços e itens vazios)\n",
    "\n",
    "```python\n",
    "['O', 'tokenizador', 'divide', 'o', 'texto', '.']\n",
    "```\n",
    "\n",
    "**O que foi feito:**\n",
    "\n",
    "* Removemos:\n",
    "\n",
    "  * espaços\n",
    "  * strings vazias\n",
    "* Mantivemos:\n",
    "\n",
    "  * palavras\n",
    "  * pontuação como token independente\n",
    "\n",
    "**Pontos importantes:**\n",
    "\n",
    "* A pontuação (`.`) é mantida como token próprio\n",
    "  → isso é fundamental em modelos de linguagem\n",
    "* A capitalização foi preservada:\n",
    "\n",
    "  * `\"O\"` ≠ `\"o\"`\n",
    "\n",
    "Esse é o **conjunto final de tokens** que representa o texto.\n",
    "\n",
    "---\n",
    "\n",
    "#### PASSO 3 — Construção do vocabulário (token → id)\n",
    "\n",
    "```python\n",
    "Tokens únicos (ordenados): ['.', 'O', 'divide', 'o', 'texto', 'tokenizador']\n",
    "Vocabulário: {\n",
    "    '.': 0,\n",
    "    'O': 1,\n",
    "    'divide': 2,\n",
    "    'o': 3,\n",
    "    'texto': 4,\n",
    "    'tokenizador': 5\n",
    "}\n",
    "```\n",
    "\n",
    "**O que é o vocabulário:**\n",
    "\n",
    "* Um mapeamento entre cada token único e um número inteiro\n",
    "* Cada token recebe um **ID fixo**\n",
    "\n",
    "**Por que ordenar alfabeticamente?**\n",
    "\n",
    "* Garante reprodutibilidade\n",
    "* Facilita entendimento didático\n",
    "* Em implementações reais, a ordem costuma ser baseada em frequência\n",
    "\n",
    "**Observações didáticas importantes:**\n",
    "\n",
    "* `\"O\"` e `\"o\"` são tokens diferentes\n",
    "* A pontuação faz parte do vocabulário\n",
    "* O modelo não “entende” palavras — ele só vê números\n",
    "\n",
    "---\n",
    "\n",
    "#### PASSO 4 — Conversão de tokens para token IDs\n",
    "\n",
    "```python\n",
    "Tokens:    ['O', 'tokenizador', 'divide', 'o', 'texto', '.']\n",
    "Token IDs: [1, 5, 2, 3, 4, 0]\n",
    "```\n",
    "\n",
    "**O que aconteceu:**\n",
    "\n",
    "* Cada token foi substituído pelo seu ID correspondente no vocabulário\n",
    "* A **ordem original do texto foi preservada**\n",
    "\n",
    "**Resultado final:**\n",
    "\n",
    "* O texto agora está em um formato **totalmente numérico**\n",
    "* Esse é o formato que pode ser:\n",
    "\n",
    "  * convertido em embeddings\n",
    "  * usado como entrada em um modelo de linguagem\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusão\n",
    "\n",
    "Esse exemplo mostra claramente que:\n",
    "\n",
    "1. **Tokenizar não é apenas separar palavras**\n",
    "2. O processo envolve:\n",
    "\n",
    "   * separação\n",
    "   * limpeza\n",
    "   * construção de vocabulário\n",
    "   * mapeamento para números\n",
    "3. O modelo de linguagem:\n",
    "\n",
    "   * **não vê texto**\n",
    "   * **não vê palavras**\n",
    "   * vê apenas **sequências de números**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f1766a-8ccf-447e-bae0-f131c870fb52",
   "metadata": {},
   "source": [
    "## Uma nova execução (com outro texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6234057-7d0b-471d-bf51-b90f63ec093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASSO 0: Texto original ===\n",
      "O tokenizador divide em pedaços menores.\n",
      "\n",
      "=== PASSO 1: Split bruto (inclui espaços e strings vazias) ===\n",
      "['O', ' ', 'tokenizador', ' ', 'divide', ' ', 'em', ' ', 'pedaços', ' ', 'menores', '.', '']\n",
      "\n",
      "=== PASSO 2: Limpeza (remove espaços e itens vazios) ===\n",
      "['O', 'tokenizador', 'divide', 'em', 'pedaços', 'menores', '.']\n",
      "\n",
      "=== PASSO 3: Construção do vocabulário (token -> id) ===\n",
      "Tokens únicos (ordenados): ['.', 'O', 'divide', 'em', 'menores', 'pedaços', 'tokenizador']\n",
      "Vocabulário: {'.': 0, 'O': 1, 'divide': 2, 'em': 3, 'menores': 4, 'pedaços': 5, 'tokenizador': 6}\n",
      "\n",
      "=== PASSO 4: Conversão tokens -> token IDs ===\n",
      "Tokens: ['O', 'tokenizador', 'divide', 'em', 'pedaços', 'menores', '.']\n",
      "Token IDs: [1, 6, 2, 3, 5, 4, 0]\n",
      "\n",
      "['O', 'tokenizador', 'divide', 'em', 'pedaços', 'menores', '.']\n",
      "[1, 6, 2, 3, 5, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "texto_exemplo = \"O tokenizador divide em pedaços menores.\"\n",
    "resultado = tokenizar_passo_a_passo(texto_exemplo)\n",
    "\n",
    "print(resultado.tokens)\n",
    "print(resultado.token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8352ecb-8f8d-4ed2-abda-2d3d68be29fe",
   "metadata": {},
   "source": [
    "**Observação:** \n",
    "\n",
    "Como podemos verificar na execução acima: O tokenizador é determinístico para um dado texto, mas seus token IDs só fazem sentido dentro do vocabulário construído."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c1b5f-54fc-4eca-8f91-63a36145ab35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
