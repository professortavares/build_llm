{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d363e92",
   "metadata": {},
   "source": [
    "# Notebook 11 — Pipeline com **token positions** (token embeddings + positional embeddings)\n",
    "\n",
    "No *Notebook 10*, vimos que uma representação de frase baseada em **média dos embeddings** (mean pooling) vira **bag-of-words**:  \n",
    "frases com os mesmos tokens em ordens diferentes ficam praticamente indistinguíveis.\n",
    "\n",
    "Aqui vamos **estender o pipeline** para incluir **positional embeddings** (como em GPT):\n",
    "\n",
    "1. texto → tokens (`tiktoken`)\n",
    "2. tokens → IDs\n",
    "3. IDs → **token embeddings** (`nn.Embedding`)\n",
    "4. posições → **positional embeddings** (`pos_embedding_layer`)\n",
    "5. **soma**: `token_emb + pos_emb` → `input_embeddings`\n",
    "\n",
    "E, por fim, vamos usar um agregador simples que **realmente explora** essas posições para diferenciar:\n",
    "\n",
    "- **A**: \"O gato sobe no tapete.\"\n",
    "- **B**: \"O tapete sobe no gato.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb1116",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47fb2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import subprocess\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10a75f",
   "metadata": {},
   "source": [
    "## Verificação da presença de GPU / device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019db79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d1e9c1",
   "metadata": {},
   "source": [
    "## Fixando seed para reproducibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86f286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int = 42) -> None:\n",
    "    \"\"\"Define seeds para resultados reprodutíveis (CPU e GPU).\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_all(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83910dca",
   "metadata": {},
   "source": [
    "## 1) Frases de exemplo + tokenização (`tiktoken`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a20c125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tokens: ['O', ' g', 'ato', ' so', 'be', ' no', ' tap', 'ete', '.']\n",
      "A raw_ids: [46, 308, 5549, 523, 1350, 645, 9814, 14471, 13]\n",
      "\n",
      "B tokens: ['O', ' tap', 'ete', ' so', 'be', ' no', ' g', 'ato', '.']\n",
      "B raw_ids: [46, 9814, 14471, 523, 1350, 645, 308, 5549, 13]\n",
      "\n",
      "Mesmos raw_ids (multiconjunto)? True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "frase_A = \"O gato sobe no tapete.\"\n",
    "frase_B = \"O tapete sobe no gato.\"\n",
    "\n",
    "def tokenize_with_tiktoken(text: str) -> Tuple[List[int], List[str]]:\n",
    "    \"\"\"Tokeniza via tiktoken retornando (raw_ids, tokens_str).\"\"\"\n",
    "    raw_ids = tokenizer.encode(text)\n",
    "    tokens_str = [tokenizer.decode([i]) for i in raw_ids]\n",
    "    return raw_ids, tokens_str\n",
    "\n",
    "raw_ids_A, tokens_A = tokenize_with_tiktoken(frase_A)\n",
    "raw_ids_B, tokens_B = tokenize_with_tiktoken(frase_B)\n",
    "\n",
    "print(\"A tokens:\", tokens_A)\n",
    "print(\"A raw_ids:\", raw_ids_A)\n",
    "print()\n",
    "print(\"B tokens:\", tokens_B)\n",
    "print(\"B raw_ids:\", raw_ids_B)\n",
    "print()\n",
    "print(\"Mesmos raw_ids (multiconjunto)?\", sorted(raw_ids_A) == sorted(raw_ids_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d40c5",
   "metadata": {},
   "source": [
    "## 2) Vocabulário compacto + batch com padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a2a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_ids.shape: torch.Size([2, 9])\n",
      "tensor([[2, 3, 7, 4, 6, 5, 8, 9, 1],\n",
      "        [2, 8, 9, 4, 6, 5, 3, 7, 1]])\n",
      "\n",
      "vocab_size_compacto (inclui PAD): 10\n"
     ]
    }
   ],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "def build_compact_vocab(raw_id_lists: List[List[int]]) -> Tuple[Dict[int, int], Dict[int, int]]:\n",
    "    \"\"\"Cria um mapeamento raw_id -> new_id (vocabulário compacto) com um token <PAD>.\"\"\"\n",
    "    uniq_raw_ids = sorted({rid for lst in raw_id_lists for rid in lst})\n",
    "    # new_id 0 reservado para PAD\n",
    "    raw_id_to_new: Dict[int, int] = {rid: idx + 1 for idx, rid in enumerate(uniq_raw_ids)}\n",
    "    new_to_raw_id: Dict[int, int] = {v: k for k, v in raw_id_to_new.items()}\n",
    "    return raw_id_to_new, new_to_raw_id\n",
    "\n",
    "raw_id_to_new, new_to_raw_id = build_compact_vocab([raw_ids_A, raw_ids_B])\n",
    "pad_new_id = 0\n",
    "\n",
    "def remap_and_pad(raw_ids: List[int], *, raw_id_to_new: Dict[int, int], max_len: int, pad_id: int = 0) -> List[int]:\n",
    "    \"\"\"Remapeia raw_ids -> new_ids e aplica padding até max_len.\"\"\"\n",
    "    new_ids = [raw_id_to_new[rid] for rid in raw_ids]\n",
    "    if len(new_ids) > max_len:\n",
    "        raise ValueError(f\"Sequência maior que max_len ({len(new_ids)} > {max_len}).\")\n",
    "    return new_ids + [pad_id] * (max_len - len(new_ids))\n",
    "\n",
    "max_len = max(len(raw_ids_A), len(raw_ids_B))\n",
    "\n",
    "new_ids_A = remap_and_pad(raw_ids_A, raw_id_to_new=raw_id_to_new, max_len=max_len, pad_id=pad_new_id)\n",
    "new_ids_B = remap_and_pad(raw_ids_B, raw_id_to_new=raw_id_to_new, max_len=max_len, pad_id=pad_new_id)\n",
    "\n",
    "batch_ids = torch.tensor([new_ids_A, new_ids_B], dtype=torch.long, device=device)  # [B, T]\n",
    "print(\"batch_ids.shape:\", batch_ids.shape)\n",
    "print(batch_ids)\n",
    "print()\n",
    "print(\"vocab_size_compacto (inclui PAD):\", max(raw_id_to_new.values()) + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f908698",
   "metadata": {},
   "source": [
    "## 3) Token embeddings (`nn.Embedding`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57fc85d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embeddings.shape: torch.Size([2, 9, 32])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = int(max(raw_id_to_new.values()) + 1)  # + PAD(0)\n",
    "emb_dim = 32\n",
    "\n",
    "token_embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim).to(device)\n",
    "\n",
    "token_embeddings = token_embedding_layer(batch_ids)  # [B, T, D]\n",
    "print(\"token_embeddings.shape:\", token_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe26061",
   "metadata": {},
   "source": [
    "## 4) Positional embeddings (`pos_embedding_layer`) + input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69e5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_embeddings.shape: torch.Size([2, 9, 32])\n",
      "input_embeddings.shape: torch.Size([2, 9, 32])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_len  # neste notebook, o \"contexto\" é o tamanho do batch após padding\n",
    "\n",
    "pos_embedding_layer = nn.Embedding(num_embeddings=context_length, embedding_dim=emb_dim).to(device)\n",
    "\n",
    "# IDs de posição: 0..T-1 para cada item do batch\n",
    "B, T = batch_ids.shape\n",
    "pos_ids = torch.arange(T, device=device).unsqueeze(0).expand(B, T)  # [B, T]\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(pos_ids)  # [B, T, D]\n",
    "print(\"pos_embeddings.shape:\", pos_embeddings.shape)\n",
    "\n",
    "# Input embeddings no estilo GPT: soma token_emb + pos_emb\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"input_embeddings.shape:\", input_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7eca4",
   "metadata": {},
   "source": [
    "## 5) Importante: mean pooling continua **sem ordem** (mesmo com pos embeddings)\n",
    "\n",
    "Se você fizer `mean(input_embeddings, dim=1)`, você terá:\n",
    "\n",
    "\\\\[ \\\\text{mean}(\\\\,tok_i + pos_i\\\\,) = \\\\text{mean}(tok_i) + \\\\text{mean}(pos_i) \\\\]\\n\\nComo o termo `mean(pos_i)` depende só do comprimento **T** (e não da ordem dos tokens),\n",
    "ele será igual para A e B quando ambas tiverem o mesmo tamanho.\n",
    "\n",
    "Abaixo confirmamos isso numericamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbff274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine(token_mean(A), token_mean(B)) : 1.0\n",
      "cosine(input_mean(A), input_mean(B)) : 1.0000001192092896\n",
      "(esperado: ~1.0 em ambos os casos)\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def cosine_similarity(u: torch.Tensor, v: torch.Tensor, eps: float = 1e-12) -> float:\n",
    "    \"\"\"Similaridade cosseno entre vetores 1D (torch).\"\"\"\n",
    "    u = u.flatten()\n",
    "    v = v.flatten()\n",
    "    num = torch.dot(u, v)\n",
    "    den = (u.norm() * v.norm()).clamp_min(eps)\n",
    "    return float((num / den).cpu().item())\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_pool(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Mean pooling em [B, T, D] -> [B, D].\"\"\"\n",
    "    return x.mean(dim=1)\n",
    "\n",
    "sent_tok_mean = mean_pool(token_embeddings)          # [B, D]\n",
    "sent_inp_mean = mean_pool(input_embeddings)          # [B, D]\n",
    "\n",
    "sim_tok_mean = cosine_similarity(sent_tok_mean[0], sent_tok_mean[1])\n",
    "sim_inp_mean = cosine_similarity(sent_inp_mean[0], sent_inp_mean[1])\n",
    "\n",
    "print(\"cosine(token_mean(A), token_mean(B)) :\", sim_tok_mean)\n",
    "print(\"cosine(input_mean(A), input_mean(B)) :\", sim_inp_mean)\n",
    "print(\"(esperado: ~1.0 em ambos os casos)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131209cc",
   "metadata": {},
   "source": [
    "## 6) Um agregador simples que *usa* posições (para diferenciar A vs B)\n",
    "\n",
    "A correção não é \"só somar pos embeddings\" — você também precisa de um módulo que\n",
    "**não colapse** a sequência de forma puramente linear/permutação-invariante.\n",
    "\n",
    "A seguir implementamos um *pooling com pesos* (softmax) onde o score de cada posição\n",
    "depende do vetor `token_emb + pos_emb`. Como os *pos embeddings* mudam de acordo com a posição,\n",
    "a associação *token↔posição* afeta o resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adfc9869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine(sent_sem_pos(A), sent_sem_pos(B))  : 1.0\n",
      "cosine(sent_com_pos(A), sent_com_pos(B))  : 0.9894437193870544\n",
      "\n",
      "max |A - B| (sem pos): 5.960464477539063e-08\n",
      "max |A - B| (com pos): 0.23108315467834473\n"
     ]
    }
   ],
   "source": [
    "class SimplePositionalSentenceEncoder(nn.Module):\n",
    "    \"\"\"Encoder mínimo para demonstrar o papel das posições.\n",
    "\n",
    "    Pipeline (estilo GPT):\n",
    "        input_ids -> token_emb -> pos_emb -> soma -> input_embeddings\n",
    "\n",
    "    Em seguida, produz um vetor de frase via pooling ponderado:\n",
    "        weights = softmax(score(input_embeddings))\n",
    "        sent_vec = sum(weights * input_embeddings)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, context_length: int, emb_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(context_length, emb_dim)\n",
    "\n",
    "        # Um scorer pequeno e não-linear (ordem importa pois pos_emb altera o input)\n",
    "        self.score = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(emb_dim, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, *, use_positional_embeddings: bool = True) -> dict:\n",
    "        if input_ids.ndim != 2:\n",
    "            raise ValueError(\"input_ids deve ter shape [B, T].\")\n",
    "\n",
    "        B, T = input_ids.shape\n",
    "\n",
    "        tok = self.tok_emb(input_ids)  # [B, T, D]\n",
    "\n",
    "        if use_positional_embeddings:\n",
    "            pos_ids = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
    "            pos = self.pos_emb(pos_ids)  # [B, T, D]\n",
    "        else:\n",
    "            pos = torch.zeros_like(tok)\n",
    "\n",
    "        x = tok + pos  # [B, T, D]\n",
    "\n",
    "        scores = self.score(x).squeeze(-1)      # [B, T]\n",
    "        weights = torch.softmax(scores, dim=-1) # [B, T]\n",
    "        sent = torch.sum(weights.unsqueeze(-1) * x, dim=1)  # [B, D]\n",
    "\n",
    "        return {\n",
    "            \"tok\": tok,\n",
    "            \"pos\": pos,\n",
    "            \"x\": x,\n",
    "            \"weights\": weights,\n",
    "            \"sent\": sent,\n",
    "        }\n",
    "\n",
    "seed_all(123)\n",
    "encoder = SimplePositionalSentenceEncoder(vocab_size=vocab_size, context_length=context_length, emb_dim=emb_dim).to(device)\n",
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_no_pos = encoder(batch_ids, use_positional_embeddings=False)\n",
    "    out_with_pos = encoder(batch_ids, use_positional_embeddings=True)\n",
    "\n",
    "sim_no_pos = cosine_similarity(out_no_pos[\"sent\"][0], out_no_pos[\"sent\"][1])\n",
    "sim_with_pos = cosine_similarity(out_with_pos[\"sent\"][0], out_with_pos[\"sent\"][1])\n",
    "\n",
    "print(\"cosine(sent_sem_pos(A), sent_sem_pos(B))  :\", sim_no_pos)\n",
    "print(\"cosine(sent_com_pos(A), sent_com_pos(B))  :\", sim_with_pos)\n",
    "\n",
    "# Um indicador simples de diferença componente-a-componente\n",
    "delta_no_pos = float((out_no_pos[\"sent\"][0] - out_no_pos[\"sent\"][1]).abs().max().cpu().item())\n",
    "delta_with_pos = float((out_with_pos[\"sent\"][0] - out_with_pos[\"sent\"][1]).abs().max().cpu().item())\n",
    "\n",
    "print(\"\\nmax |A - B| (sem pos):\", delta_no_pos)\n",
    "print(\"max |A - B| (com pos):\", delta_with_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281f0bd",
   "metadata": {},
   "source": [
    "## 7) Inspecionando os pesos por posição (intuído)\n",
    "\n",
    "Os pesos abaixo mostram como o agregador distribui atenção ao longo das posições.\n",
    "Como `pos_emb` muda o vetor em cada posição, a permutação entre **gato** e **tapete**\n",
    "altera os *scores* e, consequentemente, o vetor de frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be03f924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: O gato sobe no tapete.\n",
      "[('O', 0.08797570317983627), (' g', 0.13124318420886993), ('ato', 0.10881079733371735), (' so', 0.088450588285923), ('be', 0.1148715615272522), (' no', 0.10215062648057938), (' tap', 0.07443861663341522), ('ete', 0.14828598499298096), ('.', 0.1437729448080063)]\n",
      "\n",
      "B: O tapete sobe no gato.\n",
      "[('O', 0.08625047653913498), (' tap', 0.10123278200626373), ('ete', 0.1306297481060028), (' so', 0.08671604841947556), ('be', 0.11261890828609467), (' no', 0.10014741867780685), (' g', 0.11311985552310944), ('ato', 0.1283312290906906), ('.', 0.14095352590084076)]\n"
     ]
    }
   ],
   "source": [
    "def format_weights(weights: torch.Tensor, tokens: List[str]) -> List[Tuple[str, float]]:\n",
    "    weights = weights.detach().cpu().numpy().tolist()\n",
    "    return [(t, float(w)) for t, w in zip(tokens, weights)]\n",
    "\n",
    "# tokens (para exibir) - lembrando que estamos mostrando tokens do tiktoken,\n",
    "# mas o encoder está recebendo o vocabulário compacto.\n",
    "print(\"A:\", frase_A)\n",
    "print(format_weights(out_with_pos[\"weights\"][0], tokens_A))\n",
    "print()\n",
    "print(\"B:\", frase_B)\n",
    "print(format_weights(out_with_pos[\"weights\"][1], tokens_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0cc0c",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "- **Token embeddings** sozinhos não carregam ordem.\n",
    "- **Positional embeddings** permitem injetar informação de posição.\n",
    "- Porém, para a ordem impactar a representação final, é preciso um módulo que\n",
    "  *use* essas posições (por exemplo: atenção, convoluções, MLP sobre sequência etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
