{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620632ec-5859-4ece-b06c-b579c15e40c1",
   "metadata": {},
   "source": [
    "# Encoder-Decoder melhorado (Cap 2)\n",
    "\n",
    "**Descrição:**\n",
    "Este notebook apresenta uma versão **melhorada e didática** do modelo encoder–decoder, incorporando conceitos fundamentais do processamento de texto usados em modelos de linguagem modernos. Além da tokenização básica, o modelo passa a lidar explicitamente com **palavras desconhecidas** e **delimitação de sequências**, utilizando tokens especiais como `<|unk|>` e `<|endoftext|>`, conforme discutido no Capítulo 2 do livro *Build a Large Language Model (From Scratch)*.\n",
    "\n",
    "**Objetivo:**\n",
    "Demonstrar, de forma clara e passo a passo, como evoluir um encoder–decoder simples para um pipeline mais robusto de preparação de texto, capaz de:\n",
    "\n",
    "* Construir um vocabulário a partir de um corpus\n",
    "* Representar palavras desconhecidas de forma controlada\n",
    "* Marcar explicitamente o final de uma frase\n",
    "* Converter texto em tokens numéricos (encode)\n",
    "* Reconstruir texto a partir desses tokens (decode)\n",
    "\n",
    "O objetivo é aproximar o leitor das práticas reais usadas em LLMs, mantendo a simplicidade conceitual.\n",
    "\n",
    "**Funcionamento:**\n",
    "\n",
    "![Encoder-decoder melhorado](../../imagens/03_encode_decode_melhorado.png)\n",
    "\n",
    "O funcionamento do encoder–decoder melhorado segue as etapas abaixo:\n",
    "\n",
    "1. **Construção do corpus:**\n",
    "   Um conjunto de frases é tokenizado e usado para criar um vocabulário global. Durante essa etapa, o token especial `<|endoftext|>` é adicionado ao final de cada frase, e o token `<|unk|>` é garantido no vocabulário.\n",
    "\n",
    "2. **Encode (codificação):**\n",
    "   Um novo texto é tokenizado e convertido em uma sequência de IDs numéricos.\n",
    "\n",
    "   * Tokens presentes no vocabulário são mapeados diretamente.\n",
    "   * Tokens desconhecidos são substituídos por `<|unk|>`.\n",
    "   * O token `<|endoftext|>` é adicionado para indicar o fim da sequência.\n",
    "\n",
    "3. **Decode (decodificação):**\n",
    "   A sequência de token IDs é convertida de volta para tokens textuais e reconstruída em texto legível, ignorando o marcador de fim de texto.\n",
    "\n",
    "Esse fluxo representa uma versão simplificada, porém conceitualmente fiel, do pré-processamento de texto utilizado em modelos de linguagem baseados em transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a96665-05a0-419f-942c-388587f915c3",
   "metadata": {},
   "source": [
    "## Implementação de um encoder-decoder melhorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46bbe65-8964-47b9-9ebc-ace9299dfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections.abc import Iterable\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EncoderDecoderWithSpecialTokens:\n",
    "    \"\"\"\n",
    "    Encoder-Decoder didático com tokens especiais:\n",
    "    - <|unk|>        : palavra desconhecida\n",
    "    - <|endoftext|>  : fim de frase\n",
    "    \"\"\"\n",
    "\n",
    "    padrao: str = r\"([,.!?;:]|\\s)\"\n",
    "\n",
    "    # Tokens especiais (seguindo o livro)\n",
    "    UNK_TOKEN: str = \"<|unk|>\"\n",
    "    EOS_TOKEN: str = \"<|endoftext|>\"\n",
    "\n",
    "    vocab: dict[str, int] = field(default_factory=dict)\n",
    "    inv_vocab: dict[int, str] = field(default_factory=dict)\n",
    "\n",
    "    # ==========================\n",
    "    # Tokenização interna\n",
    "    # ==========================\n",
    "    def _tokenizar(self, texto: str) -> list[str]:\n",
    "        split_raw = re.split(self.padrao, texto)\n",
    "        tokens = [p.strip() for p in split_raw if p.strip()]\n",
    "        return tokens\n",
    "\n",
    "    # ==========================\n",
    "    # 1) Gerar corpus\n",
    "    # ==========================\n",
    "    def gerar_corpus(self, frases: Iterable[str]) -> None:\n",
    "        print(\"=== PASSO 1: Construção do corpus ===\")\n",
    "\n",
    "        todas_tokens: list[str] = []\n",
    "\n",
    "        for i, frase in enumerate(frases, start=1):\n",
    "            print(f\"\\nFrase {i}: {frase}\")\n",
    "            tokens = self._tokenizar(frase)\n",
    "            tokens.append(self.EOS_TOKEN)  # marca fim da frase\n",
    "            print(\"Tokens + <|endoftext|>:\", tokens)\n",
    "            todas_tokens.extend(tokens)\n",
    "\n",
    "        print(\"\\n=== PASSO 2: Tokens coletados ===\")\n",
    "        print(todas_tokens)\n",
    "\n",
    "        print(\"\\n=== PASSO 3: Construção do vocabulário ===\")\n",
    "\n",
    "        tokens_unicos = sorted(set(todas_tokens))\n",
    "\n",
    "        # Garantir tokens especiais\n",
    "        if self.UNK_TOKEN not in tokens_unicos:\n",
    "            tokens_unicos.append(self.UNK_TOKEN)\n",
    "\n",
    "        tokens_unicos = sorted(tokens_unicos)\n",
    "\n",
    "        self.vocab = {tok: i for i, tok in enumerate(tokens_unicos)}\n",
    "        self.inv_vocab = {i: tok for tok, i in self.vocab.items()}\n",
    "\n",
    "        print(\"Vocabulário final:\")\n",
    "        for tok, idx in self.vocab.items():\n",
    "            print(f\"{tok:15s} -> {idx}\")\n",
    "\n",
    "    # ==========================\n",
    "    # 2) Encode\n",
    "    # ==========================\n",
    "    def encode(self, texto: str) -> list[int]:\n",
    "        if not self.vocab:\n",
    "            raise ValueError(\"Vocabulário não criado. Execute gerar_corpus().\")\n",
    "\n",
    "        print(\"\\n=== ENCODE ===\")\n",
    "        print(\"Texto:\", texto)\n",
    "\n",
    "        tokens = self._tokenizar(texto)\n",
    "        tokens.append(self.EOS_TOKEN)\n",
    "\n",
    "        print(\"Tokens identificados:\", tokens)\n",
    "\n",
    "        token_ids = []\n",
    "        for tok in tokens:\n",
    "            if tok in self.vocab:\n",
    "                token_ids.append(self.vocab[tok])\n",
    "            else:\n",
    "                print(f\"Token desconhecido encontrado: '{tok}' -> <|unk|>\")\n",
    "                token_ids.append(self.vocab[self.UNK_TOKEN])\n",
    "\n",
    "        print(\"Token IDs:\", token_ids)\n",
    "        return token_ids\n",
    "\n",
    "    # ==========================\n",
    "    # 3) Decode\n",
    "    # ==========================\n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        if not self.inv_vocab:\n",
    "            raise ValueError(\"Vocabulário não criado.\")\n",
    "\n",
    "        print(\"\\n=== DECODE ===\")\n",
    "        print(\"Token IDs:\", token_ids)\n",
    "\n",
    "        tokens = [self.inv_vocab[i] for i in token_ids]\n",
    "        print(\"Tokens:\", tokens)\n",
    "\n",
    "        # Remove <|endoftext|> da reconstrução\n",
    "        tokens = [t for t in tokens if t != self.EOS_TOKEN]\n",
    "\n",
    "        texto = \" \".join(tokens)\n",
    "        texto = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", texto)\n",
    "\n",
    "        print(\"Texto decodificado:\", texto)\n",
    "        return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475dc9aa-7ef0-48c7-92ab-028a60e32127",
   "metadata": {},
   "source": [
    "## Geração do corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e25ef03-4b46-45a7-b999-03b9359075f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASSO 1: Construção do corpus ===\n",
      "\n",
      "Frase 1: O gato sobe no tapete.\n",
      "Tokens + <|endoftext|>: ['O', 'gato', 'sobe', 'no', 'tapete', '.', '<|endoftext|>']\n",
      "\n",
      "Frase 2: O cachorro sobe na mesa.\n",
      "Tokens + <|endoftext|>: ['O', 'cachorro', 'sobe', 'na', 'mesa', '.', '<|endoftext|>']\n",
      "\n",
      "Frase 3: A aranha desce a parede.\n",
      "Tokens + <|endoftext|>: ['A', 'aranha', 'desce', 'a', 'parede', '.', '<|endoftext|>']\n",
      "\n",
      "Frase 4: O gato desce da mesa.\n",
      "Tokens + <|endoftext|>: ['O', 'gato', 'desce', 'da', 'mesa', '.', '<|endoftext|>']\n",
      "\n",
      "=== PASSO 2: Tokens coletados ===\n",
      "['O', 'gato', 'sobe', 'no', 'tapete', '.', '<|endoftext|>', 'O', 'cachorro', 'sobe', 'na', 'mesa', '.', '<|endoftext|>', 'A', 'aranha', 'desce', 'a', 'parede', '.', '<|endoftext|>', 'O', 'gato', 'desce', 'da', 'mesa', '.', '<|endoftext|>']\n",
      "\n",
      "=== PASSO 3: Construção do vocabulário ===\n",
      "Vocabulário final:\n",
      ".               -> 0\n",
      "<|endoftext|>   -> 1\n",
      "<|unk|>         -> 2\n",
      "A               -> 3\n",
      "O               -> 4\n",
      "a               -> 5\n",
      "aranha          -> 6\n",
      "cachorro        -> 7\n",
      "da              -> 8\n",
      "desce           -> 9\n",
      "gato            -> 10\n",
      "mesa            -> 11\n",
      "na              -> 12\n",
      "no              -> 13\n",
      "parede          -> 14\n",
      "sobe            -> 15\n",
      "tapete          -> 16\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"O gato sobe no tapete.\",\n",
    "    \"O cachorro sobe na mesa.\",\n",
    "    \"A aranha desce a parede.\",\n",
    "    \"O gato desce da mesa.\",\n",
    "]\n",
    "\n",
    "ed = EncoderDecoderWithSpecialTokens()\n",
    "ed.gerar_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31008bdf-404a-4984-ad29-352d1ce5db53",
   "metadata": {},
   "source": [
    "## Exemplo de uso do encode (com uma palavra desconhecida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1286279-b828-4696-8900-de6d3dc0af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ENCODE ===\n",
      "Texto: O rato sobe na mesa.\n",
      "Tokens identificados: ['O', 'rato', 'sobe', 'na', 'mesa', '.', '<|endoftext|>']\n",
      "Token desconhecido encontrado: 'rato' -> <|unk|>\n",
      "Token IDs: [4, 2, 15, 12, 11, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "texto_novo = \"O rato sobe na mesa.\"\n",
    "token_ids = ed.encode(texto_novo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4e009-8c46-4c04-a341-19c8393b4c58",
   "metadata": {},
   "source": [
    "## Exemplo do decode (com a utilização do <|unk|> e <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7505fce-b6e3-4357-9e30-ff88c59e286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DECODE ===\n",
      "Token IDs: [4, 2, 15, 12, 11, 0, 1]\n",
      "Tokens: ['O', '<|unk|>', 'sobe', 'na', 'mesa', '.', '<|endoftext|>']\n",
      "Texto decodificado: O <|unk|> sobe na mesa.\n"
     ]
    }
   ],
   "source": [
    "texto_recuperado = ed.decode(token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
