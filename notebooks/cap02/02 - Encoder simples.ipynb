{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9314316-139b-453c-80e7-f974b64ee1c3",
   "metadata": {},
   "source": [
    "# Encoder-Decoder simples (Cap 2)\n",
    "\n",
    "**Descrição:**\n",
    "Este notebook apresenta uma implementação didática e simplificada do processo **encoder–decoder**, mostrando como textos são convertidos em representações numéricas (tokens) e, posteriormente, reconstruídos em texto. O foco está na preparação de dados textuais, tokenização e no fluxo conceitual de codificação e decodificação, conforme introduzido no Capítulo 2 do livro *Build a Large Language Model (From Scratch)*.\n",
    "\n",
    "**Objetivo:**\n",
    "Demonstrar, de forma prática e intuitiva, como funciona o pipeline básico de processamento de texto utilizado em modelos de linguagem:\n",
    "\n",
    "* Construção de um corpus (vocabulário)\n",
    "* Conversão de texto em tokens (encode)\n",
    "* Representação do texto como vetores numéricos\n",
    "* Reconstrução do texto a partir dos tokens (decode)\n",
    "\n",
    "Ao final, o leitor deverá compreender como textos são transformados em dados numéricos que podem ser processados por modelos de linguagem.\n",
    "\n",
    "**Funcionamento:**\n",
    "\n",
    "![Exemplo de funcionamento de um encoder-decoder](../../imagens/02_encode_decode.png)\n",
    "\n",
    "\n",
    "O funcionamento segue três etapas principais, conforme ilustrado na imagem de referência:\n",
    "\n",
    "1. **Construção do corpus:**\n",
    "   A partir de um conjunto de frases, criamos um vocabulário que associa cada token (palavra ou símbolo) a um identificador numérico.\n",
    "\n",
    "2. **Encode (codificação):**\n",
    "   Um novo texto é tokenizado e transformado em um vetor de inteiros, onde cada número representa um token do vocabulário.\n",
    "\n",
    "3. **Decode (decodificação):**\n",
    "   O vetor de tokens é convertido novamente em texto, demonstrando como a informação original pode ser recuperada a partir da representação numérica.\n",
    "\n",
    "Esse fluxo simples representa a base de sistemas mais complexos usados em modelos de linguagem modernos, como transformers e LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e44ceb-a091-4eb0-bbd3-a822a731c3db",
   "metadata": {},
   "source": [
    "## Implementação de um encoder-decoder simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75d3789-a9bd-400b-9a62-8480be68c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections.abc import Iterable\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleEncoderDecoder:\n",
    "    \"\"\"\n",
    "    Implementação didática de um encoder-decoder baseado em vocabulário (corpus).\n",
    "\n",
    "    A classe executa 3 etapas principais:\n",
    "    1) gerar_corpus: constrói o vocabulário a partir de frases (corpus)\n",
    "    2) encode: converte texto -> lista de token IDs\n",
    "    3) decode: converte lista de token IDs -> texto\n",
    "\n",
    "    Observações:\n",
    "    - Tokenização simples via regex (separa palavras e pontuação básica)\n",
    "    - Inclui prints com passo a passo, no estilo do exemplo do usuário\n",
    "    - O vocabulário é ordenado alfabeticamente para reprodutibilidade (didático)\n",
    "    \"\"\"\n",
    "\n",
    "    # Regex didática:\n",
    "    # - Captura pontuação básica em um grupo separado: [,.!?;:]\n",
    "    # - Captura espaços (\\s) como separadores\n",
    "    padrao: str = r\"([,.!?;:]|\\s)\"\n",
    "\n",
    "    # Estado interno (gerado a partir do corpus)\n",
    "    vocab: dict[str, int] = field(default_factory=dict)  # token -> id\n",
    "    inv_vocab: dict[int, str] = field(default_factory=dict)  # id -> token\n",
    "    corpus_tokens: list[list[str]] = field(default_factory=list)  # tokens por frase\n",
    "\n",
    "    def _tokenizar(self, texto: str, mostrar_passos: bool = True) -> list[str]:\n",
    "        \"\"\"\n",
    "        Tokeniza um texto de forma simples (didática): separa pontuação e remove espaços/itens vazios.\n",
    "        \"\"\"\n",
    "        if not isinstance(texto, str):\n",
    "            raise TypeError(\"O parâmetro `texto` deve ser uma string (str).\")\n",
    "\n",
    "        if not texto.strip():\n",
    "            raise ValueError(\"O parâmetro `texto` não pode ser vazio.\")\n",
    "\n",
    "        if mostrar_passos:\n",
    "            print(\"=== TOKENIZAÇÃO: Texto original ===\")\n",
    "            print(texto)\n",
    "            print()\n",
    "\n",
    "            print(\"=== TOKENIZAÇÃO: Split bruto (inclui espaços e strings vazias) ===\")\n",
    "        split_raw = re.split(self.padrao, texto)\n",
    "        if mostrar_passos:\n",
    "            print(split_raw)\n",
    "            print()\n",
    "\n",
    "            print(\"=== TOKENIZAÇÃO: Limpeza (remove espaços e itens vazios) ===\")\n",
    "        tokens = [p.strip() for p in split_raw if p.strip()]\n",
    "        if mostrar_passos:\n",
    "            print(tokens)\n",
    "            print()\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def gerar_corpus(self, frases: Iterable[str]) -> None:\n",
    "        \"\"\"\n",
    "        Constrói o vocabulário (corpus) a partir de uma lista/iterável de frases.\n",
    "\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        frases : Iterable[str]\n",
    "            Conjunto de frases para formar o corpus/vocabulário.\n",
    "\n",
    "        Exceções:\n",
    "        --------\n",
    "        TypeError\n",
    "            Se `frases` não for iterável ou contiver itens não-string.\n",
    "        ValueError\n",
    "            Se não houver tokens após a tokenização do corpus.\n",
    "        \"\"\"\n",
    "        if frases is None:\n",
    "            raise TypeError(\"O parâmetro `frases` não pode ser None.\")\n",
    "\n",
    "        frases = list(frases)\n",
    "        if not frases:\n",
    "            raise ValueError(\"O corpus precisa conter pelo menos 1 frase.\")\n",
    "\n",
    "        print(\"=== PASSO 1: Construção do corpus ===\")\n",
    "        print(\"Frases do corpus:\")\n",
    "        for i, f in enumerate(frases, start=1):\n",
    "            print(f\"  {i}. {f}\")\n",
    "        print()\n",
    "\n",
    "        # Tokenizar cada frase (com passo a passo)\n",
    "        self.corpus_tokens = []\n",
    "        todos_tokens: list[str] = []\n",
    "\n",
    "        for i, frase in enumerate(frases, start=1):\n",
    "            print(f\"--- Tokenizando frase {i} ---\")\n",
    "            tokens_frase = self._tokenizar(frase, mostrar_passos=True)\n",
    "            self.corpus_tokens.append(tokens_frase)\n",
    "            todos_tokens.extend(tokens_frase)\n",
    "\n",
    "        if not todos_tokens:\n",
    "            raise ValueError(\"Não foi possível gerar tokens a partir do corpus.\")\n",
    "\n",
    "        print(\"=== PASSO 2: Tokens coletados do corpus ===\")\n",
    "        print(\"Tokens por frase:\")\n",
    "        for i, toks in enumerate(self.corpus_tokens, start=1):\n",
    "            print(f\"  Frase {i}: {toks}\")\n",
    "        print()\n",
    "\n",
    "        print(\"=== PASSO 3: Construção do vocabulário (token -> id) ===\")\n",
    "        tokens_unicos_ordenados = sorted(set(todos_tokens))\n",
    "        self.vocab = {tok: idx for idx, tok in enumerate(tokens_unicos_ordenados)}\n",
    "        self.inv_vocab = {idx: tok for tok, idx in self.vocab.items()}\n",
    "\n",
    "        print(\"Tokens únicos (ordenados):\")\n",
    "        print(tokens_unicos_ordenados)\n",
    "        print()\n",
    "        print(\"Vocabulário (token -> id):\")\n",
    "        print(self.vocab)\n",
    "        print()\n",
    "\n",
    "    def encode(self, texto: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Converte um texto em uma lista de token IDs com base no vocabulário do corpus.\n",
    "\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        texto : str\n",
    "            Texto a ser codificado.\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        list[int]\n",
    "            Lista de token IDs.\n",
    "\n",
    "        Exceções:\n",
    "        --------\n",
    "        ValueError\n",
    "            Se o vocabulário ainda não foi gerado.\n",
    "            Se houver token fora do vocabulário.\n",
    "        \"\"\"\n",
    "        if not self.vocab:\n",
    "            raise ValueError(\n",
    "                \"Vocabulário não encontrado. Execute `gerar_corpus()` antes de `encode()`.\"\n",
    "            )\n",
    "\n",
    "        print(\"=== ENCODE: Converter texto -> token IDs ===\")\n",
    "        tokens = self._tokenizar(texto, mostrar_passos=True)\n",
    "\n",
    "        print(\"=== ENCODE: Checagem de tokens fora do vocabulário ===\")\n",
    "        desconhecidos = [t for t in tokens if t not in self.vocab]\n",
    "        if desconhecidos:\n",
    "            print(\"Tokens desconhecidos:\", desconhecidos)\n",
    "            raise ValueError(\n",
    "                \"Existem tokens que não estão no vocabulário do corpus. \"\n",
    "                \"Para este exemplo didático, todos os tokens precisam existir no corpus.\"\n",
    "            )\n",
    "        print(\"Nenhum token desconhecido ✅\")\n",
    "        print()\n",
    "\n",
    "        print(\"=== ENCODE: Conversão tokens -> IDs ===\")\n",
    "        token_ids = [self.vocab[t] for t in tokens]\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Token IDs:\", token_ids)\n",
    "        print()\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Converte uma lista de token IDs de volta para texto.\n",
    "\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        token_ids : list[int]\n",
    "            Lista de IDs.\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        str\n",
    "            Texto reconstruído.\n",
    "\n",
    "        Exceções:\n",
    "        --------\n",
    "        ValueError\n",
    "            Se o vocabulário ainda não foi gerado.\n",
    "            Se existir ID inválido.\n",
    "        TypeError\n",
    "            Se `token_ids` não for uma lista de inteiros.\n",
    "        \"\"\"\n",
    "        if not self.inv_vocab:\n",
    "            raise ValueError(\n",
    "                \"Vocabulário inverso não encontrado. Execute `gerar_corpus()` antes de `decode()`.\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(token_ids, list) or any(\n",
    "            not isinstance(i, int) for i in token_ids\n",
    "        ):\n",
    "            raise TypeError(\n",
    "                \"O parâmetro `token_ids` deve ser uma lista de inteiros (list[int]).\"\n",
    "            )\n",
    "\n",
    "        print(\"=== DECODE: Converter token IDs -> tokens ===\")\n",
    "        invalidos = [i for i in token_ids if i not in self.inv_vocab]\n",
    "        if invalidos:\n",
    "            print(\"IDs inválidos:\", invalidos)\n",
    "            raise ValueError(\"Existem IDs que não existem no vocabulário.\")\n",
    "\n",
    "        tokens = [self.inv_vocab[i] for i in token_ids]\n",
    "        print(\"Token IDs:\", token_ids)\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print()\n",
    "\n",
    "        print(\"=== DECODE: Reconstrução do texto ===\")\n",
    "        # Regra simples: junta com espaço, mas remove espaço antes de pontuação\n",
    "        texto = \" \".join(tokens)\n",
    "        texto = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", texto)\n",
    "\n",
    "        print(\"Texto decodificado:\", texto)\n",
    "        print()\n",
    "        return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a766c8f-acbc-4536-9d1b-5b1855b623a3",
   "metadata": {},
   "source": [
    "### Geração do corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a52f7e2-e49c-42c2-9023-4ef1b445d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASSO 1: Construção do corpus ===\n",
      "Frases do corpus:\n",
      "  1. O gato sobe no tapete.\n",
      "  2. O cachorro sobe na mesa.\n",
      "  3. A aranha desce a parede.\n",
      "  4. O gato desce da mesa.\n",
      "\n",
      "--- Tokenizando frase 1 ---\n",
      "=== TOKENIZAÇÃO: Texto original ===\n",
      "O gato sobe no tapete.\n",
      "\n",
      "=== TOKENIZAÇÃO: Split bruto (inclui espaços e strings vazias) ===\n",
      "['O', ' ', 'gato', ' ', 'sobe', ' ', 'no', ' ', 'tapete', '.', '']\n",
      "\n",
      "=== TOKENIZAÇÃO: Limpeza (remove espaços e itens vazios) ===\n",
      "['O', 'gato', 'sobe', 'no', 'tapete', '.']\n",
      "\n",
      "--- Tokenizando frase 2 ---\n",
      "=== TOKENIZAÇÃO: Texto original ===\n",
      "O cachorro sobe na mesa.\n",
      "\n",
      "=== TOKENIZAÇÃO: Split bruto (inclui espaços e strings vazias) ===\n",
      "['O', ' ', 'cachorro', ' ', 'sobe', ' ', 'na', ' ', 'mesa', '.', '']\n",
      "\n",
      "=== TOKENIZAÇÃO: Limpeza (remove espaços e itens vazios) ===\n",
      "['O', 'cachorro', 'sobe', 'na', 'mesa', '.']\n",
      "\n",
      "--- Tokenizando frase 3 ---\n",
      "=== TOKENIZAÇÃO: Texto original ===\n",
      "A aranha desce a parede.\n",
      "\n",
      "=== TOKENIZAÇÃO: Split bruto (inclui espaços e strings vazias) ===\n",
      "['A', ' ', 'aranha', ' ', 'desce', ' ', 'a', ' ', 'parede', '.', '']\n",
      "\n",
      "=== TOKENIZAÇÃO: Limpeza (remove espaços e itens vazios) ===\n",
      "['A', 'aranha', 'desce', 'a', 'parede', '.']\n",
      "\n",
      "--- Tokenizando frase 4 ---\n",
      "=== TOKENIZAÇÃO: Texto original ===\n",
      "O gato desce da mesa.\n",
      "\n",
      "=== TOKENIZAÇÃO: Split bruto (inclui espaços e strings vazias) ===\n",
      "['O', ' ', 'gato', ' ', 'desce', ' ', 'da', ' ', 'mesa', '.', '']\n",
      "\n",
      "=== TOKENIZAÇÃO: Limpeza (remove espaços e itens vazios) ===\n",
      "['O', 'gato', 'desce', 'da', 'mesa', '.']\n",
      "\n",
      "=== PASSO 2: Tokens coletados do corpus ===\n",
      "Tokens por frase:\n",
      "  Frase 1: ['O', 'gato', 'sobe', 'no', 'tapete', '.']\n",
      "  Frase 2: ['O', 'cachorro', 'sobe', 'na', 'mesa', '.']\n",
      "  Frase 3: ['A', 'aranha', 'desce', 'a', 'parede', '.']\n",
      "  Frase 4: ['O', 'gato', 'desce', 'da', 'mesa', '.']\n",
      "\n",
      "=== PASSO 3: Construção do vocabulário (token -> id) ===\n",
      "Tokens únicos (ordenados):\n",
      "['.', 'A', 'O', 'a', 'aranha', 'cachorro', 'da', 'desce', 'gato', 'mesa', 'na', 'no', 'parede', 'sobe', 'tapete']\n",
      "\n",
      "Vocabulário (token -> id):\n",
      "{'.': 0, 'A': 1, 'O': 2, 'a': 3, 'aranha': 4, 'cachorro': 5, 'da': 6, 'desce': 7, 'gato': 8, 'mesa': 9, 'na': 10, 'no': 11, 'parede': 12, 'sobe': 13, 'tapete': 14}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# corpus com 4 frases (≈ 5 palavras) repetindo\n",
    "# =========================================================\n",
    "corpus = [\n",
    "    \"O gato sobe no tapete.\",\n",
    "    \"O cachorro sobe na mesa.\",\n",
    "    \"A aranha desce a parede.\",\n",
    "    \"O gato desce da mesa.\",\n",
    "]\n",
    "\n",
    "ed = SimpleEncoderDecoder()\n",
    "\n",
    "# 01) Gerar corpus (vocabulário)\n",
    "ed.gerar_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f41a9-abb9-40ef-9d42-b88d43e77cbc",
   "metadata": {},
   "source": [
    "### Exemplo de uso do encode (uma frase ainda não utilizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c5ba7e-7fab-4fc3-b6ba-6a2942556722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENCODE: Converter texto -> token IDs ===\n",
      "=== TOKENIZAÇÃO: Texto original ===\n",
      "A aranha sobe no gato.\n",
      "\n",
      "=== TOKENIZAÇÃO: Split bruto (inclui espaços e strings vazias) ===\n",
      "['A', ' ', 'aranha', ' ', 'sobe', ' ', 'no', ' ', 'gato', '.', '']\n",
      "\n",
      "=== TOKENIZAÇÃO: Limpeza (remove espaços e itens vazios) ===\n",
      "['A', 'aranha', 'sobe', 'no', 'gato', '.']\n",
      "\n",
      "=== ENCODE: Checagem de tokens fora do vocabulário ===\n",
      "Nenhum token desconhecido ✅\n",
      "\n",
      "=== ENCODE: Conversão tokens -> IDs ===\n",
      "Tokens: ['A', 'aranha', 'sobe', 'no', 'gato', '.']\n",
      "Token IDs: [1, 4, 13, 11, 8, 0]\n",
      "\n",
      ">>> Resultado final do ENCODE\n",
      "Texto: A aranha sobe no gato.\n",
      "Token IDs: [1, 4, 13, 11, 8, 0]\n"
     ]
    }
   ],
   "source": [
    "novo_texto = \"A aranha sobe no gato.\"\n",
    "token_ids = ed.encode(novo_texto)\n",
    "\n",
    "print(\">>> Resultado final do ENCODE\")\n",
    "print(\"Texto:\", novo_texto)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151d5d7-21ed-40cf-ac1b-b4725c5500f3",
   "metadata": {},
   "source": [
    "### Exemplo de uso do decode (Uma frase ainda não utilizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c006f902-173d-40a1-82b4-bf60383ae146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DECODE: Converter token IDs -> tokens ===\n",
      "Token IDs: [2, 8, 13, 11, 5]\n",
      "Tokens: ['O', 'gato', 'sobe', 'no', 'cachorro']\n",
      "\n",
      "=== DECODE: Reconstrução do texto ===\n",
      "Texto decodificado: O gato sobe no cachorro\n",
      "\n",
      ">>> Resultado final do DECODE\n",
      "Token IDs: [2, 8, 13, 11, 5]\n",
      "Texto: O gato sobe no cachorro\n"
     ]
    }
   ],
   "source": [
    "token_ids = [2, 8, 13, 11, 5]\n",
    "texto_decodificado = ed.decode(token_ids)\n",
    "\n",
    "print(\">>> Resultado final do DECODE\")\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Texto:\", texto_decodificado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8984a54-f17c-4a67-8556-e34c04ba258e",
   "metadata": {},
   "source": [
    "## Propriedade das funções inversas:\n",
    "\n",
    "Duas funções `a` e `b` são inversas se:\n",
    "\n",
    "$$\n",
    "b(a(x)) = x \\quad \\text{e} \\quad a(b(y)) = y\n",
    "$$\n",
    "\n",
    "No caso do tokenizer:\n",
    "\n",
    "$$\n",
    "decode = encode^{-1}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b7c6454-1976-47db-8a6d-f19cd7fc0027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENCODE: Converter texto -> token IDs ===\n",
      "=== TOKENIZAÇÃO: Texto original ===\n",
      "A aranha sobe no gato.\n",
      "\n",
      "=== TOKENIZAÇÃO: Split bruto (inclui espaços e strings vazias) ===\n",
      "['A', ' ', 'aranha', ' ', 'sobe', ' ', 'no', ' ', 'gato', '.', '']\n",
      "\n",
      "=== TOKENIZAÇÃO: Limpeza (remove espaços e itens vazios) ===\n",
      "['A', 'aranha', 'sobe', 'no', 'gato', '.']\n",
      "\n",
      "=== ENCODE: Checagem de tokens fora do vocabulário ===\n",
      "Nenhum token desconhecido ✅\n",
      "\n",
      "=== ENCODE: Conversão tokens -> IDs ===\n",
      "Tokens: ['A', 'aranha', 'sobe', 'no', 'gato', '.']\n",
      "Token IDs: [1, 4, 13, 11, 8, 0]\n",
      "\n",
      "=== DECODE: Converter token IDs -> tokens ===\n",
      "Token IDs: [1, 4, 13, 11, 8, 0]\n",
      "Tokens: ['A', 'aranha', 'sobe', 'no', 'gato', '.']\n",
      "\n",
      "=== DECODE: Reconstrução do texto ===\n",
      "Texto decodificado: A aranha sobe no gato.\n",
      "\n",
      "====================\n",
      "A aranha sobe no gato.\n",
      "A aranha sobe no gato.\n"
     ]
    }
   ],
   "source": [
    "texto1 = \"A aranha sobe no gato.\"\n",
    "texto2 = ed.decode(ed.encode(novo_texto))\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(texto1)\n",
    "print(texto2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957cf464-4041-4a7e-9789-0a6b8b509b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DECODE: Converter token IDs -> tokens ===\n",
      "Token IDs: [1, 4, 13, 11, 8, 0]\n",
      "Tokens: ['A', 'aranha', 'sobe', 'no', 'gato', '.']\n",
      "\n",
      "=== DECODE: Reconstrução do texto ===\n",
      "Texto decodificado: A aranha sobe no gato.\n",
      "\n",
      "=== ENCODE: Converter texto -> token IDs ===\n",
      "=== TOKENIZAÇÃO: Texto original ===\n",
      "A aranha sobe no gato.\n",
      "\n",
      "=== TOKENIZAÇÃO: Split bruto (inclui espaços e strings vazias) ===\n",
      "['A', ' ', 'aranha', ' ', 'sobe', ' ', 'no', ' ', 'gato', '.', '']\n",
      "\n",
      "=== TOKENIZAÇÃO: Limpeza (remove espaços e itens vazios) ===\n",
      "['A', 'aranha', 'sobe', 'no', 'gato', '.']\n",
      "\n",
      "=== ENCODE: Checagem de tokens fora do vocabulário ===\n",
      "Nenhum token desconhecido ✅\n",
      "\n",
      "=== ENCODE: Conversão tokens -> IDs ===\n",
      "Tokens: ['A', 'aranha', 'sobe', 'no', 'gato', '.']\n",
      "Token IDs: [1, 4, 13, 11, 8, 0]\n",
      "\n",
      "====================\n",
      "[1, 4, 13, 11, 8, 0]\n",
      "[1, 4, 13, 11, 8, 0]\n"
     ]
    }
   ],
   "source": [
    "tokens1 = [1, 4, 13, 11, 8, 0]\n",
    "tokens2 = ed.encode(ed.decode(tokens1))\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b17bc-bbdb-4502-a549-18c5a2b7e891",
   "metadata": {},
   "source": [
    "## Falha ao 'encodar' palavras desconhecidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ac3a87f-c1a2-42aa-b566-a2d34f1db6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENCODE: Converter texto -> token IDs ===\n",
      "=== TOKENIZAÇÃO: Texto original ===\n",
      "O gato sobe na sopa\n",
      "\n",
      "=== TOKENIZAÇÃO: Split bruto (inclui espaços e strings vazias) ===\n",
      "['O', ' ', 'gato', ' ', 'sobe', ' ', 'na', ' ', 'sopa']\n",
      "\n",
      "=== TOKENIZAÇÃO: Limpeza (remove espaços e itens vazios) ===\n",
      "['O', 'gato', 'sobe', 'na', 'sopa']\n",
      "\n",
      "=== ENCODE: Checagem de tokens fora do vocabulário ===\n",
      "Tokens desconhecidos: ['sopa']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Existem tokens que não estão no vocabulário do corpus. Para este exemplo didático, todos os tokens precisam existir no corpus.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m texto = \u001b[33m\"\u001b[39m\u001b[33mO gato sobe na sopa\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43med\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mSimpleEncoderDecoder.encode\u001b[39m\u001b[34m(self, texto)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m desconhecidos:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTokens desconhecidos:\u001b[39m\u001b[33m\"\u001b[39m, desconhecidos)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExistem tokens que não estão no vocabulário do corpus. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPara este exemplo didático, todos os tokens precisam existir no corpus.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    154\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNenhum token desconhecido ✅\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Existem tokens que não estão no vocabulário do corpus. Para este exemplo didático, todos os tokens precisam existir no corpus."
     ]
    }
   ],
   "source": [
    "texto = \"O gato sobe na sopa\"\n",
    "ed.encode(texto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
