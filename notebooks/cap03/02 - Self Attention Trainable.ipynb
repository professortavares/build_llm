{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e38794-ca28-46c9-8ca9-b6ed788092d2",
   "metadata": {},
   "source": [
    "# Self-attention with trainable weights\n",
    "\n",
    "**Descrição**  \n",
    "A *self-attention with trainable weights* é a versão “completa” do mecanismo de atenção usada em Transformers: em vez de comparar diretamente os embeddings dos tokens, aprendemos **três projeções lineares** (pesos treináveis) que transformam cada token em vetores **Query (Q)**, **Key (K)** e **Value (V)**.  \n",
    "Com isso, o modelo passa a decidir *como* medir similaridade (via Q·K) e *quais informações* combinar (via V), ajustando esses pesos durante o treinamento para produzir representações contextuais mais úteis.\n",
    "\n",
    "**Objetivo**  \n",
    "Este notebook tem como objetivo mostrar, de forma didática, como implementar self-attention com parâmetros treináveis em PyTorch, destacando:\n",
    "- por que introduzimos matrizes/pesos para gerar **Q, K e V** (em vez de usar os embeddings “crus”);\n",
    "- como esses pesos tornam a atenção **aprendível** (ajustável por backprop);\n",
    "- como os **pesos de atenção** (dinâmicos, por entrada) diferem dos **pesos do modelo** (parâmetros treináveis das projeções).\n",
    "\n",
    "**Funcionamento**  \n",
    "Em alto nível, o mecanismo segue estes passos:\n",
    "\n",
    "1. **Entrada**: uma sequência de vetores (embeddings) `X` com shape `[seq_len, d_in]`.\n",
    "2. **Projeções treináveis**: aplica-se três camadas lineares para obter:\n",
    "   - `Q = W_q(X)`, `K = W_k(X)`, `V = W_v(X)` (cada uma com shape `[seq_len, d_attn]`).\n",
    "3. **Scores de atenção**: calcula-se a similaridade entre cada query e todas as keys:\n",
    "   - `scores = Q @ K.T`  → shape `[seq_len, seq_len]`.\n",
    "4. **Normalização**: aplica-se `softmax` por linha para transformar scores em probabilidades:\n",
    "   - `attn_weights = softmax(scores, dim=-1)` (cada linha soma 1).\n",
    "5. **Agregação de contexto**: combina-se a informação dos values com esses pesos:\n",
    "   - `context = attn_weights @ V` → shape `[seq_len, d_attn]`.\n",
    "6. **Saída**: o `context` é a representação contextualizada de cada token, agora construída a partir de uma combinação ponderada dos demais tokens — e essa combinação é guiada por **projeções aprendidas** (Wq, Wk, Wv).\n",
    " \n",
    "\n",
    "\n",
    "![O gato fazendo supino](../../imagens/cap03/02_gato_sobe_no_tapete.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33383d3-ffec-4848-8c90-e37feb858788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1297a6a6410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Para reprodutibilidade\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12fd88d-4a81-4014-98aa-76dc42acc479",
   "metadata": {},
   "source": [
    "## Frase de exemplo e vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc587e2-948b-48ef-8a03-9946a3804359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário:\n",
      "O -> 0\n",
      "gato -> 1\n",
      "sobe -> 2\n",
      "no -> 3\n",
      "tapete -> 4\n"
     ]
    }
   ],
   "source": [
    "# Frase de exemplo\n",
    "sentence = \"O gato sobe no tapete\".split()\n",
    "\n",
    "# Vocabulário\n",
    "vocab = {word: idx for idx, word in enumerate(sentence)}\n",
    "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "print(\"Vocabulário:\")\n",
    "for k, v in vocab.items():\n",
    "    print(f\"{k} -> {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd3a8a-9a75-4e32-ad3c-a4079c4706bc",
   "metadata": {},
   "source": [
    "## Conversão para índices (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a54fcf-f476-4c93-97cf-d2ad11652571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "O -> 0\n",
      "gato -> 1\n",
      "sobe -> 2\n",
      "no -> 3\n",
      "tapete -> 4\n"
     ]
    }
   ],
   "source": [
    "# Convertendo palavras para índices\n",
    "token_ids = torch.tensor([vocab[word] for word in sentence])\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for word, idx in zip(sentence, token_ids, strict=False):\n",
    "    print(f\"{word} -> {idx.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8947fb-62ea-46c4-9aa2-1bccc5b4d948",
   "metadata": {},
   "source": [
    "## Camada de Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84582284-5747-4e2e-aca8-4dd50664b67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dos embeddings: torch.Size([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367,  0.1288,  0.2345],\n",
       "        [ 0.2303, -1.1229, -0.1863],\n",
       "        [ 2.2082, -0.6380,  0.4617],\n",
       "        [ 0.2674,  0.5349,  0.8094],\n",
       "        [ 1.1103, -1.6898, -0.9890]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 3  # dimensão pequena para fins didáticos\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# Aplicando embedding\n",
    "X = embedding(token_ids)\n",
    "\n",
    "print(\"Shape dos embeddings:\", X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457444e-5291-4281-a012-b32be798ea15",
   "metadata": {},
   "source": [
    "## Conceito: o que são Query, Key e Value?\n",
    "\n",
    "Em **self-attention**, cada token da sequência “conversa” com todos os outros tokens para produzir uma **representação contextualizada**.\n",
    "\n",
    "- **Query (Q):** representa o que o token está procurando (a “pergunta” do token).\n",
    "- **Key (K):** representa o que cada token oferece como índice para ser encontrado (as “etiquetas” / “chaves”).\n",
    "- **Value (V):** representa a informação que será agregada (o “conteúdo” que será combinado).\n",
    "\n",
    "O mecanismo funciona assim (para cada token *i*):\n",
    "\n",
    "1. Calcula-se um **score de similaridade** entre o token *i* e cada token *j*:\n",
    "   \n",
    "   <img src=\"https://latex.codecogs.com/svg.image?&space;\\text{score}_{i,j}=q_i\\cdot&space;k_j\"/>\n",
    "\n",
    "3. Aplica-se **softmax** para transformar scores em pesos (que somam 1):\n",
    "\n",
    "\n",
    "   <img src=\"https://latex.codecogs.com/svg.image?\\alpha_{i,j}=\\text{softmax}(score_{i,:})\"/>\n",
    "\n",
    "\n",
    "3. Forma-se o **context vector** do token *i* como a média ponderada dos values:\n",
    "\n",
    "\n",
    "    <img src=\"https://latex.codecogs.com/svg.image?&space;\\text{context}_i=\\sum_j\\alpha_{i,j}v_j&space;\"/>\n",
    "\n",
    "\n",
    "\n",
    "![Attention gato](../../imagens/cap03/02_attention_gato.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261b089-4d63-4c66-b959-573dbf9b608e",
   "metadata": {},
   "source": [
    "## Definição da camada de Self-Attention (pesos treináveis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb86f4d2-ecb6-4e4a-8a3b-7ae3ab85064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa Self-Attention (single-head) para uma sequência, sem batch explícito,\n",
    "    retornando também tensores intermediários úteis para inspeção/debug.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    d_in : int\n",
    "        Dimensão de entrada (features por token).\n",
    "    d_attn : int\n",
    "        Dimensão interna da atenção (projeções Q, K e V).\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "        Retorna (Q, K, V, scores, context, attn_weights), onde:\n",
    "        - Q: (seq_len, d_attn)\n",
    "        - K: (seq_len, d_attn)\n",
    "        - V: (seq_len, d_attn)\n",
    "        - scores: (seq_len, seq_len)\n",
    "        - context: (seq_len, d_attn)\n",
    "        - attn_weights: (seq_len, seq_len)\n",
    "\n",
    "    Exceções:\n",
    "    --------\n",
    "    Levanta ValueError se x não tiver shape 2D (seq_len, d_in).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in: int, d_attn: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(d_in, d_attn, bias=False)\n",
    "        self.W_k = nn.Linear(d_in, d_attn, bias=False)\n",
    "        self.W_v = nn.Linear(d_in, d_attn, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Executa self-attention na sequência de entrada.\n",
    "\n",
    "        Parâmetros:\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Tensor de shape (seq_len, d_in).\n",
    "\n",
    "        Retorno:\n",
    "        -------\n",
    "        tuple\n",
    "            (Q, K, V, scores, context, attn_weights)\n",
    "        \"\"\"\n",
    "        if x.ndim != 2:\n",
    "            raise ValueError(\n",
    "                f\"Esperado x com 2 dimensões (seq_len, d_in), mas veio shape={tuple(x.shape)}.\"\n",
    "            )\n",
    "\n",
    "        # Projeções\n",
    "        Q = self.W_q(x)  # (T, d_attn)\n",
    "        K = self.W_k(x)  # (T, d_attn)\n",
    "        V = self.W_v(x)  # (T, d_attn)\n",
    "\n",
    "        # Scores: (T, d_attn) @ (d_attn, T) -> (T, T)\n",
    "        scores = Q @ K.T\n",
    "\n",
    "        # Pesos de atenção: (T, T)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Contexto: (T, T) @ (T, d_attn) -> (T, d_attn)\n",
    "        context = attn_weights @ V\n",
    "\n",
    "        return Q, K, V, scores, context, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96afe2-fc41-4efa-99c1-9ca793529d09",
   "metadata": {},
   "source": [
    "### Inicialização e verificação dos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab06d90b-5833-455d-9b31-0148608b5f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos da query\n",
      "self_attention.W_q.weight=Parameter containing:\n",
      "tensor([[ 0.4457,  0.0961, -0.1875],\n",
      "        [ 0.3568,  0.0900,  0.4665]], requires_grad=True)\n",
      "\n",
      "Pesos das keys\n",
      "self_attention.W_k.weight=Parameter containing:\n",
      "tensor([[ 0.0631, -0.1821,  0.1551],\n",
      "        [-0.1566,  0.2430,  0.5155]], requires_grad=True)\n",
      "\n",
      "Pesos dos values\n",
      "self_attention.W_v.weight=Parameter containing:\n",
      "tensor([[ 0.3337, -0.2524,  0.3333],\n",
      "        [ 0.1033,  0.2932, -0.3519]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "d_attn = 2  # apenas 2 para ficar didático\n",
    "\n",
    "self_attention = SelfAttention(d_in=embedding_dim, d_attn=d_attn)\n",
    "\n",
    "Q, K, V, scores, context, attn_weights = self_attention(X)\n",
    "\n",
    "print(\"Pesos da query\")\n",
    "print(f\"{self_attention.W_q.weight=}\")\n",
    "print()\n",
    "print(\"Pesos das keys\")\n",
    "print(f\"{self_attention.W_k.weight=}\")\n",
    "print()\n",
    "print(\"Pesos dos values\")\n",
    "print(f\"{self_attention.W_v.weight=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95c5d9-59c2-43b4-8e00-9b1dd0909e94",
   "metadata": {},
   "source": [
    "### O Q do gato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae99572e-467d-4144-bd7a-c2c9bdb60457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0297, -0.1058], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126d9fe-ce3a-4720-bd6b-40465bf222af",
   "metadata": {},
   "source": [
    "### O K do gato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04fa2cbc-1e55-418e-95e4-2ec2b9df1a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1901, -0.4049], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c0459-0e7e-40a1-9037-a316991be350",
   "metadata": {},
   "source": [
    "### O V do gato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0bce442-5f04-4086-958c-c3956beda4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2982, -0.2399], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53afa22-fa7c-46f6-9ef3-622d17e1eed0",
   "metadata": {},
   "source": [
    "### Scores do gato em relação ao próprio gato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d557288-89b5-4803-99a3-1f67cb7e3fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0485, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1137c7b-ad74-4be7-a32c-d401a5598ec3",
   "metadata": {},
   "source": [
    "## Score do gato em relação aos demais itens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4311def8-a410-4254-858b-5f0b8d17b20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0095,  0.0485,  0.0375, -0.0521,  0.1224],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc552b18-3019-4109-92f8-459e4fe4e282",
   "metadata": {},
   "source": [
    "### Normalização por softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d250640-58d0-46ee-a37e-563ec69b5616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1920, 0.2035, 0.2013, 0.1840, 0.2191], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c12f755-5c0f-4289-9b59-b012b72cc3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0280, -0.0751, -0.0246,  0.1272, -0.2372],\n",
       "        [-0.0095,  0.0485,  0.0375, -0.0521,  0.1224],\n",
       "        [ 0.1226, -0.2240,  0.0251,  0.5156, -0.8472],\n",
       "        [ 0.0525, -0.2074, -0.1308,  0.2641, -0.5659],\n",
       "        [-0.0039,  0.1864,  0.2265, -0.0865,  0.3539]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb36b296-853b-4efd-be84-8954017f9e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2118, 0.1910, 0.2009, 0.2338, 0.1624],\n",
       "        [0.1920, 0.2035, 0.2013, 0.1840, 0.2191],\n",
       "        [0.2235, 0.1580, 0.2027, 0.3311, 0.0847],\n",
       "        [0.2284, 0.1761, 0.1902, 0.2822, 0.1231],\n",
       "        [0.1718, 0.2079, 0.2164, 0.1582, 0.2458]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c008ec3c-27df-4bdd-a5b2-00e5e95250d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4301, -0.1011],\n",
       "        [ 0.4464, -0.1008],\n",
       "        [ 0.4094, -0.1007],\n",
       "        [ 0.4094, -0.1000],\n",
       "        [ 0.4670, -0.1018]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c15be79d-5b32-4aee-9d2b-5612a90b0e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do contexto: torch.Size([5, 2])\n",
      "Shape dos pesos de atenção: torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape do contexto:\", context.shape)\n",
    "print(\"Shape dos pesos de atenção:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e4476-75d2-4f87-983d-4d9ec95eb3f6",
   "metadata": {},
   "source": [
    "## Visualizando os pesos de atenção (parte mais didática)\n",
    "\n",
    "Aqui mostramos quem presta atenção em quem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6def1531-2cb7-4716-8b32-db9b4b69feef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de atenção:\n",
      "\n",
      "Palavra: 'O'\n",
      "  → atenção em 'O': 0.21\n",
      "  → atenção em 'gato': 0.19\n",
      "  → atenção em 'sobe': 0.20\n",
      "  → atenção em 'no': 0.23\n",
      "  → atenção em 'tapete': 0.16\n",
      "\n",
      "Palavra: 'gato'\n",
      "  → atenção em 'O': 0.19\n",
      "  → atenção em 'gato': 0.20\n",
      "  → atenção em 'sobe': 0.20\n",
      "  → atenção em 'no': 0.18\n",
      "  → atenção em 'tapete': 0.22\n",
      "\n",
      "Palavra: 'sobe'\n",
      "  → atenção em 'O': 0.22\n",
      "  → atenção em 'gato': 0.16\n",
      "  → atenção em 'sobe': 0.20\n",
      "  → atenção em 'no': 0.33\n",
      "  → atenção em 'tapete': 0.08\n",
      "\n",
      "Palavra: 'no'\n",
      "  → atenção em 'O': 0.23\n",
      "  → atenção em 'gato': 0.18\n",
      "  → atenção em 'sobe': 0.19\n",
      "  → atenção em 'no': 0.28\n",
      "  → atenção em 'tapete': 0.12\n",
      "\n",
      "Palavra: 'tapete'\n",
      "  → atenção em 'O': 0.17\n",
      "  → atenção em 'gato': 0.21\n",
      "  → atenção em 'sobe': 0.22\n",
      "  → atenção em 'no': 0.16\n",
      "  → atenção em 'tapete': 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Pesos de atenção:\\n\")\n",
    "\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"Palavra: '{word}'\")\n",
    "    for j, weight in enumerate(attn_weights[i]):\n",
    "        print(f\"  → atenção em '{sentence[j]}': {weight.item():.2f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
