{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69811d4c-ca49-4e20-ab49-74f55a646188",
   "metadata": {},
   "source": [
    "# Multi-head attention\n",
    "\n",
    "**Descrição**  \n",
    "Neste notebook, vamos implementar e explorar **multi-head attention** (atenção multi-cabeças). A ideia é sair do “resultado final” e abrir a caixa-preta do mecanismo de atenção, visualizando o que acontece em **cada cabeça (head)** separadamente.\n",
    "\n",
    "**Objetivo**  \n",
    "- Construir um pipeline mínimo: tokenização → índices → embeddings → multi-head attention.  \n",
    "- Entender **conceitualmente** por que usar várias cabeças (heads) em vez de uma só.  \n",
    "- Inspecionar, de forma interpretável, os principais componentes internos:\n",
    "  - **matriz de atenção** de cada head (quem olha para quem)\n",
    "  - **vetores de contexto** produzidos por cada head (o que cada head “constrói”)\n",
    "\n",
    "**Funcionamento**  \n",
    "1. **Tokenização e vocabulário**: a frase é separada por espaços (`split`) e mapeada para IDs (`vocab` / `inv_vocab`).  \n",
    "2. **Embeddings**: cada token ID é convertido em um vetor denso com dimensão pequena para facilitar a visualização.  \n",
    "3. **Projeções Q, K, V**: o embedding é projetado em **Queries (Q)**, **Keys (K)** e **Values (V)**.  \n",
    "4. **Divisão em múltiplas cabeças**: Q/K/V são reorganizados para criar `num_heads` subconjuntos (“heads”), cada um com dimensão `head_dim`.  \n",
    "5. **Atenção por head**: cada head calcula sua própria matriz de atenção e gera seus próprios vetores de contexto.  \n",
    "6. **Concatenação e projeção final**: as saídas das heads são concatenadas e combinadas por uma camada linear (`out_proj`) para produzir a saída final do bloco de atenção.\n",
    "\n",
    "![O gato sobe no tapete](../../imagens/cap03/04_gato_sobe_no_tapete.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca9fc65-9d40-4f30-8d8e-e9fb66650d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fbb2922410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Para reprodutibilidade\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f54609-fdae-49f5-af3f-b2da28c97291",
   "metadata": {},
   "source": [
    "## Frase de exemplo e vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4423f09-d2d4-429b-af66-eb8abd832c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário:\n",
      "O -> 0\n",
      "gato -> 1\n",
      "sobe -> 2\n",
      "no -> 3\n",
      "tapete -> 4\n"
     ]
    }
   ],
   "source": [
    "# Frase de exemplo\n",
    "sentence = \"O gato sobe no tapete\".split()\n",
    "\n",
    "# Vocabulário\n",
    "vocab = {word: idx for idx, word in enumerate(sentence)}\n",
    "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "print(\"Vocabulário:\")\n",
    "for k, v in vocab.items():\n",
    "    print(f\"{k} -> {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aede00-b7d6-4c19-8d61-363fb4205dc2",
   "metadata": {},
   "source": [
    "## Conversão para índices (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf56ac3-513c-4fe7-a10e-0ba1b6bea8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "O -> 0\n",
      "gato -> 1\n",
      "sobe -> 2\n",
      "no -> 3\n",
      "tapete -> 4\n"
     ]
    }
   ],
   "source": [
    "# Convertendo palavras para índices\n",
    "token_ids = torch.tensor([vocab[word] for word in sentence])\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for word, idx in zip(sentence, token_ids, strict=False):\n",
    "    print(f\"{word} -> {idx.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528e57b-3fa0-4185-84ed-1da857103aed",
   "metadata": {},
   "source": [
    "## Camada de Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da926712-d8fd-4138-b97a-65fa7dac05d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dos embeddings: torch.Size([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367,  0.1288,  0.2345],\n",
       "        [ 0.2303, -1.1229, -0.1863],\n",
       "        [ 2.2082, -0.6380,  0.4617],\n",
       "        [ 0.2674,  0.5349,  0.8094],\n",
       "        [ 1.1103, -1.6898, -0.9890]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 3  # dimensão pequena para fins didáticos\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# Aplicando embedding\n",
    "X = embedding(token_ids)\n",
    "\n",
    "print(\"Shape dos embeddings:\", X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8589e8f-2ae4-4b04-b07b-43a1619b11f8",
   "metadata": {},
   "source": [
    "## O que é Multi-Head Attention?\n",
    "\n",
    "Na **self-attention** “single-head”, você calcula uma única matriz de atenção:\n",
    "- projeta o input para **Q** (queries), **K** (keys), **V** (values)\n",
    "- computa similaridades `Q @ Kᵀ`\n",
    "- aplica softmax (e máscara causal, se for o caso)\n",
    "- mistura os valores `V` de acordo com os pesos de atenção\n",
    "\n",
    "Na **multi-head attention**, em vez de fazer isso uma vez, você faz **em paralelo** em múltiplas “cabeças” (heads):\n",
    "- cada head tem sua própria forma de projetar Q/K/V (na prática, uma projeção maior que é “fatiada”)\n",
    "- cada head pode focar em padrões diferentes (ex.: concordância, dependências locais, estruturas sintáticas etc.)\n",
    "- no final, você concatena as saídas das heads e aplica uma projeção final (`out_proj`)\n",
    "\n",
    "Intuição curta:\n",
    "- **single-head** = “um único tipo de olhar” sobre a sequência\n",
    "- **multi-head** = “vários olhares em paralelo”, depois combinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d631949f-fe2f-4276-a517-ed82311d1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa Multi-Head Causal Self-Attention (estilo Transformer) com máscara causal\n",
    "    para impedir que cada token atenda tokens futuros.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    d_in : int\n",
    "        Dimensão de entrada (features por token).\n",
    "    d_out : int\n",
    "        Dimensão total de saída (soma das dimensões de todas as heads).\n",
    "        Deve ser divisível por num_heads.\n",
    "    context_length : int\n",
    "        Comprimento máximo de contexto (número máximo de tokens).\n",
    "    dropout : float\n",
    "        Probabilidade de dropout aplicada aos pesos de atenção.\n",
    "    num_heads : int\n",
    "        Número de cabeças de atenção.\n",
    "    qkv_bias : bool, default = False\n",
    "        Se True, adiciona bias nas camadas lineares de Q, K e V.\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Tensor de shape (batch_size, num_tokens, d_out) com os vetores de contexto.\n",
    "\n",
    "    Exceções:\n",
    "    --------\n",
    "    Levanta ValueError se d_out não for divisível por num_heads.\n",
    "    Levanta ValueError se x não tiver shape 3D (B, T, D).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if d_out % num_heads != 0:\n",
    "            raise ValueError(\"d_out must be divisible by num_heads\")\n",
    "\n",
    "        self.d_out = int(d_out)\n",
    "        self.num_heads = int(num_heads)\n",
    "        self.head_dim = self.d_out // self.num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Projeção final para combinar as heads\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Máscara causal: 1s acima da diagonal principal (tokens futuros).\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim != 3:\n",
    "            raise ValueError(\n",
    "                f\"Esperado x com 3 dimensões (batch, tokens, d_in), mas veio shape={tuple(x.shape)}.\"\n",
    "            )\n",
    "\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Projeções: (b, T, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # (b, T, d_out) -> (b, T, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (b, T, num_heads, head_dim) -> (b, num_heads, T, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Scores por head: (b, h, T, head_dim) @ (b, h, head_dim, T) -> (b, h, T, T)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        # Aplicar máscara causal (broadcast nas heads e batch)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # Softmax com escala por sqrt(d_k)\n",
    "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Contexto por head: (b, h, T, T) @ (b, h, T, head_dim) -> (b, h, T, head_dim)\n",
    "        context = attn_weights @ values\n",
    "\n",
    "        # (b, h, T, head_dim) -> (b, T, h, head_dim)\n",
    "        context = context.transpose(1, 2)\n",
    "\n",
    "        # Concatenar heads: (b, T, d_out)\n",
    "        context = context.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        # Projeção final\n",
    "        context = self.out_proj(context)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc8f8a-5081-4b57-a858-720a4b15d801",
   "metadata": {},
   "source": [
    "## Rodando a MHA (com 2 heads)\n",
    "\n",
    "Aqui a gente escolhe `d_out` divisível por `num_heads`. Como o embedding é `d_in=3`, vamos usar `d_out=4` e `num_heads=2 ⇒ head_dim=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760d9a92-3348-4d6f-bf05-d1fd76c6cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([1, 5, 3])\n",
      "Output shape: torch.Size([1, 5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3455,  0.0400,  0.1735, -0.3224],\n",
       "         [ 0.3484,  0.0101,  0.1389, -0.2539],\n",
       "         [ 0.4340,  0.2990, -0.0576, -0.1211],\n",
       "         [ 0.4144,  0.2152, -0.0041, -0.1653],\n",
       "         [ 0.4142,  0.1889,  0.0125, -0.1536]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = len(sentence)\n",
    "context_length = num_tokens\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=embedding_dim,\n",
    "    d_out=4,  # precisa ser divisível por num_heads\n",
    "    context_length=context_length,\n",
    "    dropout=0.0,  # 0.0 pra ficar determinístico/didático\n",
    "    num_heads=2,\n",
    "    qkv_bias=False,\n",
    ")\n",
    "\n",
    "# A MHA espera (batch, tokens, d_in)\n",
    "x_in = X.unsqueeze(0)  # (1, 5, 3)\n",
    "\n",
    "y = mha(x_in)  # (1, 5, 4)\n",
    "print(\"Input shape :\", x_in.shape)\n",
    "print(\"Output shape:\", y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521efd56-1989-4670-8839-d52cc3dc1605",
   "metadata": {},
   "source": [
    "## O que existe “dentro” de cada head?\n",
    "\n",
    "Para cada head `h`, existem:\n",
    "- `Q_h`, `K_h`, `V_h` com shape `(tokens, head_dim)`\n",
    "- `attn_scores_h = Q_h @ K_h^T` com shape `(tokens, tokens)`\n",
    "- `attn_weights_h = softmax(attn_scores_h / sqrt(head_dim))` com shape `(tokens, tokens)`\n",
    "- `head_context_h = attn_weights_h @ V_h` com shape `(tokens, head_dim)`\n",
    "\n",
    "Depois:\n",
    "- concatenamos `[head_context_0 || head_context_1]` → `(tokens, d_out)`\n",
    "- aplicamos `out_proj` → `(tokens, d_out)`\n",
    "\n",
    "⚠️ Importante:\n",
    "Como estamos com pesos aleatórios (não treinamos nada), as atenções vão parecer “meio aleatórias”.\n",
    "A graça aqui é **entender as peças e visualizar por head**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61449607-6c0c-468f-a937-76b415190d4a",
   "metadata": {},
   "source": [
    "### Função de inspeção\n",
    "\n",
    "A ideia: usar os pesos da mha e reproduzir o forward passo-a-passo para capturar attn_weights e head_context de cada head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8370ebc-c8b4-491e-b36a-8a5805d1250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights shape: torch.Size([1, 2, 5, 5])\n",
      "head_context shape: torch.Size([1, 2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def inspect_multihead_attention(\n",
    "    mha: MultiHeadAttention, x: torch.Tensor, tokens: list[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Inspeciona Q/K/V, matriz de atenção e vetores de contexto por head,\n",
    "    reproduzindo o forward com os pesos do módulo 'mha'.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    mha : MultiHeadAttention\n",
    "        Módulo já instanciado (com pesos).\n",
    "    x : torch.Tensor\n",
    "        Entrada com shape (batch, tokens, d_in).\n",
    "    tokens : list[str]\n",
    "        Lista de tokens para rotular linhas/colunas.\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    dict com tensores intermediários.\n",
    "    \"\"\"\n",
    "    b, num_tokens, _ = x.shape\n",
    "\n",
    "    keys = mha.W_key(x)  # (b, tokens, d_out)\n",
    "    queries = mha.W_query(x)  # (b, tokens, d_out)\n",
    "    values = mha.W_value(x)  # (b, tokens, d_out)\n",
    "\n",
    "    # split em heads\n",
    "    keys = keys.view(b, num_tokens, mha.num_heads, mha.head_dim).transpose(\n",
    "        1, 2\n",
    "    )  # (b, heads, tokens, head_dim)\n",
    "    queries = queries.view(b, num_tokens, mha.num_heads, mha.head_dim).transpose(1, 2)\n",
    "    values = values.view(b, num_tokens, mha.num_heads, mha.head_dim).transpose(1, 2)\n",
    "\n",
    "    attn_scores = queries @ keys.transpose(2, 3)  # (b, heads, tokens, tokens)\n",
    "\n",
    "    mask_bool = mha.mask.bool()[:num_tokens, :num_tokens]\n",
    "    attn_scores = attn_scores.masked_fill(mask_bool, float(\"-inf\"))\n",
    "\n",
    "    attn_weights = torch.softmax(\n",
    "        attn_scores / (mha.head_dim**0.5), dim=-1\n",
    "    )  # (b, heads, tokens, tokens)\n",
    "    head_context = attn_weights @ values  # (b, heads, tokens, head_dim)\n",
    "\n",
    "    # concat heads + out_proj (igual ao forward)\n",
    "    concat_context = (\n",
    "        head_context.transpose(1, 2).contiguous().view(b, num_tokens, mha.d_out)\n",
    "    )\n",
    "    out = mha.out_proj(concat_context)\n",
    "\n",
    "    return {\n",
    "        \"queries\": queries,\n",
    "        \"keys\": keys,\n",
    "        \"values\": values,\n",
    "        \"attn_scores\": attn_scores,\n",
    "        \"attn_weights\": attn_weights,\n",
    "        \"head_context\": head_context,\n",
    "        \"concat_context\": concat_context,\n",
    "        \"out\": out,\n",
    "    }\n",
    "\n",
    "\n",
    "ins = inspect_multihead_attention(mha, x_in, sentence)\n",
    "print(\"attn_weights shape:\", ins[\"attn_weights\"].shape)\n",
    "print(\"head_context shape:\", ins[\"head_context\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae166c-c796-4621-8493-b7bed6c19e57",
   "metadata": {},
   "source": [
    "### Mostrar a matriz de atenção do Head 0 e Head 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9495b36b-2afb-45cc-8210-ee2f632a45b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Head 0 | Matriz de atenção (attn_weights) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K:O</th>\n",
       "      <th>K:gato</th>\n",
       "      <th>K:sobe</th>\n",
       "      <th>K:no</th>\n",
       "      <th>K:tapete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q:O</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q:gato</th>\n",
       "      <td>0.495</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q:sobe</th>\n",
       "      <td>0.285</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q:no</th>\n",
       "      <td>0.259</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q:tapete</th>\n",
       "      <td>0.177</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            K:O  K:gato  K:sobe   K:no  K:tapete\n",
       "Q:O       1.000   0.000   0.000  0.000     0.000\n",
       "Q:gato    0.495   0.505   0.000  0.000     0.000\n",
       "Q:sobe    0.285   0.266   0.449  0.000     0.000\n",
       "Q:no      0.259   0.238   0.252  0.251     0.000\n",
       "Q:tapete  0.177   0.193   0.249  0.183     0.198"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Head 1 | Matriz de atenção (attn_weights) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K:O</th>\n",
       "      <th>K:gato</th>\n",
       "      <th>K:sobe</th>\n",
       "      <th>K:no</th>\n",
       "      <th>K:tapete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q:O</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q:gato</th>\n",
       "      <td>0.457</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q:sobe</th>\n",
       "      <td>0.346</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q:no</th>\n",
       "      <td>0.236</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q:tapete</th>\n",
       "      <td>0.199</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            K:O  K:gato  K:sobe   K:no  K:tapete\n",
       "Q:O       1.000   0.000   0.000  0.000     0.000\n",
       "Q:gato    0.457   0.543   0.000  0.000     0.000\n",
       "Q:sobe    0.346   0.418   0.236  0.000     0.000\n",
       "Q:no      0.236   0.211   0.305  0.248     0.000\n",
       "Q:tapete  0.199   0.286   0.091  0.166     0.258"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_head_attention_tables(ins_dict, tokens):\n",
    "    attn = ins_dict[\"attn_weights\"][0]  # (heads, tokens, tokens)\n",
    "    num_heads = attn.shape[0]\n",
    "\n",
    "    for h in range(num_heads):\n",
    "        df = pd.DataFrame(\n",
    "            attn[h].cpu().numpy(),\n",
    "            index=[f\"Q:{t}\" for t in tokens],\n",
    "            columns=[f\"K:{t}\" for t in tokens],\n",
    "        )\n",
    "        print(f\"\\n=== Head {h} | Matriz de atenção (attn_weights) ===\")\n",
    "        display(df.round(3))\n",
    "\n",
    "\n",
    "show_head_attention_tables(ins, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79befd7-df1f-4dbc-96c1-f995687a4d52",
   "metadata": {},
   "source": [
    "### Mostrar os vetores de contexto produzidos por cada head\n",
    "\n",
    "Interpretação:\n",
    "\n",
    "* Cada head produz um vetor por token (aqui com head_dim=2).\n",
    "* Esse vetor é uma “mistura” dos V (values) ponderada pelos attn_weights daquele head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a29b35e-9e4f-45ba-bbee-7cd9a98c14ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Head 0 | Vetores de contexto (head_context) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim0</th>\n",
       "      <th>dim1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gato</th>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sobe</th>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tapete</th>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dim0   dim1\n",
       "O       0.166 -0.226\n",
       "gato    0.067 -0.143\n",
       "sobe    0.317 -0.580\n",
       "no      0.294 -0.455\n",
       "tapete  0.207 -0.429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Head 1 | Vetores de contexto (head_context) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim0</th>\n",
       "      <th>dim1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gato</th>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sobe</th>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tapete</th>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dim0   dim1\n",
       "O       0.076 -0.059\n",
       "gato    0.160 -0.299\n",
       "sobe    0.265 -0.402\n",
       "no      0.271 -0.355\n",
       "tapete  0.242 -0.386"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_head_context_vectors(ins_dict, tokens):\n",
    "    ctx = ins_dict[\"head_context\"][0]  # (heads, tokens, head_dim)\n",
    "    num_heads = ctx.shape[0]\n",
    "\n",
    "    for h in range(num_heads):\n",
    "        df = pd.DataFrame(\n",
    "            ctx[h].cpu().numpy(),\n",
    "            index=[f\"{t}\" for t in tokens],\n",
    "            columns=[f\"dim{j}\" for j in range(ctx.shape[-1])],\n",
    "        )\n",
    "        print(f\"\\n=== Head {h} | Vetores de contexto (head_context) ===\")\n",
    "        display(df.round(3))\n",
    "\n",
    "\n",
    "show_head_context_vectors(ins, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd733ea-0260-4f3c-80e6-65b01c1e21ac",
   "metadata": {},
   "source": [
    "### Mostrar concatenação das heads + saída final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0376f7f-182d-464d-9dbe-a4a0b4cd9429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Concatenação [head0 || head1] (antes do out_proj) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concat_dim0</th>\n",
       "      <th>concat_dim1</th>\n",
       "      <th>concat_dim2</th>\n",
       "      <th>concat_dim3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gato</th>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sobe</th>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tapete</th>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        concat_dim0  concat_dim1  concat_dim2  concat_dim3\n",
       "O             0.166       -0.226        0.076       -0.059\n",
       "gato          0.067       -0.143        0.160       -0.299\n",
       "sobe          0.317       -0.580        0.265       -0.402\n",
       "no            0.294       -0.455        0.271       -0.355\n",
       "tapete        0.207       -0.429        0.242       -0.386"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saída final (depois do out_proj) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>out_dim0</th>\n",
       "      <th>out_dim1</th>\n",
       "      <th>out_dim2</th>\n",
       "      <th>out_dim3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.346</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gato</th>\n",
       "      <td>0.348</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sobe</th>\n",
       "      <td>0.434</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.414</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tapete</th>\n",
       "      <td>0.414</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        out_dim0  out_dim1  out_dim2  out_dim3\n",
       "O          0.346     0.040     0.173    -0.322\n",
       "gato       0.348     0.010     0.139    -0.254\n",
       "sobe       0.434     0.299    -0.058    -0.121\n",
       "no         0.414     0.215    -0.004    -0.165\n",
       "tapete     0.414     0.189     0.012    -0.154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_df = pd.DataFrame(\n",
    "    ins[\"concat_context\"][0].cpu().numpy(),\n",
    "    index=sentence,\n",
    "    columns=[f\"concat_dim{j}\" for j in range(ins[\"concat_context\"].shape[-1])],\n",
    ")\n",
    "print(\"=== Concatenação [head0 || head1] (antes do out_proj) ===\")\n",
    "display(concat_df.round(3))\n",
    "\n",
    "out_df = pd.DataFrame(\n",
    "    ins[\"out\"][0].cpu().numpy(),\n",
    "    index=sentence,\n",
    "    columns=[f\"out_dim{j}\" for j in range(ins[\"out\"].shape[-1])],\n",
    ")\n",
    "print(\"\\n=== Saída final (depois do out_proj) ===\")\n",
    "display(out_df.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
