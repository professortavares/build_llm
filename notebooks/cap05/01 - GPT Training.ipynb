{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5375005-e9b6-46d3-b1b6-0028ef50656c",
   "metadata": {},
   "source": [
    "# GPT Trainning\n",
    "\n",
    "**Descrição:**\n",
    "Notebook de treinamento de um modelo do tipo GPT (Transformer decodificador) para modelagem de linguagem, usando PyTorch e tokenização via `tiktoken`. O corpus utilizado é o texto do livro **“Dom Casmurro”** (Projeto Gutenberg), com pipeline completo de preparação de dados, treinamento, avaliação (loss) e geração de texto.\n",
    "\n",
    "**Objetivo:**\n",
    "1. Construir um fluxo reprodutível de fine-tuning/treinamento de linguagem (next-token prediction) em um corpus textual.\n",
    "2. Executar e monitorar o treinamento (treino/validação) por meio da métrica de *loss*.\n",
    "3. Validar qualitativamente o modelo por geração de texto a partir de *prompts*.\n",
    "\n",
    "**Funcionamento:**\n",
    "1. **Ambiente e reprodutibilidade:** checagem de GPU, definição de seeds e importação de utilitários/modelo (`GPTModel`, `create_dataloader_v1`, `generate_text_simple`).\n",
    "2. **Configuração do modelo:** definição do dicionário de configuração (ex.: `vocab_size`, `context_length` reduzido para 256, dimensões, camadas, heads, dropout).\n",
    "3. **Aquisição do dataset:** download/leitura do texto do *Dom Casmurro* e cálculo de estatísticas básicas (caracteres/tokens).\n",
    "4. **Preparação dos dados:** tokenização, divisão em treino/validação e criação dos `DataLoader`s com batching e janelas de contexto.\n",
    "5. **Treinamento:** loop de treino com cálculo de loss por batch, avaliação periódica em validação e registro de histórico (loss e tokens processados).\n",
    "6. **Análise:** plot do *loss* de treino e validação ao longo das épocas.\n",
    "7. **Inferência:** geração de texto a partir de um prompt inicial, respeitando o tamanho de contexto configurado.\n",
    "\n",
    "\n",
    "![Bloco transformer](../../imagens/cap05/01_treinamento_modelo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66890960-0aa9-4d47-8156-7473674a9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tiktoken\n",
    "import torch\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch import nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from build_llm.gpt import GPTModel\n",
    "from build_llm.util import create_dataloader_v1, generate_text_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8c68c6-1b37-4b5c-9f05-d2ccd833b13c",
   "metadata": {},
   "source": [
    "## Verifica a presença da GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1113c835-9a33-43de-a677-284d717afc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Usando: cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # Versão do torch\n",
    "print(torch.cuda.is_available())  # Verificação de GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdd853-4805-4aa4-8d25-d7002f4bae50",
   "metadata": {},
   "source": [
    "## Fixando o seed para reproducibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f6a92a-3ef6-43dc-aaf9-c93d1ae629c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Fixa seeds para reprodutibilidade em Python, NumPy e PyTorch.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Garante determinismo (pode afetar performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b8c56-7b1d-4c4f-8bbb-e3544a6ba481",
   "metadata": {},
   "source": [
    "## Configuração do modelo\n",
    "\n",
    "As mesmas utilizadas no notebook [01 - LLM architecture](./01%20-%20LLM%20architecture.ipynb)\n",
    "**exceto** pela janela de contexto (context_length) que foi reduziada para 256 (anteriormente era 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675cbceb-4609-4f4c-a092-9f2d5c970733",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50257,  # Tamanho do vocabulário\n",
    "    \"context_length\": 256,  # Comprimento do contexto\n",
    "    \"emb_dim\": 768,  # Dimensão do embedding\n",
    "    \"n_heads\": 12,  # Número de cabeças de atenção\n",
    "    \"n_layers\": 12,  # Número de camadas\n",
    "    \"drop_rate\": 0.1,  # Taxa de dropout\n",
    "    \"qkv_bias\": False,  # Viés em Query-Key-Value (QKV)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e523ca2a-0179-4ab5-9518-cfceb170084d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a semente para reprodutibilidade\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Instancia o modelo com a configuração desejada\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "# Coloca o modelo em modo de avaliação (desativa dropout, etc.)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2663d0a-300e-4d78-bfca-718a2e747e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text: str, tokenizer) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converte um texto em IDs de tokens e retorna um tensor (com dimensão de batch).\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    text : str\n",
    "        Texto de entrada a ser tokenizado.\n",
    "    tokenizer : Any\n",
    "        Tokenizer compatível com a API do `tiktoken` (precisa ter `.encode`).\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Tensor com shape (1, seq_len), contendo os IDs de tokens.\n",
    "\n",
    "    Exceções:\n",
    "    --------\n",
    "    Levanta TypeError se `text` não for string.\n",
    "    Levanta RuntimeError se falhar ao codificar o texto.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(\"O parâmetro `text` deve ser do tipo str.\")\n",
    "\n",
    "    try:\n",
    "        encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)  # adiciona batch\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Falha ao converter texto em token IDs.\") from e\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids: torch.Tensor, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Converte um tensor de IDs de tokens (com dimensão de batch) de volta para texto.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    token_ids : torch.Tensor\n",
    "        Tensor com shape (1, seq_len) (ou compatível) contendo IDs de tokens.\n",
    "    tokenizer : Any\n",
    "        Tokenizer compatível com a API do `tiktoken` (precisa ter `.decode`).\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    str\n",
    "        Texto decodificado.\n",
    "\n",
    "    Exceções:\n",
    "    --------\n",
    "    Levanta TypeError se `token_ids` não for um torch.Tensor.\n",
    "    Levanta ValueError se o tensor estiver vazio.\n",
    "    \"\"\"\n",
    "    if not isinstance(token_ids, torch.Tensor):\n",
    "        raise TypeError(\"O parâmetro `token_ids` deve ser um torch.Tensor.\")\n",
    "    if token_ids.numel() == 0:\n",
    "        raise ValueError(\"`token_ids` não pode estar vazio.\")\n",
    "\n",
    "    flat = token_ids.squeeze(0)  # remove batch\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd6ed9d-f107-4522-a3b0-e5974e3e90f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      "Uma noite destas, vindo da cidade para oralcycleardedEXPladoin grittyfracrenheit days\n"
     ]
    }
   ],
   "source": [
    "# Texto inicial (prompt) para geração\n",
    "start_context = \"Uma noite destas, vindo da cidade para o\"\n",
    "\n",
    "# Inicializa o tokenizer (encoding compatível com GPT-2)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Converte o texto em IDs de tokens e gera novos tokens\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "\n",
    "# Texto será aleatório pois o modelo não foi treinado\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(f\"Output text:\\n{output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b565e3-e4e9-4edb-8838-f0b673722193",
   "metadata": {},
   "source": [
    "## Buscando o livro 'Dom Casmurro' de Machado de Assis (Projeto Gutenberg)\n",
    "\n",
    "Url.: https://www.gutenberg.org/ebooks/55752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b07412-e6f5-4c38-bd33-dd4686ffbd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: dom_casmurro.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../../data/dom_casmurro.txt\"\n",
    "url = \"https://www.gutenberg.org/cache/epub/55752/pg55752.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    text_data = response.text\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "print(\"Salvo:\", \"dom_casmurro.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0f0f087-4570-4f81-ab16-f6a9b292595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção das marcas do projeto gutenberg\n",
    "idx_ini = text_data.find(\"*** START OF THE PROJECT GUTENBERG EBOOK DOM CASMURRO ***\")\n",
    "idx_ini += 59\n",
    "\n",
    "idx_fim = text_data.find(\"*** END OF THE PROJECT GUTENBERG EBOOK DOM CASMURRO ***\")\n",
    "idx_fim -= 9\n",
    "\n",
    "text_data = text_data[idx_ini:idx_fim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b39dd49-4b67-4aeb-b3ef-ec32dc0a24b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de caracteres: 381375\n",
      "Total de tokens: 155540\n",
      "Relação: 2.45 caracteres por token\n"
     ]
    }
   ],
   "source": [
    "# Calcula estatísticas do texto\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "# Exibe os resultados\n",
    "print(\"Total de caracteres:\", total_characters)\n",
    "print(\"Total de tokens:\", total_tokens)\n",
    "print(f\"Relação: {total_characters / total_tokens:.2f} caracteres por token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab1d93-add1-4f61-9888-919ebfacc890",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a194de2-e59f-457e-88aa-25d664585d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide o texto em treino e validação\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "# Cria o DataLoader de treino\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Cria o DataLoader de validação\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a97e420-cd2e-4cb8-8961-81451566b7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Inspeciona os batches do DataLoader de treino\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "# Inspeciona os batches do DataLoader de validação\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331c4ccb-80fd-4e40-a79a-e709e18b6dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 138240\n",
      "Validation tokens: 16896\n",
      "All tokens: 155136\n"
     ]
    }
   ],
   "source": [
    "# Conta a quantidade de tokens (elementos) nos batches de treino\n",
    "train_tokens = 0\n",
    "for input_batch, _target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "# Conta a quantidade de tokens (elementos) nos batches de validação\n",
    "val_tokens = 0\n",
    "for input_batch, _target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "# Exibe os totais\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cde4d59b-cede-46fa-98a8-40d05f6610ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(\n",
    "    input_batch: torch.Tensor,\n",
    "    target_batch: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calcula a loss (cross-entropy) para um único batch.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    input_batch : torch.Tensor\n",
    "        Tensor de entradas (tokens) do batch.\n",
    "    target_batch : torch.Tensor\n",
    "        Tensor de targets do batch.\n",
    "    model : nn.Module\n",
    "        Modelo que retorna logits no forward.\n",
    "    device : torch.device\n",
    "        Dispositivo de execução (CPU/GPU).\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Loss do batch (tensor escalar).\n",
    "    \"\"\"\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    logits = model(input_batch)  # esperado: (B, T, V)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1),  # (B*T, V)\n",
    "        target_batch.flatten(),  # (B*T,)\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(\n",
    "    data_loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    num_batches: int | None = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calcula a loss média ao longo de um DataLoader.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    data_loader : DataLoader\n",
    "        DataLoader que retorna (input_batch, target_batch).\n",
    "    model : nn.Module\n",
    "        Modelo que retorna logits no forward.\n",
    "    device : torch.device\n",
    "        Dispositivo de execução (CPU/GPU).\n",
    "    num_batches : int | None, default = None\n",
    "        Se fornecido, limita o cálculo aos primeiros `num_batches` batches.\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    float\n",
    "        Loss média no conjunto avaliado. Retorna NaN se o loader estiver vazio.\n",
    "    \"\"\"\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    max_batches = (\n",
    "        len(data_loader) if num_batches is None else min(num_batches, len(data_loader))\n",
    "    )\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "    return total_loss / max_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76acb0e1-3396-4d3a-ac0d-afe2c6d90929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Retorna o device apropriado (CUDA se disponível, caso contrário CPU).\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8008c6c0-b4b9-4ca3-9218-07dd3abc790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987617390244095\n",
      "Validation loss: 11.049835031682795\n"
     ]
    }
   ],
   "source": [
    "# Seleciona o dispositivo (CPU/GPU) e move o modelo\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "\n",
    "# Calcula as losses sem rastrear gradientes\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36a429ab-ca47-484d-8bed-5f3ad314a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    device: torch.device,\n",
    "    num_epochs: int,\n",
    "    eval_freq: int,\n",
    "    eval_iter: int,\n",
    "    start_context: str,\n",
    "    tokenizer,\n",
    ") -> tuple[list[float], list[float], list[int]]:\n",
    "    \"\"\"\n",
    "    Treina o modelo e avalia periodicamente, registrando perdas e tokens vistos.\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    tuple[list[float], list[float], list[int]]\n",
    "        (train_losses, val_losses, track_tokens_seen)\n",
    "    \"\"\"\n",
    "    # Inicializa listas para acompanhar perdas e tokens vistos\n",
    "    train_losses: list[float] = []\n",
    "    val_losses: list[float] = []\n",
    "    track_tokens_seen: list[int] = []\n",
    "\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "\n",
    "    # Loop principal de treinamento\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Coloca o modelo em modo de treino\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Zera os gradientes acumulados do batch anterior\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calcula os gradientes da loss\n",
    "            optimizer.step()  # Atualiza os pesos do modelo usando os gradientes\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Etapa opcional de avaliação\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    device=device,\n",
    "                    eval_iter=eval_iter,\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(\n",
    "                    f\"Ep {epoch + 1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        # Imprime um texto de exemplo ao final de cada época\n",
    "        generate_and_print_sample(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            start_context=start_context,\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    eval_iter: int,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Avalia o modelo em treino e validação usando um número limitado de batches.\n",
    "    \"\"\"\n",
    "    model.eval()  # Coloca o modelo em modo de avaliação\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            data_loader=train_loader,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            num_batches=eval_iter,\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            data_loader=val_loader,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            num_batches=eval_iter,\n",
    "        )\n",
    "    model.train()  # Restaura o modo de treino\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    device: torch.device,\n",
    "    start_context: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Gera um pequeno texto a partir de `start_context` e imprime na tela.\n",
    "    \"\"\"\n",
    "    model.eval()  # Coloca o modelo em modo de avaliação\n",
    "\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size,\n",
    "        )\n",
    "\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Impressão compacta (sem quebras de linha)\n",
    "\n",
    "    model.train()  # Restaura o modo de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5d17a-19c1-44aa-9353-f83a65e136b1",
   "metadata": {},
   "source": [
    "## Início do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c210f2c-e037-44c9-b510-f7a06ba3adb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.947, Val loss 10.030\n",
      "Ep 1 (Step 000005): Train loss 8.215, Val loss 8.272\n",
      "Ep 1 (Step 000010): Train loss 6.953, Val loss 6.996\n",
      "Ep 1 (Step 000015): Train loss 6.456, Val loss 6.527\n",
      "Ep 1 (Step 000020): Train loss 6.305, Val loss 6.430\n",
      "Ep 1 (Step 000025): Train loss 6.310, Val loss 6.300\n",
      "Ep 1 (Step 000030): Train loss 6.053, Val loss 6.163\n",
      "Ep 1 (Step 000035): Train loss 5.952, Val loss 6.073\n",
      "Ep 1 (Step 000040): Train loss 5.746, Val loss 5.920\n",
      "Ep 1 (Step 000045): Train loss 5.748, Val loss 5.822\n",
      "Ep 1 (Step 000050): Train loss 5.537, Val loss 5.717\n",
      "Ep 1 (Step 000055): Train loss 5.342, Val loss 5.591\n",
      "Ep 1 (Step 000060): Train loss 5.392, Val loss 5.493\n",
      "Ep 1 (Step 000065): Train loss 5.135, Val loss 5.399\n",
      "Ep 1 (Step 000070): Train loss 5.300, Val loss 5.368\n",
      "Ep 1 (Step 000075): Train loss 5.002, Val loss 5.261\n",
      "Ep 1 (Step 000080): Train loss 5.050, Val loss 5.234\n",
      "Ep 1 (Step 000085): Train loss 4.858, Val loss 5.148\n",
      "Ep 1 (Step 000090): Train loss 4.788, Val loss 5.116\n",
      "Ep 1 (Step 000095): Train loss 4.852, Val loss 5.077\n",
      "Ep 1 (Step 000100): Train loss 4.940, Val loss 5.019\n",
      "Ep 1 (Step 000105): Train loss 4.628, Val loss 4.974\n",
      "Ep 1 (Step 000110): Train loss 4.755, Val loss 4.952\n",
      "Ep 1 (Step 000115): Train loss 4.737, Val loss 4.944\n",
      "Ep 1 (Step 000120): Train loss 4.663, Val loss 4.912\n",
      "Ep 1 (Step 000125): Train loss 4.650, Val loss 4.886\n",
      "Ep 1 (Step 000130): Train loss 4.490, Val loss 4.853\n",
      "Ep 1 (Step 000135): Train loss 4.573, Val loss 4.824\n",
      "Ep 1 (Step 000140): Train loss 4.625, Val loss 4.838\n",
      "Ep 1 (Step 000145): Train loss 4.535, Val loss 4.772\n",
      "Ep 1 (Step 000150): Train loss 4.471, Val loss 4.775\n",
      "Ep 1 (Step 000155): Train loss 4.605, Val loss 4.785\n",
      "Ep 1 (Step 000160): Train loss 4.437, Val loss 4.709\n",
      "Ep 1 (Step 000165): Train loss 4.314, Val loss 4.703\n",
      "Ep 1 (Step 000170): Train loss 4.404, Val loss 4.702\n",
      "Ep 1 (Step 000175): Train loss 4.276, Val loss 4.710\n",
      "Ep 1 (Step 000180): Train loss 4.532, Val loss 4.664\n",
      "Ep 1 (Step 000185): Train loss 4.338, Val loss 4.648\n",
      "Ep 1 (Step 000190): Train loss 4.227, Val loss 4.652\n",
      "Ep 1 (Step 000195): Train loss 4.254, Val loss 4.663\n",
      "Ep 1 (Step 000200): Train loss 4.243, Val loss 4.642\n",
      "Ep 1 (Step 000205): Train loss 4.170, Val loss 4.630\n",
      "Ep 1 (Step 000210): Train loss 4.055, Val loss 4.606\n",
      "Ep 1 (Step 000215): Train loss 4.119, Val loss 4.582\n",
      "Ep 1 (Step 000220): Train loss 4.025, Val loss 4.565\n",
      "Ep 1 (Step 000225): Train loss 4.161, Val loss 4.574\n",
      "Ep 1 (Step 000230): Train loss 3.987, Val loss 4.555\n",
      "Ep 1 (Step 000235): Train loss 4.069, Val loss 4.545\n",
      "Ep 1 (Step 000240): Train loss 4.045, Val loss 4.528\n",
      "Ep 1 (Step 000245): Train loss 4.124, Val loss 4.505\n",
      "Ep 1 (Step 000250): Train loss 4.180, Val loss 4.490\n",
      "Ep 1 (Step 000255): Train loss 4.150, Val loss 4.482\n",
      "Ep 1 (Step 000260): Train loss 4.125, Val loss 4.480\n",
      "Ep 1 (Step 000265): Train loss 4.003, Val loss 4.481\n",
      "Uma noite destas, vindo da cidade para ouviuvi,                               --A minha mãe, euviuviu\n",
      "Ep 2 (Step 000270): Train loss 4.020, Val loss 4.483\n",
      "Ep 2 (Step 000275): Train loss 3.815, Val loss 4.483\n",
      "Ep 2 (Step 000280): Train loss 4.132, Val loss 4.474\n",
      "Ep 2 (Step 000285): Train loss 4.257, Val loss 4.468\n",
      "Ep 2 (Step 000290): Train loss 3.877, Val loss 4.452\n",
      "Ep 2 (Step 000295): Train loss 3.958, Val loss 4.451\n",
      "Ep 2 (Step 000300): Train loss 3.971, Val loss 4.440\n",
      "Ep 2 (Step 000305): Train loss 3.872, Val loss 4.457\n",
      "Ep 2 (Step 000310): Train loss 3.784, Val loss 4.461\n",
      "Ep 2 (Step 000315): Train loss 4.022, Val loss 4.460\n",
      "Ep 2 (Step 000320): Train loss 3.925, Val loss 4.465\n",
      "Ep 2 (Step 000325): Train loss 3.935, Val loss 4.452\n",
      "Ep 2 (Step 000330): Train loss 3.866, Val loss 4.435\n",
      "Ep 2 (Step 000335): Train loss 4.025, Val loss 4.420\n",
      "Ep 2 (Step 000340): Train loss 3.801, Val loss 4.418\n",
      "Ep 2 (Step 000345): Train loss 3.919, Val loss 4.399\n",
      "Ep 2 (Step 000350): Train loss 3.732, Val loss 4.388\n",
      "Ep 2 (Step 000355): Train loss 3.789, Val loss 4.414\n",
      "Ep 2 (Step 000360): Train loss 3.691, Val loss 4.413\n",
      "Ep 2 (Step 000365): Train loss 3.733, Val loss 4.417\n",
      "Ep 2 (Step 000370): Train loss 3.666, Val loss 4.384\n",
      "Ep 2 (Step 000375): Train loss 3.941, Val loss 4.367\n",
      "Ep 2 (Step 000380): Train loss 3.894, Val loss 4.340\n",
      "Ep 2 (Step 000385): Train loss 3.725, Val loss 4.334\n",
      "Ep 2 (Step 000390): Train loss 3.764, Val loss 4.350\n",
      "Ep 2 (Step 000395): Train loss 3.642, Val loss 4.358\n",
      "Ep 2 (Step 000400): Train loss 3.730, Val loss 4.351\n",
      "Ep 2 (Step 000405): Train loss 3.494, Val loss 4.360\n",
      "Ep 2 (Step 000410): Train loss 3.888, Val loss 4.344\n",
      "Ep 2 (Step 000415): Train loss 3.520, Val loss 4.334\n",
      "Ep 2 (Step 000420): Train loss 3.564, Val loss 4.322\n",
      "Ep 2 (Step 000425): Train loss 3.785, Val loss 4.339\n",
      "Ep 2 (Step 000430): Train loss 3.638, Val loss 4.341\n",
      "Ep 2 (Step 000435): Train loss 3.728, Val loss 4.324\n",
      "Ep 2 (Step 000440): Train loss 3.834, Val loss 4.332\n",
      "Ep 2 (Step 000445): Train loss 3.554, Val loss 4.347\n",
      "Ep 2 (Step 000450): Train loss 3.623, Val loss 4.339\n",
      "Ep 2 (Step 000455): Train loss 3.583, Val loss 4.369\n",
      "Ep 2 (Step 000460): Train loss 3.865, Val loss 4.362\n",
      "Ep 2 (Step 000465): Train loss 3.627, Val loss 4.384\n",
      "Ep 2 (Step 000470): Train loss 3.415, Val loss 4.359\n",
      "Ep 2 (Step 000475): Train loss 3.701, Val loss 4.347\n",
      "Ep 2 (Step 000480): Train loss 3.638, Val loss 4.331\n",
      "Ep 2 (Step 000485): Train loss 3.393, Val loss 4.336\n",
      "Ep 2 (Step 000490): Train loss 3.609, Val loss 4.320\n",
      "Ep 2 (Step 000495): Train loss 3.520, Val loss 4.315\n",
      "Ep 2 (Step 000500): Train loss 3.520, Val loss 4.309\n",
      "Ep 2 (Step 000505): Train loss 3.613, Val loss 4.314\n",
      "Ep 2 (Step 000510): Train loss 3.489, Val loss 4.305\n",
      "Ep 2 (Step 000515): Train loss 3.600, Val loss 4.301\n",
      "Ep 2 (Step 000520): Train loss 3.432, Val loss 4.284\n",
      "Ep 2 (Step 000525): Train loss 3.585, Val loss 4.290\n",
      "Ep 2 (Step 000530): Train loss 3.568, Val loss 4.282\n",
      "Ep 2 (Step 000535): Train loss 3.517, Val loss 4.293\n",
      "Uma noite destas, vindo da cidade para o que ainda que não, e que ainda que não, e que ainda que não, e ainda que não, e que ainda que não, e ainda que ainda que a\n",
      "Ep 3 (Step 000540): Train loss 3.615, Val loss 4.281\n",
      "Ep 3 (Step 000545): Train loss 3.500, Val loss 4.269\n",
      "Ep 3 (Step 000550): Train loss 3.587, Val loss 4.264\n",
      "Ep 3 (Step 000555): Train loss 3.528, Val loss 4.244\n",
      "Ep 3 (Step 000560): Train loss 3.487, Val loss 4.272\n",
      "Ep 3 (Step 000565): Train loss 3.566, Val loss 4.259\n",
      "Ep 3 (Step 000570): Train loss 3.575, Val loss 4.278\n",
      "Ep 3 (Step 000575): Train loss 3.406, Val loss 4.276\n",
      "Ep 3 (Step 000580): Train loss 3.660, Val loss 4.274\n",
      "Ep 3 (Step 000585): Train loss 3.424, Val loss 4.274\n",
      "Ep 3 (Step 000590): Train loss 3.501, Val loss 4.259\n",
      "Ep 3 (Step 000595): Train loss 3.544, Val loss 4.285\n",
      "Ep 3 (Step 000600): Train loss 3.405, Val loss 4.307\n",
      "Ep 3 (Step 000605): Train loss 3.540, Val loss 4.321\n",
      "Ep 3 (Step 000610): Train loss 3.437, Val loss 4.309\n",
      "Ep 3 (Step 000615): Train loss 3.480, Val loss 4.323\n",
      "Ep 3 (Step 000620): Train loss 3.390, Val loss 4.304\n",
      "Ep 3 (Step 000625): Train loss 3.191, Val loss 4.305\n",
      "Ep 3 (Step 000630): Train loss 3.424, Val loss 4.304\n",
      "Ep 3 (Step 000635): Train loss 3.439, Val loss 4.291\n",
      "Ep 3 (Step 000640): Train loss 3.394, Val loss 4.277\n",
      "Ep 3 (Step 000645): Train loss 3.256, Val loss 4.294\n",
      "Ep 3 (Step 000650): Train loss 3.381, Val loss 4.282\n",
      "Ep 3 (Step 000655): Train loss 3.158, Val loss 4.285\n",
      "Ep 3 (Step 000660): Train loss 3.679, Val loss 4.322\n",
      "Ep 3 (Step 000665): Train loss 3.503, Val loss 4.312\n",
      "Ep 3 (Step 000670): Train loss 3.393, Val loss 4.292\n",
      "Ep 3 (Step 000675): Train loss 3.163, Val loss 4.289\n",
      "Ep 3 (Step 000680): Train loss 3.346, Val loss 4.307\n",
      "Ep 3 (Step 000685): Train loss 3.433, Val loss 4.282\n",
      "Ep 3 (Step 000690): Train loss 3.359, Val loss 4.259\n",
      "Ep 3 (Step 000695): Train loss 3.176, Val loss 4.274\n",
      "Ep 3 (Step 000700): Train loss 3.523, Val loss 4.287\n",
      "Ep 3 (Step 000705): Train loss 3.402, Val loss 4.308\n",
      "Ep 3 (Step 000710): Train loss 3.085, Val loss 4.268\n",
      "Ep 3 (Step 000715): Train loss 3.520, Val loss 4.259\n",
      "Ep 3 (Step 000720): Train loss 3.261, Val loss 4.268\n",
      "Ep 3 (Step 000725): Train loss 3.239, Val loss 4.272\n",
      "Ep 3 (Step 000730): Train loss 3.391, Val loss 4.263\n",
      "Ep 3 (Step 000735): Train loss 3.431, Val loss 4.258\n",
      "Ep 3 (Step 000740): Train loss 3.283, Val loss 4.297\n",
      "Ep 3 (Step 000745): Train loss 3.299, Val loss 4.292\n",
      "Ep 3 (Step 000750): Train loss 3.128, Val loss 4.251\n",
      "Ep 3 (Step 000755): Train loss 3.106, Val loss 4.258\n",
      "Ep 3 (Step 000760): Train loss 3.283, Val loss 4.270\n",
      "Ep 3 (Step 000765): Train loss 3.365, Val loss 4.272\n",
      "Ep 3 (Step 000770): Train loss 3.158, Val loss 4.254\n",
      "Ep 3 (Step 000775): Train loss 3.287, Val loss 4.260\n",
      "Ep 3 (Step 000780): Train loss 3.102, Val loss 4.247\n",
      "Ep 3 (Step 000785): Train loss 3.144, Val loss 4.232\n",
      "Ep 3 (Step 000790): Train loss 3.244, Val loss 4.228\n",
      "Ep 3 (Step 000795): Train loss 3.294, Val loss 4.228\n",
      "Ep 3 (Step 000800): Train loss 3.044, Val loss 4.243\n",
      "Ep 3 (Step 000805): Train loss 3.135, Val loss 4.243\n",
      "Uma noite destas, vindo da cidade para ou cção, mas ao, eu, caminho, mas ao, eu, eu não de um parecia, cado, eu não. que ao, a\n",
      "Ep 4 (Step 000810): Train loss 3.055, Val loss 4.258\n",
      "Ep 4 (Step 000815): Train loss 3.187, Val loss 4.267\n",
      "Ep 4 (Step 000820): Train loss 3.053, Val loss 4.253\n",
      "Ep 4 (Step 000825): Train loss 3.020, Val loss 4.248\n",
      "Ep 4 (Step 000830): Train loss 2.980, Val loss 4.274\n",
      "Ep 4 (Step 000835): Train loss 3.077, Val loss 4.268\n",
      "Ep 4 (Step 000840): Train loss 2.956, Val loss 4.290\n",
      "Ep 4 (Step 000845): Train loss 3.060, Val loss 4.306\n",
      "Ep 4 (Step 000850): Train loss 2.975, Val loss 4.311\n",
      "Ep 4 (Step 000855): Train loss 2.946, Val loss 4.339\n",
      "Ep 4 (Step 000860): Train loss 2.880, Val loss 4.332\n",
      "Ep 4 (Step 000865): Train loss 2.879, Val loss 4.344\n",
      "Ep 4 (Step 000870): Train loss 3.140, Val loss 4.313\n",
      "Ep 4 (Step 000875): Train loss 2.923, Val loss 4.298\n",
      "Ep 4 (Step 000880): Train loss 2.880, Val loss 4.311\n",
      "Ep 4 (Step 000885): Train loss 3.040, Val loss 4.323\n",
      "Ep 4 (Step 000890): Train loss 3.052, Val loss 4.331\n",
      "Ep 4 (Step 000895): Train loss 2.931, Val loss 4.364\n",
      "Ep 4 (Step 000900): Train loss 2.924, Val loss 4.329\n",
      "Ep 4 (Step 000905): Train loss 3.070, Val loss 4.330\n",
      "Ep 4 (Step 000910): Train loss 2.777, Val loss 4.315\n",
      "Ep 4 (Step 000915): Train loss 2.922, Val loss 4.323\n",
      "Ep 4 (Step 000920): Train loss 2.678, Val loss 4.319\n",
      "Ep 4 (Step 000925): Train loss 2.731, Val loss 4.318\n",
      "Ep 4 (Step 000930): Train loss 2.822, Val loss 4.326\n",
      "Ep 4 (Step 000935): Train loss 2.984, Val loss 4.359\n",
      "Ep 4 (Step 000940): Train loss 2.924, Val loss 4.353\n",
      "Ep 4 (Step 000945): Train loss 2.977, Val loss 4.339\n",
      "Ep 4 (Step 000950): Train loss 2.677, Val loss 4.303\n",
      "Ep 4 (Step 000955): Train loss 2.876, Val loss 4.304\n",
      "Ep 4 (Step 000960): Train loss 3.004, Val loss 4.314\n",
      "Ep 4 (Step 000965): Train loss 2.808, Val loss 4.304\n",
      "Ep 4 (Step 000970): Train loss 2.715, Val loss 4.305\n",
      "Ep 4 (Step 000975): Train loss 2.796, Val loss 4.298\n",
      "Ep 4 (Step 000980): Train loss 2.677, Val loss 4.325\n",
      "Ep 4 (Step 000985): Train loss 2.835, Val loss 4.321\n",
      "Ep 4 (Step 000990): Train loss 2.860, Val loss 4.294\n",
      "Ep 4 (Step 000995): Train loss 2.800, Val loss 4.297\n",
      "Ep 4 (Step 001000): Train loss 2.618, Val loss 4.312\n",
      "Ep 4 (Step 001005): Train loss 2.682, Val loss 4.323\n",
      "Ep 4 (Step 001010): Train loss 2.873, Val loss 4.322\n",
      "Ep 4 (Step 001015): Train loss 2.838, Val loss 4.318\n",
      "Ep 4 (Step 001020): Train loss 2.939, Val loss 4.350\n",
      "Ep 4 (Step 001025): Train loss 2.886, Val loss 4.363\n",
      "Ep 4 (Step 001030): Train loss 2.793, Val loss 4.364\n",
      "Ep 4 (Step 001035): Train loss 2.568, Val loss 4.372\n",
      "Ep 4 (Step 001040): Train loss 2.846, Val loss 4.333\n",
      "Ep 4 (Step 001045): Train loss 2.621, Val loss 4.302\n",
      "Ep 4 (Step 001050): Train loss 2.669, Val loss 4.305\n",
      "Ep 4 (Step 001055): Train loss 2.782, Val loss 4.314\n",
      "Ep 4 (Step 001060): Train loss 2.908, Val loss 4.308\n",
      "Ep 4 (Step 001065): Train loss 2.715, Val loss 4.321\n",
      "Ep 4 (Step 001070): Train loss 2.714, Val loss 4.319\n",
      "Ep 4 (Step 001075): Train loss 2.639, Val loss 4.318\n",
      "Uma noite destas, vindo da cidade para ou a, ao ao dares, como da vida, eu que ao ao da eu, como, a que ao da vida, como, como eu, com\n",
      "Ep 5 (Step 001080): Train loss 2.737, Val loss 4.317\n",
      "Ep 5 (Step 001085): Train loss 2.606, Val loss 4.317\n",
      "Ep 5 (Step 001090): Train loss 2.643, Val loss 4.348\n",
      "Ep 5 (Step 001095): Train loss 2.661, Val loss 4.333\n",
      "Ep 5 (Step 001100): Train loss 2.432, Val loss 4.382\n",
      "Ep 5 (Step 001105): Train loss 2.505, Val loss 4.375\n",
      "Ep 5 (Step 001110): Train loss 2.780, Val loss 4.383\n",
      "Ep 5 (Step 001115): Train loss 2.361, Val loss 4.380\n",
      "Ep 5 (Step 001120): Train loss 2.537, Val loss 4.393\n",
      "Ep 5 (Step 001125): Train loss 2.478, Val loss 4.397\n",
      "Ep 5 (Step 001130): Train loss 2.435, Val loss 4.424\n",
      "Ep 5 (Step 001135): Train loss 2.516, Val loss 4.418\n",
      "Ep 5 (Step 001140): Train loss 2.502, Val loss 4.428\n",
      "Ep 5 (Step 001145): Train loss 2.469, Val loss 4.447\n",
      "Ep 5 (Step 001150): Train loss 2.282, Val loss 4.476\n",
      "Ep 5 (Step 001155): Train loss 2.569, Val loss 4.453\n",
      "Ep 5 (Step 001160): Train loss 2.384, Val loss 4.453\n",
      "Ep 5 (Step 001165): Train loss 2.608, Val loss 4.434\n",
      "Ep 5 (Step 001170): Train loss 2.293, Val loss 4.430\n",
      "Ep 5 (Step 001175): Train loss 2.376, Val loss 4.475\n",
      "Ep 5 (Step 001180): Train loss 2.468, Val loss 4.472\n",
      "Ep 5 (Step 001185): Train loss 2.497, Val loss 4.493\n",
      "Ep 5 (Step 001190): Train loss 2.533, Val loss 4.531\n",
      "Ep 5 (Step 001195): Train loss 2.370, Val loss 4.514\n",
      "Ep 5 (Step 001200): Train loss 2.612, Val loss 4.509\n",
      "Ep 5 (Step 001205): Train loss 2.428, Val loss 4.491\n",
      "Ep 5 (Step 001210): Train loss 2.250, Val loss 4.495\n",
      "Ep 5 (Step 001215): Train loss 2.342, Val loss 4.505\n",
      "Ep 5 (Step 001220): Train loss 2.622, Val loss 4.485\n",
      "Ep 5 (Step 001225): Train loss 2.280, Val loss 4.503\n",
      "Ep 5 (Step 001230): Train loss 2.329, Val loss 4.472\n",
      "Ep 5 (Step 001235): Train loss 2.221, Val loss 4.459\n",
      "Ep 5 (Step 001240): Train loss 2.385, Val loss 4.455\n",
      "Ep 5 (Step 001245): Train loss 2.203, Val loss 4.476\n",
      "Ep 5 (Step 001250): Train loss 2.338, Val loss 4.483\n",
      "Ep 5 (Step 001255): Train loss 2.367, Val loss 4.477\n",
      "Ep 5 (Step 001260): Train loss 2.408, Val loss 4.498\n",
      "Ep 5 (Step 001265): Train loss 2.241, Val loss 4.498\n",
      "Ep 5 (Step 001270): Train loss 2.298, Val loss 4.500\n",
      "Ep 5 (Step 001275): Train loss 2.004, Val loss 4.479\n",
      "Ep 5 (Step 001280): Train loss 2.335, Val loss 4.481\n",
      "Ep 5 (Step 001285): Train loss 2.460, Val loss 4.481\n",
      "Ep 5 (Step 001290): Train loss 2.497, Val loss 4.470\n",
      "Ep 5 (Step 001295): Train loss 2.473, Val loss 4.479\n",
      "Ep 5 (Step 001300): Train loss 2.277, Val loss 4.492\n",
      "Ep 5 (Step 001305): Train loss 2.050, Val loss 4.491\n",
      "Ep 5 (Step 001310): Train loss 2.270, Val loss 4.478\n",
      "Ep 5 (Step 001315): Train loss 2.065, Val loss 4.468\n",
      "Ep 5 (Step 001320): Train loss 2.312, Val loss 4.491\n",
      "Ep 5 (Step 001325): Train loss 2.193, Val loss 4.522\n",
      "Ep 5 (Step 001330): Train loss 2.189, Val loss 4.499\n",
      "Ep 5 (Step 001335): Train loss 2.010, Val loss 4.466\n",
      "Ep 5 (Step 001340): Train loss 2.043, Val loss 4.459\n",
      "Ep 5 (Step 001345): Train loss 2.024, Val loss 4.477\n",
      "Uma noite destas, vindo da cidade para ou que ao, eu que ao, não achei para mim, e não me lembrasse um cabeça dos cimento, eu cocheiro, que me lembra\n",
      "Ep 6 (Step 001350): Train loss 2.099, Val loss 4.480\n",
      "Ep 6 (Step 001355): Train loss 2.270, Val loss 4.530\n",
      "Ep 6 (Step 001360): Train loss 2.013, Val loss 4.548\n",
      "Ep 6 (Step 001365): Train loss 1.933, Val loss 4.556\n",
      "Ep 6 (Step 001370): Train loss 2.168, Val loss 4.586\n",
      "Ep 6 (Step 001375): Train loss 1.793, Val loss 4.599\n",
      "Ep 6 (Step 001380): Train loss 1.975, Val loss 4.577\n",
      "Ep 6 (Step 001385): Train loss 2.042, Val loss 4.580\n",
      "Ep 6 (Step 001390): Train loss 1.948, Val loss 4.608\n",
      "Ep 6 (Step 001395): Train loss 1.943, Val loss 4.611\n",
      "Ep 6 (Step 001400): Train loss 1.722, Val loss 4.587\n",
      "Ep 6 (Step 001405): Train loss 1.971, Val loss 4.661\n",
      "Ep 6 (Step 001410): Train loss 2.039, Val loss 4.629\n",
      "Ep 6 (Step 001415): Train loss 2.014, Val loss 4.622\n",
      "Ep 6 (Step 001420): Train loss 2.028, Val loss 4.651\n",
      "Ep 6 (Step 001425): Train loss 2.069, Val loss 4.674\n",
      "Ep 6 (Step 001430): Train loss 1.697, Val loss 4.692\n",
      "Ep 6 (Step 001435): Train loss 1.807, Val loss 4.713\n",
      "Ep 6 (Step 001440): Train loss 1.501, Val loss 4.677\n",
      "Ep 6 (Step 001445): Train loss 1.881, Val loss 4.697\n",
      "Ep 6 (Step 001450): Train loss 1.662, Val loss 4.690\n",
      "Ep 6 (Step 001455): Train loss 1.914, Val loss 4.716\n",
      "Ep 6 (Step 001460): Train loss 1.904, Val loss 4.697\n",
      "Ep 6 (Step 001465): Train loss 1.924, Val loss 4.705\n",
      "Ep 6 (Step 001470): Train loss 2.283, Val loss 4.718\n",
      "Ep 6 (Step 001475): Train loss 1.632, Val loss 4.724\n",
      "Ep 6 (Step 001480): Train loss 1.748, Val loss 4.686\n",
      "Ep 6 (Step 001485): Train loss 1.816, Val loss 4.706\n",
      "Ep 6 (Step 001490): Train loss 1.730, Val loss 4.721\n",
      "Ep 6 (Step 001495): Train loss 1.826, Val loss 4.715\n",
      "Ep 6 (Step 001500): Train loss 1.702, Val loss 4.742\n",
      "Ep 6 (Step 001505): Train loss 2.008, Val loss 4.719\n",
      "Ep 6 (Step 001510): Train loss 1.804, Val loss 4.695\n",
      "Ep 6 (Step 001515): Train loss 1.781, Val loss 4.713\n",
      "Ep 6 (Step 001520): Train loss 1.464, Val loss 4.706\n",
      "Ep 6 (Step 001525): Train loss 1.468, Val loss 4.737\n",
      "Ep 6 (Step 001530): Train loss 1.677, Val loss 4.744\n",
      "Ep 6 (Step 001535): Train loss 1.862, Val loss 4.716\n",
      "Ep 6 (Step 001540): Train loss 1.977, Val loss 4.726\n",
      "Ep 6 (Step 001545): Train loss 1.695, Val loss 4.723\n",
      "Ep 6 (Step 001550): Train loss 1.625, Val loss 4.708\n",
      "Ep 6 (Step 001555): Train loss 1.865, Val loss 4.734\n",
      "Ep 6 (Step 001560): Train loss 1.755, Val loss 4.718\n",
      "Ep 6 (Step 001565): Train loss 1.625, Val loss 4.704\n",
      "Ep 6 (Step 001570): Train loss 1.411, Val loss 4.727\n",
      "Ep 6 (Step 001575): Train loss 1.645, Val loss 4.724\n",
      "Ep 6 (Step 001580): Train loss 1.668, Val loss 4.709\n",
      "Ep 6 (Step 001585): Train loss 1.675, Val loss 4.709\n",
      "Ep 6 (Step 001590): Train loss 1.548, Val loss 4.702\n",
      "Ep 6 (Step 001595): Train loss 1.530, Val loss 4.714\n",
      "Ep 6 (Step 001600): Train loss 1.655, Val loss 4.727\n",
      "Ep 6 (Step 001605): Train loss 1.539, Val loss 4.723\n",
      "Ep 6 (Step 001610): Train loss 1.696, Val loss 4.746\n",
      "Ep 6 (Step 001615): Train loss 1.522, Val loss 4.727\n",
      "Uma noite destas, vindo da cidade para ou, e que é que fazia acostumara dia, e peita. E os temposina.  Não ainda  --Vásino, eu lá. Eu\n",
      "Ep 7 (Step 001620): Train loss 1.432, Val loss 4.748\n",
      "Ep 7 (Step 001625): Train loss 1.486, Val loss 4.760\n",
      "Ep 7 (Step 001630): Train loss 1.401, Val loss 4.789\n",
      "Ep 7 (Step 001635): Train loss 1.408, Val loss 4.806\n",
      "Ep 7 (Step 001640): Train loss 1.368, Val loss 4.838\n",
      "Ep 7 (Step 001645): Train loss 1.318, Val loss 4.868\n",
      "Ep 7 (Step 001650): Train loss 1.504, Val loss 4.883\n",
      "Ep 7 (Step 001655): Train loss 1.301, Val loss 4.878\n",
      "Ep 7 (Step 001660): Train loss 1.326, Val loss 4.863\n",
      "Ep 7 (Step 001665): Train loss 1.497, Val loss 4.889\n",
      "Ep 7 (Step 001670): Train loss 1.122, Val loss 4.901\n",
      "Ep 7 (Step 001675): Train loss 1.198, Val loss 4.924\n",
      "Ep 7 (Step 001680): Train loss 1.101, Val loss 4.927\n",
      "Ep 7 (Step 001685): Train loss 1.244, Val loss 4.933\n",
      "Ep 7 (Step 001690): Train loss 1.275, Val loss 4.971\n",
      "Ep 7 (Step 001695): Train loss 1.347, Val loss 4.985\n",
      "Ep 7 (Step 001700): Train loss 1.121, Val loss 4.991\n",
      "Ep 7 (Step 001705): Train loss 1.332, Val loss 4.987\n",
      "Ep 7 (Step 001710): Train loss 1.312, Val loss 4.975\n",
      "Ep 7 (Step 001715): Train loss 1.182, Val loss 5.001\n",
      "Ep 7 (Step 001720): Train loss 1.278, Val loss 5.030\n",
      "Ep 7 (Step 001725): Train loss 1.219, Val loss 5.011\n",
      "Ep 7 (Step 001730): Train loss 1.160, Val loss 4.993\n",
      "Ep 7 (Step 001735): Train loss 1.321, Val loss 5.020\n",
      "Ep 7 (Step 001740): Train loss 1.286, Val loss 4.998\n",
      "Ep 7 (Step 001745): Train loss 0.747, Val loss 4.994\n",
      "Ep 7 (Step 001750): Train loss 0.974, Val loss 4.994\n",
      "Ep 7 (Step 001755): Train loss 1.019, Val loss 4.983\n",
      "Ep 7 (Step 001760): Train loss 1.174, Val loss 4.992\n",
      "Ep 7 (Step 001765): Train loss 1.141, Val loss 5.009\n",
      "Ep 7 (Step 001770): Train loss 1.073, Val loss 5.035\n",
      "Ep 7 (Step 001775): Train loss 1.295, Val loss 5.025\n",
      "Ep 7 (Step 001780): Train loss 1.128, Val loss 5.002\n",
      "Ep 7 (Step 001785): Train loss 1.340, Val loss 5.026\n",
      "Ep 7 (Step 001790): Train loss 1.158, Val loss 4.997\n",
      "Ep 7 (Step 001795): Train loss 1.221, Val loss 5.002\n",
      "Ep 7 (Step 001800): Train loss 1.200, Val loss 5.033\n",
      "Ep 7 (Step 001805): Train loss 1.198, Val loss 5.025\n",
      "Ep 7 (Step 001810): Train loss 0.945, Val loss 5.034\n",
      "Ep 7 (Step 001815): Train loss 0.937, Val loss 5.065\n",
      "Ep 7 (Step 001820): Train loss 1.166, Val loss 5.062\n",
      "Ep 7 (Step 001825): Train loss 1.139, Val loss 5.012\n",
      "Ep 7 (Step 001830): Train loss 0.889, Val loss 5.012\n",
      "Ep 7 (Step 001835): Train loss 0.935, Val loss 5.026\n",
      "Ep 7 (Step 001840): Train loss 0.958, Val loss 5.012\n",
      "Ep 7 (Step 001845): Train loss 1.267, Val loss 4.981\n",
      "Ep 7 (Step 001850): Train loss 1.087, Val loss 5.005\n",
      "Ep 7 (Step 001855): Train loss 1.033, Val loss 5.047\n",
      "Ep 7 (Step 001860): Train loss 1.260, Val loss 5.016\n",
      "Ep 7 (Step 001865): Train loss 0.939, Val loss 5.029\n",
      "Ep 7 (Step 001870): Train loss 0.989, Val loss 5.030\n",
      "Ep 7 (Step 001875): Train loss 0.979, Val loss 4.988\n",
      "Ep 7 (Step 001880): Train loss 0.995, Val loss 4.989\n",
      "Ep 7 (Step 001885): Train loss 1.071, Val loss 5.015\n",
      "Uma noite destas, vindo da cidade para ou. Tinha ouvi Não, não vier em um anno bonito. E se pesso, como, dezenove é tudo o que não digo, não pess\n",
      "Ep 8 (Step 001890): Train loss 0.803, Val loss 4.996\n",
      "Ep 8 (Step 001895): Train loss 1.029, Val loss 5.063\n",
      "Ep 8 (Step 001900): Train loss 0.990, Val loss 5.098\n",
      "Ep 8 (Step 001905): Train loss 0.680, Val loss 5.166\n",
      "Ep 8 (Step 001910): Train loss 0.793, Val loss 5.159\n",
      "Ep 8 (Step 001915): Train loss 0.724, Val loss 5.141\n",
      "Ep 8 (Step 001920): Train loss 0.944, Val loss 5.176\n",
      "Ep 8 (Step 001925): Train loss 0.795, Val loss 5.205\n",
      "Ep 8 (Step 001930): Train loss 0.910, Val loss 5.272\n",
      "Ep 8 (Step 001935): Train loss 0.693, Val loss 5.226\n",
      "Ep 8 (Step 001940): Train loss 0.714, Val loss 5.278\n",
      "Ep 8 (Step 001945): Train loss 0.743, Val loss 5.271\n",
      "Ep 8 (Step 001950): Train loss 0.823, Val loss 5.249\n",
      "Ep 8 (Step 001955): Train loss 0.642, Val loss 5.298\n",
      "Ep 8 (Step 001960): Train loss 0.465, Val loss 5.306\n",
      "Ep 8 (Step 001965): Train loss 0.786, Val loss 5.331\n",
      "Ep 8 (Step 001970): Train loss 0.778, Val loss 5.322\n",
      "Ep 8 (Step 001975): Train loss 0.785, Val loss 5.308\n",
      "Ep 8 (Step 001980): Train loss 0.806, Val loss 5.316\n",
      "Ep 8 (Step 001985): Train loss 0.597, Val loss 5.341\n",
      "Ep 8 (Step 001990): Train loss 0.948, Val loss 5.366\n",
      "Ep 8 (Step 001995): Train loss 0.782, Val loss 5.315\n",
      "Ep 8 (Step 002000): Train loss 0.835, Val loss 5.281\n",
      "Ep 8 (Step 002005): Train loss 0.666, Val loss 5.286\n",
      "Ep 8 (Step 002010): Train loss 0.893, Val loss 5.323\n",
      "Ep 8 (Step 002015): Train loss 0.641, Val loss 5.278\n",
      "Ep 8 (Step 002020): Train loss 0.913, Val loss 5.368\n",
      "Ep 8 (Step 002025): Train loss 0.470, Val loss 5.379\n",
      "Ep 8 (Step 002030): Train loss 0.975, Val loss 5.386\n",
      "Ep 8 (Step 002035): Train loss 0.690, Val loss 5.397\n",
      "Ep 8 (Step 002040): Train loss 0.659, Val loss 5.400\n",
      "Ep 8 (Step 002045): Train loss 0.686, Val loss 5.388\n",
      "Ep 8 (Step 002050): Train loss 0.789, Val loss 5.390\n",
      "Ep 8 (Step 002055): Train loss 0.664, Val loss 5.393\n",
      "Ep 8 (Step 002060): Train loss 0.769, Val loss 5.385\n",
      "Ep 8 (Step 002065): Train loss 0.849, Val loss 5.375\n",
      "Ep 8 (Step 002070): Train loss 0.697, Val loss 5.389\n",
      "Ep 8 (Step 002075): Train loss 0.864, Val loss 5.401\n",
      "Ep 8 (Step 002080): Train loss 0.546, Val loss 5.377\n",
      "Ep 8 (Step 002085): Train loss 0.531, Val loss 5.416\n",
      "Ep 8 (Step 002090): Train loss 0.595, Val loss 5.391\n",
      "Ep 8 (Step 002095): Train loss 0.509, Val loss 5.382\n",
      "Ep 8 (Step 002100): Train loss 0.747, Val loss 5.387\n",
      "Ep 8 (Step 002105): Train loss 0.555, Val loss 5.352\n",
      "Ep 8 (Step 002110): Train loss 0.852, Val loss 5.402\n",
      "Ep 8 (Step 002115): Train loss 0.655, Val loss 5.420\n",
      "Ep 8 (Step 002120): Train loss 0.737, Val loss 5.372\n",
      "Ep 8 (Step 002125): Train loss 0.602, Val loss 5.411\n",
      "Ep 8 (Step 002130): Train loss 0.693, Val loss 5.425\n",
      "Ep 8 (Step 002135): Train loss 0.416, Val loss 5.432\n",
      "Ep 8 (Step 002140): Train loss 0.536, Val loss 5.426\n",
      "Ep 8 (Step 002145): Train loss 0.708, Val loss 5.410\n",
      "Ep 8 (Step 002150): Train loss 0.549, Val loss 5.436\n",
      "Ep 8 (Step 002155): Train loss 0.642, Val loss 5.456\n",
      "Uma noite destas, vindo da cidade para o no governo, e a religiado, depois que alguma vez baile; não sejam com que eu eito, a escre com ao me dera, o que eu que os\n",
      "Ep 9 (Step 002160): Train loss 0.610, Val loss 5.428\n",
      "Ep 9 (Step 002165): Train loss 0.696, Val loss 5.511\n",
      "Ep 9 (Step 002170): Train loss 0.551, Val loss 5.552\n",
      "Ep 9 (Step 002175): Train loss 0.536, Val loss 5.560\n",
      "Ep 9 (Step 002180): Train loss 0.417, Val loss 5.561\n",
      "Ep 9 (Step 002185): Train loss 0.507, Val loss 5.522\n",
      "Ep 9 (Step 002190): Train loss 0.432, Val loss 5.571\n",
      "Ep 9 (Step 002195): Train loss 0.470, Val loss 5.601\n",
      "Ep 9 (Step 002200): Train loss 0.435, Val loss 5.609\n",
      "Ep 9 (Step 002205): Train loss 0.407, Val loss 5.625\n",
      "Ep 9 (Step 002210): Train loss 0.474, Val loss 5.589\n",
      "Ep 9 (Step 002215): Train loss 0.339, Val loss 5.572\n",
      "Ep 9 (Step 002220): Train loss 0.341, Val loss 5.583\n",
      "Ep 9 (Step 002225): Train loss 0.496, Val loss 5.653\n",
      "Ep 9 (Step 002230): Train loss 0.333, Val loss 5.672\n",
      "Ep 9 (Step 002235): Train loss 0.488, Val loss 5.698\n",
      "Ep 9 (Step 002240): Train loss 0.527, Val loss 5.713\n",
      "Ep 9 (Step 002245): Train loss 0.468, Val loss 5.666\n",
      "Ep 9 (Step 002250): Train loss 0.402, Val loss 5.705\n",
      "Ep 9 (Step 002255): Train loss 0.517, Val loss 5.748\n",
      "Ep 9 (Step 002260): Train loss 0.558, Val loss 5.752\n",
      "Ep 9 (Step 002265): Train loss 0.421, Val loss 5.726\n",
      "Ep 9 (Step 002270): Train loss 0.444, Val loss 5.725\n",
      "Ep 9 (Step 002275): Train loss 0.476, Val loss 5.745\n",
      "Ep 9 (Step 002280): Train loss 0.444, Val loss 5.752\n",
      "Ep 9 (Step 002285): Train loss 0.440, Val loss 5.792\n",
      "Ep 9 (Step 002290): Train loss 0.344, Val loss 5.778\n",
      "Ep 9 (Step 002295): Train loss 0.415, Val loss 5.759\n",
      "Ep 9 (Step 002300): Train loss 0.377, Val loss 5.787\n",
      "Ep 9 (Step 002305): Train loss 0.440, Val loss 5.744\n",
      "Ep 9 (Step 002310): Train loss 0.463, Val loss 5.703\n",
      "Ep 9 (Step 002315): Train loss 0.277, Val loss 5.787\n",
      "Ep 9 (Step 002320): Train loss 0.461, Val loss 5.778\n",
      "Ep 9 (Step 002325): Train loss 0.302, Val loss 5.781\n",
      "Ep 9 (Step 002330): Train loss 0.290, Val loss 5.791\n",
      "Ep 9 (Step 002335): Train loss 0.331, Val loss 5.793\n",
      "Ep 9 (Step 002340): Train loss 0.586, Val loss 5.849\n",
      "Ep 9 (Step 002345): Train loss 0.410, Val loss 5.887\n",
      "Ep 9 (Step 002350): Train loss 0.465, Val loss 5.838\n",
      "Ep 9 (Step 002355): Train loss 0.395, Val loss 5.847\n",
      "Ep 9 (Step 002360): Train loss 0.333, Val loss 5.833\n",
      "Ep 9 (Step 002365): Train loss 0.515, Val loss 5.830\n",
      "Ep 9 (Step 002370): Train loss 0.400, Val loss 5.825\n",
      "Ep 9 (Step 002375): Train loss 0.366, Val loss 5.829\n",
      "Ep 9 (Step 002380): Train loss 0.229, Val loss 5.756\n",
      "Ep 9 (Step 002385): Train loss 0.478, Val loss 5.768\n",
      "Ep 9 (Step 002390): Train loss 0.424, Val loss 5.781\n",
      "Ep 9 (Step 002395): Train loss 0.443, Val loss 5.796\n",
      "Ep 9 (Step 002400): Train loss 0.405, Val loss 5.823\n",
      "Ep 9 (Step 002405): Train loss 0.274, Val loss 5.800\n",
      "Ep 9 (Step 002410): Train loss 0.280, Val loss 5.807\n",
      "Ep 9 (Step 002415): Train loss 0.297, Val loss 5.740\n",
      "Ep 9 (Step 002420): Train loss 0.330, Val loss 5.763\n",
      "Ep 9 (Step 002425): Train loss 0.392, Val loss 5.780\n",
      "Uma noite destas, vindo da cidade para o marque são os dizer-me. Quando-lhe que sentido que se esta algumas criticas descem com um antigos, ella Shakespeare eram no Genesis, e\n",
      "Ep 10 (Step 002430): Train loss 0.283, Val loss 5.763\n",
      "Ep 10 (Step 002435): Train loss 0.350, Val loss 5.851\n",
      "Ep 10 (Step 002440): Train loss 0.319, Val loss 5.923\n",
      "Ep 10 (Step 002445): Train loss 0.345, Val loss 5.924\n",
      "Ep 10 (Step 002450): Train loss 0.292, Val loss 5.901\n",
      "Ep 10 (Step 002455): Train loss 0.307, Val loss 5.920\n",
      "Ep 10 (Step 002460): Train loss 0.321, Val loss 5.978\n",
      "Ep 10 (Step 002465): Train loss 0.197, Val loss 5.997\n",
      "Ep 10 (Step 002470): Train loss 0.255, Val loss 5.965\n",
      "Ep 10 (Step 002475): Train loss 0.252, Val loss 5.979\n",
      "Ep 10 (Step 002480): Train loss 0.237, Val loss 5.946\n",
      "Ep 10 (Step 002485): Train loss 0.205, Val loss 5.998\n",
      "Ep 10 (Step 002490): Train loss 0.256, Val loss 6.006\n",
      "Ep 10 (Step 002495): Train loss 0.262, Val loss 6.021\n",
      "Ep 10 (Step 002500): Train loss 0.269, Val loss 6.030\n",
      "Ep 10 (Step 002505): Train loss 0.294, Val loss 5.992\n",
      "Ep 10 (Step 002510): Train loss 0.304, Val loss 5.970\n",
      "Ep 10 (Step 002515): Train loss 0.213, Val loss 6.012\n",
      "Ep 10 (Step 002520): Train loss 0.237, Val loss 6.046\n",
      "Ep 10 (Step 002525): Train loss 0.183, Val loss 6.010\n",
      "Ep 10 (Step 002530): Train loss 0.321, Val loss 6.039\n",
      "Ep 10 (Step 002535): Train loss 0.341, Val loss 6.084\n",
      "Ep 10 (Step 002540): Train loss 0.250, Val loss 6.059\n",
      "Ep 10 (Step 002545): Train loss 0.271, Val loss 6.063\n",
      "Ep 10 (Step 002550): Train loss 0.273, Val loss 6.076\n",
      "Ep 10 (Step 002555): Train loss 0.198, Val loss 6.143\n",
      "Ep 10 (Step 002560): Train loss 0.183, Val loss 6.088\n",
      "Ep 10 (Step 002565): Train loss 0.294, Val loss 6.061\n",
      "Ep 10 (Step 002570): Train loss 0.201, Val loss 6.021\n",
      "Ep 10 (Step 002575): Train loss 0.212, Val loss 6.074\n",
      "Ep 10 (Step 002580): Train loss 0.230, Val loss 6.104\n",
      "Ep 10 (Step 002585): Train loss 0.216, Val loss 6.114\n",
      "Ep 10 (Step 002590): Train loss 0.229, Val loss 6.130\n",
      "Ep 10 (Step 002595): Train loss 0.176, Val loss 6.096\n",
      "Ep 10 (Step 002600): Train loss 0.142, Val loss 6.068\n",
      "Ep 10 (Step 002605): Train loss 0.272, Val loss 6.085\n",
      "Ep 10 (Step 002610): Train loss 0.134, Val loss 6.113\n",
      "Ep 10 (Step 002615): Train loss 0.310, Val loss 6.126\n",
      "Ep 10 (Step 002620): Train loss 0.139, Val loss 6.124\n",
      "Ep 10 (Step 002625): Train loss 0.245, Val loss 6.069\n",
      "Ep 10 (Step 002630): Train loss 0.245, Val loss 6.099\n",
      "Ep 10 (Step 002635): Train loss 0.211, Val loss 6.149\n",
      "Ep 10 (Step 002640): Train loss 0.235, Val loss 6.154\n",
      "Ep 10 (Step 002645): Train loss 0.214, Val loss 6.126\n",
      "Ep 10 (Step 002650): Train loss 0.264, Val loss 6.084\n",
      "Ep 10 (Step 002655): Train loss 0.243, Val loss 6.101\n",
      "Ep 10 (Step 002660): Train loss 0.251, Val loss 6.098\n",
      "Ep 10 (Step 002665): Train loss 0.190, Val loss 6.063\n",
      "Ep 10 (Step 002670): Train loss 0.224, Val loss 6.093\n",
      "Ep 10 (Step 002675): Train loss 0.218, Val loss 6.108\n",
      "Ep 10 (Step 002680): Train loss 0.220, Val loss 6.155\n",
      "Ep 10 (Step 002685): Train loss 0.164, Val loss 6.231\n",
      "Ep 10 (Step 002690): Train loss 0.198, Val loss 6.196\n",
      "Ep 10 (Step 002695): Train loss 0.180, Val loss 6.049\n",
      "Uma noite destas, vindo da cidade para o marque é justamente casa escura e o metal para que os que estás estréa, dizer, sem quinito. Levantou-me que se livrou. O gente, que faziam\n",
      "Treino completo em 12.13 minutos.\n"
     ]
    }
   ],
   "source": [
    "# Marca o tempo de início\n",
    "start_time = time.time()\n",
    "\n",
    "# Instancia o modelo e move para o dispositivo (CPU/GPU)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "# Configura o otimizador (AdamW)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=4e-4,\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "\n",
    "# Treina o modelo\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Uma noite destas, vindo da cidade para o\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Calcula e exibe o tempo total de execução\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Treino completo em {execution_time_minutes:.2f} minutos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda855f-30a8-492a-87de-bf1699665c1d",
   "metadata": {},
   "source": [
    "## Plot do Loss de treinamento e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90bda2fc-ce70-4525-a4f0-2d10eb586a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZyxJREFUeJzt3Xdc1PUfwPHXcWxkK0sRBVGUELcpmoucmaPSygyzshJTs6GmuUrNMvOXmmVDK1PLUrPcmnsvzIV7K+Ji77vv74+vHJ7gOATvwPfz8bhH3He+Pyfdm8/3szSKoigIIYQQwiJZmTsAIYQQQtyZJGohhBDCgkmiFkIIISyYJGohhBDCgkmiFkIIISyYJGohhBDCgkmiFkIIISyYJGohhBDCgkmiFkIIISyYJGohhBDCgkmiFkIIIW6zYcMGOnbsiJ+fHxqNhkWLFpl8DUVRmDhxIlWrVsXOzo7y5cszduxYk68jiVqIUuD06dNoNBpiYmLMHYoQpUJqairh4eFMmzat0NcYMGAA33//PRMnTiQ2NpbFixfToEEDk69jXegIhBBFSqPR3HX/yJEjGTVq1MMJRohHXLt27WjXrt0d92dmZjJs2DDmzp1LQkICjz32GBMmTKB58+YAHD58mOnTp3PgwAGqVasGQOXKlQsViyRqISzEpUuXDD//9ttvjBgxgiNHjhi2lSlTxhxhCSEK0K9fPw4dOsS8efPw8/Nj4cKFtG3blv379xMcHMzff/9NYGAg//zzD23btkVRFCIjI/nss8/w8PAw6V7y6FsIC+Hj42N4ubq6otFoDO+9vLyYNGkSFSpUwM7Ojlq1arF8+fI7Xkun09G7d29CQkI4e/YsAH/99Rd16tTB3t6ewMBARo8eTU5OjuEcjUbD999/T5cuXXB0dCQ4OJjFixcb9t+4cYMePXpQrlw5HBwcCA4OZubMmXeM4Y8//iAsLAwHBwc8PT2JjIwkNTXVsP/777+nevXq2NvbExISwtdff210/rlz5+jWrRtubm54eHjQqVMnTp8+bdjfq1cvOnfuzMSJE/H19cXT05Po6Giys7Pv+zMXojDOnj3LzJkzmT9/Pk2bNiUoKIj33nuPJk2aGP6fOHnyJGfOnGH+/Pn8/PPPzJo1i927d/Pss8+afkNFCGFxZs6cqbi6uhreT5o0SXFxcVHmzp2rxMbGKh988IFiY2OjHD16VFEURTl16pQCKHv37lUyMjKULl26KLVr11bi4+MVRVGUDRs2KC4uLsqsWbOUEydOKCtXrlQqVaqkjBo1ynAPQKlQoYIyZ84c5dixY0r//v2VMmXKKNeuXVMURVGio6OVWrVqKTt37lROnTqlrFq1Slm8eHGB8V+8eFGxtrZWJk2apJw6dUr577//lGnTpinJycmKoijK7NmzFV9fX+XPP/9UTp48qfz555+Kh4eHMmvWLEVRFCUrK0upXr260rt3b+W///5TDh06pLz44otKtWrVlMzMTEVRFCUqKkpxcXFR3nzzTeXw4cPK33//rTg6OiozZswo2n8M8cgDlIULFxre//PPPwqgODk5Gb2sra2Vbt26KYqiKK+//roCKEeOHDGct3v3bgVQYmNjTbt/kZRCCFGkbk/Ufn5+ytixY42OqV+/vtK3b19FUfIS9caNG5VWrVopTZo0URISEgzHtmrVShk3bpzR+b/88ovi6+treA8ow4cPN7xPSUlRAGXZsmWKoihKx44dlVdeeeW+4s/9Qjp9+nSB+4OCgpQ5c+YYbfv444+VRo0aGWKrVq2aotfrDfszMzMVBwcHZcWKFYqiqIk6ICBAycnJMRzz3HPPKd27d7+vGIW4X7cn6nnz5ilarVaJjY1Vjh07ZvS6dOmSoiiKMmLECMXa2troOmlpaQqgrFy50qT7Sxu1EBYuKSmJixcvEhERYbQ9IiKCffv2GW174YUXqFChAv/++y8ODg6G7fv27WPz5s1GQ0N0Oh0ZGRmkpaXh6OgIQM2aNQ37nZyccHFxIT4+HoC33nqLZ555hj179tC6dWs6d+5M48aNC4w5PDycVq1aERYWRps2bWjdujXPPvss7u7upKamcuLECV599VVef/11wzk5OTm4uroa4j1+/DjOzs5G183IyODEiROG96GhoWi1WsN7X19f9u/ff5dPU4gHV7t2bXQ6HfHx8TRt2rTAYyIiIsjJyeHEiRMEBQUBcPToUQACAgJMup8kaiFKkfbt2zN79my2bt1Ky5YtDdtTUlIYPXo0Xbt2zXeOvb294WcbGxujfRqNBr1eD6i9YM+cOcPSpUtZtWoVrVq1Ijo6mokTJ+a7plarZdWqVWzZsoWVK1cyZcoUhg0bxvbt2w1/FHz33Xc0bNgw33m58datW5dff/0137XLlSt3X/EK8SBSUlI4fvy44f2pU6eIiYnBw8ODqlWr0qNHD15++WW++OILateuzZUrV1izZg01a9akQ4cOREZGUqdOHXr37s3kyZPR6/VER0fz5JNPUrVqVdOCeeBnAkKIIne/j76jo6MVRTFuo/7qq68UJycnZd26dYZjGzdurPTu3fuu9+S2x3uKoiiurq7KzJkzCzz+m2++UZydne+rPDk5OUr58uWVL774wlCeMWPG3PH4GTNmKO7u7kpiYuIdj4mKilI6depktG3AgAFKs2bN7ismIe5m7dq1CpDvFRUVpSiK2o9ixIgRSqVKlRQbGxvF19dX6dKli/Lff/8ZrnHhwgWla9euSpkyZRRvb2+lV69ehj4fppAatRAlwPvvv8/IkSMJCgqiVq1azJw5k5iYmAJrnG+//TY6nY6nnnqKZcuW0aRJE0aMGMFTTz1FxYoVefbZZ7GysmLfvn0cOHCATz755L5iGDFiBHXr1iU0NJTMzEz++ecfqlevXuCx27dvZ82aNbRu3RovLy+2b9/OlStXDMePHj2a/v374+rqStu2bcnMzGTXrl3cuHGDQYMG0aNHDz7//HM6derEmDFjqFChAmfOnGHBggV88MEHVKhQofAfphD3oXnz5iiKcsf9NjY2jB49mtGjR9/xGD8/P/78888HjkUStRAlQP/+/UlMTOTdd98lPj6eGjVqsHjxYoKDgws8fuDAgej1etq3b8/y5ctp06YN//zzD2PGjGHChAnY2NgQEhLCa6+9dt8x2NraMnToUE6fPo2DgwNNmzZl3rx5BR7r4uLChg0bmDx5MklJSQQEBPDFF18YJpB47bXXcHR05PPPP+f999/HycmJsLAwBg4cCICjoyMbNmxg8ODBdO3aleTkZMqXL0+rVq1wcXEx7cMTooTTKHf7k0EIIYQQZiUTngghhBAWTBK1EEIIYcEkUQshhBAWTBK1EEIIYcEkUQshhBAWTBL1baZNm0alSpWwt7enYcOG7Nix467Hz58/n5CQEOzt7QkLC2Pp0qUPKdLCMaV83333HU2bNsXd3R13d3ciIyPv+XmYk6n/drnmzZuHRqOhc+fOxRvgAzK1fAkJCURHR+Pr64udnR1Vq1a16N9PU8s3efJkqlWrhoODA/7+/rzzzjtkZGQ8pGjv34YNG+jYsSN+fn5oNBoWLVp0z3PWrVtHnTp1sLOzo0qVKsyaNavY4ywsU8u3YMECnnzyScqVK4eLiwuNGjVixYoVDyfYQijMv1+uzZs3Y21tTa1atR4siCKZwqWUmDdvnmJra6v8+OOPysGDB5XXX39dcXNzUy5fvlzg8Zs3b1a0Wq3y2WefKYcOHVKGDx+u2NjYKPv373/Ikd8fU8v34osvKtOmTVP27t2rHD58WOnVq5fi6uqqnD9//iFHfm+mli3XqVOnlPLlyytNmzbNN8uVJTG1fJmZmUq9evWU9u3bK5s2bVJOnTqlrFu3TomJiXnIkd8fU8v366+/KnZ2dsqvv/6qnDp1SlmxYoXi6+urvPPOOw858ntbunSpMmzYMGXBggUFzv52u5MnTyqOjo7KoEGDlEOHDilTpkxRtFqtsnz58ocTsIlMLd+AAQOUCRMmKDt27FCOHj2qDB06VLGxsVH27NnzcAI2kanly3Xjxg0lMDBQad26tRIeHv5AMUiivkWDBg0MUzIqiqLodDrFz89PGT9+fIHHd+vWTenQoYPRtoYNGypvvPFGscZZWKaW73Y5OTmKs7Oz8tNPPxVXiIVWmLLl5OQojRs3Vr7//vsCp6O0JKaWb/r06UpgYKCSlZX1sEJ8IKaWLzo6WmnZsqXRtkGDBikRERHFGueDup8v+g8++EAJDQ012ta9e3elTZs2xRhZ0TAlkd2qRo0ayujRo4s+oCJmSvm6d++uDB8+XBk5cuQDJ2p59H1TVlYWu3fvJjIy0rDNysqKyMhItm7dWuA5W7duNToeoE2bNnc83pwKU77bpaWlkZ2djYeHR3GFWSiFLduYMWPw8vLi1VdffRhhFlphyrd48WIaNWpEdHQ03t7ePPbYY4wbNw6dTvewwr5vhSlf48aN2b17t+Hx+MmTJ1m6dCnt27d/KDEXp5L0vVIU9Ho9ycnJFve98iBmzpzJyZMnGTlyZJFcT6YQvenq1avodDq8vb2Ntnt7exMbG1vgOXFxcQUeHxcXV2xxFlZhyne7wYMH4+fnl+9LxNwKU7ZNmzbxww8/EBMT8xAifDCFKd/Jkyf5999/6dGjB0uXLuX48eP07duX7OzsIvvyKCqFKd+LL77I1atXadKkCYqikJOTw5tvvsmHH374MEIuVnf6XklKSiI9Pd1o+dLSYOLEiaSkpNCtWzdzh1Ikjh07xpAhQ9i4cSPW1kWTYqVGLe7Lp59+yrx581i4cKHRsoglUXJyMj179uS7776jbNmy5g6nWOj1ery8vJgxYwZ169ale/fuDBs2jG+++cbcoRWJdevWMW7cOL7++mv27NnDggULWLJkCR9//LG5QxMmmDNnDqNHj+b333/Hy8vL3OE8MJ1Ox4svvsjo0aNNX8ryLqRGfVPZsmXRarVcvnzZaPvly5fx8fEp8BwfHx+TjjenwpQv18SJE/n0009ZvXo1NWvWLM4wC8XUsp04cYLTp0/TsWNHw7bcNYytra05cuSIYaF3S1CYfztfX19sbGwM6zsDVK9enbi4OLKysrC1tS3WmE1RmPJ99NFH9OzZ07CoSFhYGKmpqfTp04dhw4ZhZVVy6yB3+l5xcXEpVbXpefPm8dprrzF//nyLe0pXWMnJyezatYu9e/fSr18/QP1uURQFa2trVq5cabRO/P0qub/NRczW1pa6deuyZs0awza9Xs+aNWto1KhRgec0atTI6HiAVatW3fF4cypM+QA+++wzPv74Y5YvX069evUeRqgmM7VsISEh7N+/n5iYGMPr6aefpkWLFsTExODv7/8ww7+nwvzbRUREcPz4ccMfIABHjx7F19fXopI0FK58aWlp+ZJx7h8lSglfZ6gkfa8U1ty5c3nllVeYO3cuHTp0MHc4RcbFxSXfd8ubb75JtWrViImJoWHDhoW78AN1RStl5s2bp9jZ2SmzZs1SDh06pPTp00dxc3NT4uLiFEVRlJ49eypDhgwxHL9582bF2tpamThxonL48GFl5MiRFj88y5Tyffrpp4qtra3yxx9/KJcuXTK8kpOTzVWEOzK1bLez9F7fppbv7NmzirOzs9KvXz/lyJEjyj///KN4eXkpn3zyibmKcFemlm/kyJGKs7OzMnfuXOXkyZPKypUrlaCgIKVbt27mKsIdJScnK3v37lX27t2rAMqkSZOUvXv3KmfOnFEURVGGDBmi9OzZ03B87vCs999/Xzl8+LAybdo0ix6eZWr5fv31V8Xa2lqZNm2a0fdKQkKCuYpwV6aW73ZF0etbEvVtpkyZolSsWFGxtbVVGjRooGzbts2wr1mzZkpUVJTR8b///rtStWpVxdbWVgkNDVWWLFnykCM2jSnlCwgIUIB8r5EjRz78wO+Dqf92t7L0RK0oppdvy5YtSsOGDRU7OzslMDBQGTt2rJKTk/OQo75/ppQvOztbGTVqlBIUFKTY29sr/v7+St++fZUbN248/MDvYe3atQX+f5RbnqioKKVZs2b5zqlVq5Zia2urBAYGKjNnznzocd8vU8vXrFmzux5vaQrz73erokjUsh61EEIIYcGkjVoIIYSwYJKohRBCCAsmiVoIIYSwYJKohRBCCAsmiVoIIYSwYJKohRBCCAsmiVoIIYSwYJKoTZCZmcmoUaPIzMw0dyjFojSXrzSXDaR8JV1pLl9pLhs8nPLJhCcmSEpKwtXVlcTERFxcXMwdTpErzeUrzWUDKV9JV5rLV5rLBg+nfFKjFkIIISyYJGohhBDCgpX69ahzcnLYu3cv3t7eD7xGbXJyMgAXLlwgKSmpKMKzKKW5fKW5bCDlK+lKc/lKc9mg8OXT6/VcvnyZ2rVrY21991Rc6tuod+7cSYMGDcwdhhBCCJHPjh07qF+//l2PKfU1am9vb0D9MHx9fc0cjRBCCAGXLl2iQYMGhhx1N6U+Uec+7vb19aVChQpmjkYIIYTIcz9NstKZTAghhLBgZk3UGzZsoGPHjvj5+aHRaFi0aJHRfkVRGDFiBL6+vjg4OBAZGcmxY8fME6wQQghhBmZN1KmpqYSHhzNt2rQC93/22Wd89dVXfPPNN2zfvh0nJyfatGlDRkbGQ45UCCGEMA+ztlG3a9eOdu3aFbhPURQmT57M8OHD6dSpEwA///wz3t7eLFq0iOeff/5hhiqEeETodDqys7PNHYYo4WxsbNBqtUVyLYvtTHbq1Cni4uKIjIw0bHN1daVhw4Zs3bpVErUQokgpikJcXBwJCQnmDkWUEm5ubvj4+KDRaB7oOhabqOPi4gDydV339vY27CtIZmam0eTouYPRi8LFWb2wTruCQ5cvcfatWmTXFUKYX26S9vLywtHR8YG/XMWjS1EU0tLSiI+PB3jgocEWm6gLa/z48YwePbpYrq05vREvrnL6SrwkaiFKEZ1OZ0jSnp6e5g5HlAIODg4AxMfH4+Xl9UCPwS12eJaPjw8Aly9fNtp++fJlw76CDB06lMTERMPr0KFDRRZTDjYA6HJK53JtQjyqctukHR0dzRyJKE1yf58etM+DxSbqypUr4+Pjw5o1awzbkpKS2L59O40aNbrjeXZ2dri4uBhezs7ORRZTjkZ9AKHLlkQtRGkkj7tFUSqq3yezJuqUlBRiYmKIiYkB1A5kMTExnD17Fo1Gw8CBA/nkk09YvHgx+/fv5+WXX8bPz4/OnTubJV5Dos6SRC2EKL0qVarE5MmT7/v4devWodFoir0j3qxZs3BzcyvWe1gis7ZR79q1ixYtWhjeDxo0CICoqChmzZrFBx98QGpqKn369CEhIYEmTZqwfPly7O3tzRKvTqM++tbLo28hhAW4V41t5MiRjBo1yuTr7ty5Eycnp/s+vnHjxly6dAlXV1eT7yXuzayJunnz5txt8S6NRsOYMWMYM2bMQ4zqzvISdZaZIxFCCHVhh1y//fYbI0aM4MiRI4ZtZcqUMfysKAo6ne6eSyoClCtXzqQ4bG1t79p3SDwYi22jtkQ6K/UXXJ8tiVoIYX4+Pj6Gl6urKxqNxvA+NjYWZ2dnli1bRt26dbGzs2PTpk2cOHGCTp064e3tTZkyZahfvz6rV682uu7tj741Gg3ff/89Xbp0wdHRkeDgYBYvXmzYf/uj79xH1CtWrKB69eqUKVOGtm3bGv1hkZOTQ//+/XFzc8PT05PBgwcTFRVlctPm9OnTCQoKwtbWlmrVqvHLL78Y9imKwqhRo6hYsSJ2dnb4+fnRv39/w/6vv/6a4OBg7O3t8fb25tlnnzXp3g+LJGoT6KVGLYQoYYYMGcKnn37K4cOHqVmzJikpKbRv3541a9awd+9e2rZtS8eOHTl79uxdrzN69Gi6devGf//9R/v27enRowfXr1+/4/FpaWlMnDiRX375hQ0bNnD27Fnee+89w/4JEybw66+/MnPmTDZv3kxSUlK+9R7uZeHChQwYMIB3332XAwcO8MYbb/DKK6+wdu1aAP7880++/PJLvv32W44dO8aiRYsICwsD1KbX/v37M2bMGI4cOcLy5ct54oknTLr/w1LqxlEXJ2mjFuLRoSgK6dk6s9zbwUZbZD2Gx4wZw5NPPml47+HhQXh4uOH9xx9/zMKFC1m8eDH9+vW743V69erFCy+8AMC4ceP46quv2LFjB23bti3w+OzsbL755huCgoIA6Nevn1Ez5pQpUxg6dChdunQBYOrUqSxdutSksk2cOJFevXrRt29fQO3ntG3bNiZOnEiLFi04e/YsPj4+REZGYmNjQ8WKFWnQoAEAZ8+excnJiaeeegpnZ2cCAgKoXbu2Sfd/WCRRm0CxUhM1kqiFKPXSs3XUGLHCLPc+NKYNjrZF8/Vcr149o/cpKSmMGjWKJUuWcOnSJXJyckhPT79njbpmzZqGn52cnHBxcTHMvFUQR0dHQ5IGdXau3OMTExO5fPmyIWkCaLVa6tati16vv++yHT58mD59+hhti4iI4H//+x8Azz33HJMnTyYwMJC2bdvSvn17OnbsiLW1NU8++SQBAQGGfW3btjU82rc08ujbBPqbiVrRyaNvIUTJcHvv7ffee4+FCxcybtw4Nm7cSExMDGFhYWRl3f17zcbGxui9RqO5a1It6Pi7dR4uDv7+/hw5coSvv/4aBwcH+vbtyxNPPEF2djbOzs7s2bOHuXPn4uvry4gRIwgPD7fIud6lRm0CvZUtAIpOVtYRorRzsNFyaEwbs927uGzevJlevXoZHjmnpKRw+vTpYrtfQVxdXfH29mbnzp2GdmGdTseePXuoVavWfV+nevXqbN68maioKMO2zZs3U6NGDcN7BwcHOnbsSMeOHYmOjiYkJIT9+/dTp04drK2tiYyMJDIykpEjR+Lm5sa///5L165di6ysRUEStQnOONXk7PU0yjgE3ftgIUSJptFoiuzxsyUJDg5mwYIFdOzYEY1Gw0cffWTS4+ai8vbbbzN+/HiqVKlCSEgIU6ZM4caNGya1zb///vt069aN2rVrExkZyd9//82CBQsMvdhnzZqFTqejYcOGODo6Mnv2bBwcHAgICOCff/7h5MmTPPHEE7i7u7N06VL0ej3VqlUrriIXWun7LSxGMeU6MvdUOIPcZEEOIUTJNGnSJHr37k3jxo0pW7YsgwcPJikp6aHHMXjwYOLi4nj55ZfRarX06dOHNm3amLR4RefOnfnf//7HxIkTGTBgAJUrV2bmzJk0b94cUJeZ/PTTTxk0aBA6nY6wsDD+/vtvPD09cXNzY8GCBYwaNYqMjAyCg4OZO3cuoaGhxVTiwtMoD7vR4CE7f/48/v7+nDt3jgoVKjzQtUb+dYCftp7h7ZZVeLe15f3VJYQonIyMDE6dOkXlypXNNvPho06v11O9enW6devGxx9/bO5wisTdfq9MyU1SozaBnUaPE+koWanmDkUIIUq0M2fOsHLlSpo1a0ZmZiZTp07l1KlTvPjii+YOzeJIr28TPBH/MwftX6XZmSnmDkUIIUo0KysrZs2aRf369YmIiGD//v2sXr2a6tWrmzs0iyM1ahMoWnW4gUaGZwkhxAPx9/dn8+bN5g6jRJAatQkOVOxJtYxZ/OH73r0PFkIIIYqAJGoTaG3syMSWLL0sLi+EEOLhkERtAhut+nFl6R7+mEMhhBCPJmmjNoFf0j6+sPkOzdUQoI65wxFCCPEIkERtAtfMS7TRbuJgWpq5QxFCCPGIkEffJrCyVuf6ttLLXN9CCCEeDknUJtBYqzPLaBVJ1EKI0qN58+YMHDjQ8L5SpUpMnjz5rudoNBoWLVr0wPcuquvczahRo0xa7MPSSKI2gdXNZdu0So6ZIxFCCOjYsSNt27YtcN/GjRvRaDT8999/Jl93586d+dZ5flB3SpaXLl2iXbt2RXqv0kYStQm01nbqf6VGLYSwAK+++iqrVq3i/Pnz+fbNnDmTevXqUbNmTZOvW65cORwdHYsixHvy8fHBzs7uodyrpJJEbQKtjfrLZC2JWghhAZ566inKlSvHrFmzjLanpKQwf/58Xn31Va5du8YLL7xA+fLlcXR0JCwsjLlz5971urc/+j527BhPPPEE9vb21KhRg1WrVuU7Z/DgwVStWhVHR0cCAwP56KOPyM5WvytnzZrF6NGj2bdvHxqNBo1GY4j59kff+/fvp2XLljg4OODp6UmfPn1ISUkx7O/VqxedO3dm4sSJ+Pr64unpSXR0tOFe90Ov1zNmzBgqVKiAnZ0dtWrVYvny5Yb9WVlZ9OvXD19fX+zt7QkICGD8+PEAKIrCqFGjqFixInZ2dvj5+dG/f//7vndhSK9vE2ht1M5k1vLoW4hHR2EW4dHagfbm16suB3SZoLECG4d7X9fW6b5vY21tzcsvv8ysWbMYNmyYYS3n+fPno9PpeOGFF0hJSaFu3boMHjwYFxcXlixZQs+ePQkKCqJBgwb3vIder6dr1654e3uzfft2EhMTjdqzczk7OzNr1iz8/PzYv38/r7/+Os7OznzwwQd0796dAwcOsHz5csNa0a6urvmukZqaSps2bWjUqBE7d+4kPj6e1157jX79+hn9MbJ27Vp8fX1Zu3Ytx48fp3v37tSqVYvXX3/9vj63//3vf3zxxRd8++231K5dmx9//JGnn36agwcPEhwczFdffcXixYv5/fffqVixIufOnePcuXMA/Pnnn3z55ZfMmzeP0NBQ4uLi2Ldv333dt7AsOlHrdDpGjRrF7NmziYuLw8/Pj169ejF8+HCTFhcvKlY2amcyG0Xm+hbikTHOz/RznpsFoV3Un2P/hvm9IKAJvLIk75jJYZB2Lf+5oxJNulXv3r35/PPPWb9+vWEd5pkzZ/LMM8/g6uqKq6sr772XN+3x22+/zYoVK/j999/vK1GvXr2a2NhYVqxYgZ+f+lmMGzcuX7vy8OHDDT9XqlSJ9957j3nz5vHBBx/g4OBAmTJlsLa2xsfH5473mjNnDhkZGfz88884Oal/sEydOpWOHTsyYcIEvL29AXB3d2fq1KlotVpCQkLo0KEDa9asue9EPXHiRAYPHszzzz8PwIQJE1i7di2TJ09m2rRpnD17luDgYJo0aYJGoyEgIMBw7tmzZ/Hx8SEyMhIbGxsqVqx4X5/jg7DoR98TJkxg+vTpTJ06lcOHDzNhwgQ+++wzpkwxz+pV1rbqX8M2yKNvIYRlCAkJoXHjxvz4448AHD9+nI0bN/Lqq68CaoXn448/JiwsDA8PD8qUKcOKFSs4e/bsfV3/8OHD+Pv7G5I0QKNGjfId99tvvxEREYGPjw9lypRh+PDh932PW+8VHh5uSNIAERER6PV6jhw5YtgWGhqKVqs1vPf19SU+Pv6+7pGUlMTFixeJiIgw2h4REcHhw4cB9fF6TEwM1apVo3///qxcudJw3HPPPUd6ejqBgYG8/vrrLFy4kJyc4n3KatE16i1bttCpUyc6dOgAqH+lzZ07lx07dpglHht7tXOFLVKjFuKR8eFF08/R3tI5KqSjeg3NbfWigfsfLK5bvPrqq7z99ttMmzaNmTNnEhQURLNmzQD4/PPP+d///sfkyZMJCwvDycmJgQMHkpVVdN9jW7dupUePHowePZo2bdrg6urKvHnz+OKLL4rsHreyuTkCJ5dGo0GvL7qpnevUqcOpU6dYtmwZq1evplu3bkRGRvLHH3/g7+/PkSNHWL16NatWraJv376GJxq3x1VULLpG3bhxY9asWcPRo0cB2LdvH5s2bTJbV35be7VGbU82KIpZYhBCPGS2Tqa/tLfUgbTW6rZb26fvdt1C6NatG1ZWVsyZM4eff/6Z3r17G5oHN2/eTKdOnXjppZcIDw8nMDDQ8J16P6pXr865c+e4dOmSYdu2bduMjtmyZQsBAQEMGzaMevXqERwczJkzZ4yLa2uLTqe757327dtHampe+/3mzZuxsrKiWrVq9x3z3bi4uODn55dvic3NmzdTo0YNo+O6d+/Od999x2+//caff/7J9evXAXBwcKBjx4589dVXrFu3jq1bt7J/f9H94XU7i65RDxkyhKSkJEJCQtBqteh0OsaOHUuPHj3ueE5mZiaZmZmG98nJyUUWj51d3nAFfXYGVrYOdzlaCCEejjJlytC9e3eGDh1KUlISvXr1MuwLDg7mjz/+YMuWLbi7uzNp0iQuX75slJTuJjIykqpVqxIVFcXnn39OUlISw4YNMzomODiYs2fPMm/ePOrXr8+SJUtYuHCh0TGVKlXi1KlTxMTEUKFCBZydnfMNy+rRowcjR44kKiqKUaNGceXKFd5++2169uxpaJ8uCu+//z4jR44kKCiIWrVqMXPmTGJiYvj1118BmDRpEr6+vtSuXRsrKyvmz5+Pj48Pbm5uzJo1C51OR8OGDXF0dGT27Nk4ODgYtWMXNYuuUf/+++/8+uuvzJkzhz179vDTTz8xceJEfvrppzueM378eEMHCldX1/v+Zbwftg5OrNWFs1xXn6xibpMQQghTvPrqq9y4cYM2bdoYtScPHz6cOnXq0KZNG5o3b46Pjw+dO3e+7+taWVmxcOFC0tPTadCgAa+99hpjx441Oubpp5/mnXfeoV+/ftSqVYstW7bw0UcfGR3zzDPP0LZtW1q0aEG5cuUKHCLm6OjIihUruH79OvXr1+fZZ5+lVatWTJ061bQP4x769+/PoEGDePfddwkLC2P58uUsXryY4OBgQO3B/tlnn1GvXj3q16/P6dOnWbp0KVZWVri5ufHdd98RERFBzZo1Wb16NX///Teenp5FGuOtNIpiuc9w/f39GTJkCNHR0YZtn3zyCbNnzyY2NrbAc26vUV+4cIEaNWpw7tw5KlSo8EDx5Oj0VBm2DICYEU/i5mj7QNcTQliGjIwMTp06ReXKlbG3tzd3OKKUuNvv1fnz5/H397+v3GTRj77T0tKwsjKu9Gu12rt2GrCzszN6nJKUlFRk8VhrrdBaadDpFTJzZE1qIYQQxc+iE3XHjh0ZO3YsFStWJDQ0lL179zJp0iR69+5ttpjsra1IzcohI0sefQshhCh+Fp2op0yZwkcffUTfvn2Jj4/Hz8+PN954gxEjRpgtpiVW7+Bvd4nzcUugbMS9TxBCCCEegEUnamdnZyZPnnzP5dYeJq1GQYtCdkaauUMRQgjxCLDoRG2JBjl8wunrGUz3MH1FGiGEEMJUkqhNlGzrxRWSSddZ9Mg2IUQhWPAgGFECFdXvk2QbE9nZqPPLZmZLr28hSovcqR/T0qRJSxSd3N+nB51aVGrUJmqXtZLO1oexj9dAjfbmDkcIUQS0Wi1ubm6GhR0cHR3NskKfKB0URSEtLY34+Hjc3NyMFhApDEnUJmqQtZ061lvZfb0RIIlaiNIid/nF+12FSYh7cXNzu+uynvdLErWJdFbqZCr67AwzRyKEKEoajQZfX1+8vLzIzpalbMWDsbGxeeCadC5J1CbS5y5fl51u3kCEEMVCq9UW2ResEEVBOpOZSKdV52tVcqRGLYQQovhJojaRYp1bo5ZELYQQovhJojaRcrNGjS7z7gcKIYQQRUAStalu1qitcqSNWgghRPGTRG0ina0rALZZiWaORAghxKNAErWJNM7eANhnXTNzJEIIIR4FkqhNZOemJmqn7OtmjkQIIcSjQBK1iRzc/QBw1d0wcyRCCCEeBZKoTeTs6QtAGdJkiJYQQohiJzOTmcjDw4sYfRA3lDI0zkjBzsbe3CEJIYQoxSRRm8jF0YZncz4hR6+wVe+Er7kDEkIIUarJo28TaTQaPJxsAbiWkmXmaIQQQpR2kqgLwbOMHdbkkHN8rblDEUIIUcpJoi6Eap5a1tu9Q621UZB43tzhCCGEKMUkURdCo2oVuKh4clVx4cyx/eYORwghRCkmnckK4Ymq5Xg6awDXcKXBHjfm1TN3REIIIYqNXgdJF8Ctollub/E16gsXLvDSSy/h6emJg4MDYWFh7Nq1y6wx+bo68Eyzeuix4vClZBRFMWs8QgghitGCPjA5DJYPNcvtLTpR37hxg4iICGxsbFi2bBmHDh3iiy++wN3d3dyhMTAyGCsNpKWnc/3CMXOHI4QQ4laZybAoGvbOVt8rCsTth+0z4Pwu9f3pzbB1GpzbkXdechzcOJP3/tgqOPCH+vO2ryE+9uGV4SaLfvQ9YcIE/P39mTlzpmFb5cqVzRhRHnsbLU+6nmdE+gTs/ywH/beBRmPusIQQ4tGlKHnfw2e3Q8xs9ZV+A06uh+Or8o51cFe353ppAQS1hMX94dgKGHEDrKzg+knjexxbCV4hxV+WW1h0jXrx4sXUq1eP5557Di8vL2rXrs13331n7rAM7LyCcSMFpxuxcGGPucMRQojS6/pJtbYLas14/WfwU0dYMUxN0IoC0xrCnp/VY4IjIaCJ+vPK4WqStrKB8nXVbek3QGsLzn4Q2hX8G0BWCiTcrE2f26b+t+EbahJ/8mOwdYbstIdX5pssukZ98uRJpk+fzqBBg/jwww/ZuXMn/fv3x9bWlqioqALPyczMJDMz0/A+OTm52OKrU60yG07VpJ12J8rx1Wgq1C22ewkhxCNHlwNWWrVtePv0go+pEqnWomPmwNUjsPQDCGoFruXh5b9g4xewbhxY28MrS9VEffgfuLgHwl8E90rqPXJr4q+uggu7wa/OLfdoBRUbQcM3wdq22It9O41iwT2hbG1tqVevHlu2bDFs69+/Pzt37mTr1q0FnjNq1ChGjx6db/u5c+eoUKFCkcaXmJ7NF+OHMMbqe9K86+H41poivb4QQpR4igKnN4FHZXA14Tt44yT492NQ9AXv96sDNTpB4/7qI+prJ9RkHfYseFU3PvbcTrB3gXLVCl+OInb+/Hn8/f3vKzdZdI3a19eXGjVqGG2rXr06f/755x3PGTp0KIMGDTK8v3DhQr5rFBVXBxuu+zZDH/cDjpd3wdXjULZKsdxLCCFKpJXDYetU9edq7dUarY0jeIdCYLO84/R6tVar0ahJd9144yTdYjj4hsOZTWBbBpq+q9aEc3kGQauPCo7Bv37Rl+shsuhEHRERwZEjR4y2HT16lICAgDueY2dnh52dneF9UlJSscUHUL5SMGsv1qKVdi/s+hHajivW+wkhhMVQFNj+DZxYC11ngIOb2pa8dhzYOasduuIP5h1/ZKn6AkADvZdDxcfVJD2tPuRkwev/qr2rdVlQoQE0eQecyqptyABVWz/sUppdoRL1uXPn0Gg0hur6jh07mDNnDjVq1KBPnz5FFtw777xD48aNGTduHN26dWPHjh3MmDGDGTNmFNk9HlTN8m78oouklXYvWbt/wbblcLB1NHdYQghRdLIzIPYfsHGA1KtQ92YfocwkWD4EvB8De1d125L34MRtzYCP9wVnH8hIhOTLavLOycqbQCT2b7h2XH2cnZUMkaPBI1BtFy5fh0ddoRL1iy++SJ8+fejZsydxcXE8+eSThIaG8uuvvxIXF8eIESOKJLj69euzcOFChg4dypgxY6hcuTKTJ0+mR48eRXL9otA4yJO39eGc1ZejYvYVOPw3hHc3d1hCCPHg9Hq1E9fm/0HK5bzt1nYQ/ryanMtVV4c1aTRq56/cXtFh3aByU/AJA7/axtdNvQq/vwxWN1NQSEd4YwN41QCtjbqtUXTxl6+EKFRnMnd3d7Zt20a1atX46quv+O2339i8eTMrV67kzTff5OTJk/e+yENiSoN9Ye05e4ONM95hgPVClGrt0Lwwr1juI4QQxSr+MOz8Qa0R+9QER0/Y9YPxMeVC4OXF4Oytvtfr1c5coNaY/4qGio2hUd+HG3sJU+ydybKzsw3twKtXr+bpp58GICQkhEuXLhXmkiXaY36ufKQ0ZgAL4dga9a9Fp7LmDksIIe7f8TUwu2ve+1sn+mg5HBr0gRP/QmBzdbKQXFa3TMdh7wrdZxd7qI+aQk14EhoayjfffMPGjRtZtWoVbdu2BeDixYt4enoWaYAlga21Fda+NYjRB6LRZ8Gumfc+SQghzCknK+9nRcmbKKTyE2oHLu8wsHaAer2h6XtqEg7tYpykxUNRqBr1hAkT6NKlC59//jlRUVGEh4cD6kxiDRo0KNIAS4pwfzd+vNiOr2ynwc7vIGKAWQbGCyEeMbnTZuqyISdDHbPsVQPcA9RtR5aC1k5d/alCPXWIE8A4P9Bnw1tb1I5bGg3UfB6e/kptg44cZdZiiTyFStTNmzfn6tWrJCUlGS2Q0adPHxwdH80ezzUruDFka0Ne1P+Lzq8jEXcapC+EEPeiKBC7BFKvQJ0o9fFy0kVY87GagJsPUY/T5cBYb7XdOOEcZCaq2zVacCoHKXHG120zLi9Rd/kG/nxVnSSkzVh4btZDK54wTaESdXp6OoqiGJL0mTNnWLhwIdWrV6dNmzZFGmBJEV7BlRyseT7rI/gP5jdOo34le3OHJYSwZIqirnWccEadh+HQX+Dqr9aML95cP+DKEWg7HtaOhX1z1NpybqLWWqvzV18+cNt1dfmTtIMHeATlva9QHzp/A489U3zlE0WiUIm6U6dOdO3alTfffJOEhAQaNmyIjY0NV69eZdKkSbz11ltFHafFCypXhrDyruy/oP5F+9w3W9nROQWvuk+rYw+FECLXmS3wexSkxuffl3jO+P2xlWqNt+MUdV5rXY7x/n471EUqHMuqczhobdVhT2c2q52/6r2qDpnyqQkuvnnnuQeoL2HxCpWo9+zZw5dffgnAH3/8gbe3N3v37uXPP/9kxIgRj2SitrLS8Fd0BIEfqrPu1NPE4rV8DFzdBk99aebohBBml5UK2enqiJDs9PxJumxVtWe1lVbt6FX9KTi+Wh3DrLFS25BDu+S/rlvFvIlDbuUZBHVeLp6yiIeqUIk6LS0NZ2dnAFauXEnXrl2xsrLi8ccf58yZM/c4u/Sysspbj9pTc3Pq0uTLxuMMhRCPnvQb8E1TdbrMZ75XV2NqPlSdIKlxf8hIgJrd1Sk4b1W3l/oSj7RCZY8qVaqwaNEizp07x4oVK2jdWp17NT4+HhcXlyINsKRpE6pOArBC34BR/jNRnv9VkrQQpc3lg7Dze7h+ynh7Vqra5ny7C7vVR9oZt6w90HwIvLVZncmw4Rv5k7QQNxWqRj1ixAhefPFF3nnnHVq2bEmjRo0AtXZdu3bte5xdun32TDiZOXtZd+QKs47Z4bfxJH2eCFLbldJvQJly5g5RiEebLhtunFb7jpzfBVu+yutkVb4OPPas+v+pXgezOqj/feY7dd1igFUj1XMUvfpIOqgl2LmoPa+3TlN7XntWUXtXR45SH0tXiYQn3ofAFmYqtCjJCpWon332WZo0acKlS5cMY6gBWrVqRZcuBbShPEJcHW3o16IK645cAeC3nefoFWpD1m+v4JQZhyZiIIQ9J389C/EwKYo6Y+Cen9Qkm5FovP/CbvW/+38HtwAIaa+2FdfoDMsHw5WjeYn65Lq85RcVvdqOfLtrx9VXRhK89Ie6reXwYiiYeBQUeplLHx8ffHx8OH/+PAAVKlR4ZCc7uV2diu60qFaOtUeucCEhnQ8XHeTtuLOUsYqHpe/BpsnqknCVIswdqhClW04m7J0Nu2dB3H95222cQJepJvCGb6jrH189ArbOeQkZ1IUnMhLh8v685RVbf6J2CHPxU6+blQbXjsHBRRDSQW2H1mhh90yIPwQ3zkjvavFACrUoh16v55NPPuGLL74gJSUFAGdnZ959912GDRuGlQW1yT6MRTkKotcr1P54FYnp2QC4kMpI3208o6yChLOABoJaqAup139N7dEphHhwiRfAtbz689XjML2RurYxgLMfNO4HDd9UxyoD2DoVzX112XkrPwlxD8W+KMewYcP44Ycf+PTTT4mIUGuFmzZtYtSoUWRkZDB27NjCXLZUsbLS0KRKWZbsVxcpScKJ3+yfo32PkTis+VD9K//Ev+rr5Dp1rdbI0WBXxryBC1FSZSTCH73VMcp91kG5alC2Cjw5Rk3KtXsaL5ZTVAk6lyRpUUwKlah/+uknvv/+e8OqWQA1a9akfPny9O3bVxL1Te+1qcbqw5fJzFHbs3acuk79z7cx5YWRtAhura7HCuqC7KD2GA1uDY4e6go1QpRkOVnq77aDm7pc4tltavtvNXURHxLOqY+OfcIgtLO6LfUa/NJZferkVV19vJyRpE7g4VRW7ZBZtqr6eDrtmvo4u9PX6kQfdi7qcdnp6h+/5aqp13z80ZvXQZQuhUrU169fJyQkJN/2kJAQrl+//sBBlRaVyzqxpH9Tzt1I45WZOwFIycyh/9y9zOr9BHU/ugabvlR7oMbMhn1z4cCf8Mwt67+e2ar2IJXe4sLSZaWpQ5YCGsPVY7BmNCTftuztrYk6/TpsnKgm8UpNwckTctLz2pLPbr2/+1ZooK59rNFAx68gO1VdZEKIUqJQiTo8PJypU6fy1VdfGW2fOnUqNWvWLJLASosqXmUI8DReqCQ5M4cXZmyjYaAH11Mf54M2UdgmONPQKhYrr+pQ9eYXWUYSzH0eMpOg1xL1C1AIS3Nxr/oH56G/7n6cnYvakzozRW3i8Q2Hx6PVlZqyUtREnZ4AjfqBd6ha4wZ1/LGNg5r0vULVdZLTrqpTZQY0Nk7Kzt7FVkwhzKVQifqzzz6jQ4cOrF692jCGeuvWrZw7d46lS5cWaYClgY3Wig41fTl0MYkpL9TmqSmbyNLp2XjsKgBRM3cCTRnb5S16NAxg79kbeLvo8Ms8rz7iU/Tg3zDvgkveA9+a6nSCds5mKZN4RGSlqQnS8+Y445xM2PmDujJTzefUbVeO5k/S1g7QfDA83lddXtGlvJqQb9d2nPF7n8fAR5rOhLhVoRJ1s2bNOHr0KNOmTSM2NhaArl270qdPHz755BOaNm1apEGWBtNerGP4uU2oNysOXs53zI5T16nt706Xr7fgZKvl4Ji28MZ6tQZipVUPSk9Q17sGWDZYHQ7iVE5N5tZ24P0Y+NVRaxlp19QVdDKS1BqKjN0uXa4cVWuzwU+Cg7vaLnv1mDqKIHfkxYI+avNJr7/VP/r0OrV5JW6/uirTjdPquumVnlDbeR091dm20q6qqzgdWACJZ+G5n9R25GWD1WFH1TvmJWoXP3X6S2s79dF2vd7qz7mdteQxtBAPpNDjqP38/PJ1Gtu3bx8//PADM2bMeODASrMutSuw4uBlQnycOX8jnZRMdTWcvWcT+G3nWQBSs3RcT83Cw8k2f0/wlsNh32/q2M398wu+idZOHScKgAYGHcpL1ClX4PxO9RG7BQ2lE/eg16uJ+dgKdQjS/t/zhh3ZOKorJAFcPwFtP1XbbHXZaqK9sPvm+GCNusjD7csixu2/832dyqmJGaDZB+r0mcGt8/ZXbqq+hBDFotCJWhRem1BvZr1Sn1r+blxPzWLc0lhWH77M2etp/LQ1b1GTfecSaBHiZXyyg5s6FWHT99TpD4+tBH02oIHMZLi0T+2MkztG1KkcVGqi1npy/dJZ/aL+8GJerSc9Qf2yvnZMrZ07uENQK3VfZpI6/jRunzr05dI+tZ0w6u+iH+JiCRRFHeqTna4Om9No1Clg9/5yczrIVnnHZiSpf/Tcuu1e1z7xr7oEYcpltUmjds87j6Pf+ytsn64+ck6JVxdvKEhuki5fV11nOFezwRD2LNi7qu+trKDVCDi6XG02qdxMrVVfiVV/Z1Kvqo+57V3hYozawzpiQN4THRc/eHWljPsX4iGSRG0GGo2G5tXUBOzmaMv3UfXoNG0z+84lGB2368z1/Ik67yLgX1993U6XDVePgr1b3sQPudJvqDMl2ZZRv/xtndQkNOspdfalO0cN3DI3TsTAvCSdkwnzekCFenkL2gNcPqR2BPKtZdzJJz1Bjd/GUU2IW76Cs9uh/qtQs1veced2qEPWKjVRx6jmZKo943My1UetiefU2aACm0HlJ9Rz9Ho4uRb0OeofGloTfsUTzqrj2//7HW7cXGyhfF212eDkOnW/qz9E71AfE189BjPbqffOTdSxSyB2qfoHVdx/6uppQS3Vz+70Jki7DskX8+65d7b6B1ajaPX93wPUP5hajVCH6NXspp63b87NfwYr8H8c/GpDtXbqZ5McB6c2qLNfVXzcuExeIerrVlXbqK/CkiQtxEMlidpCvNqkMv3n7jXaNm3tCVIzdbz+RCDl3RzueY1snZ7PVxyhTkV32j4WWvBBDu7wwUl1vGnuY+/TG9XEbueqJn69DpIuqlMqGihgbQ9lvCAgIi8xgpqcjq9Sr3Frop7fK+8a3mFqjS3pQl7t73Yaq7xEnZGkLoigtVPjBTWmvweoP68akffUYONEdQhb4oWb227+QVGxEfRennf97TPA3kWdv9nGXt02r4fa4/jCHvXJwe0u7M6bBxrUtcVtb/biT74EqVfU6SJ1OTf/KNCoQ+1uZfQ5on72NburCfrqMaj1Yt6++MPq/TKT846t/ZL6udi5qGOOrW2Nr+fiq67AJIQolUxK1F27dr3r/oSEhAeJ5Z4+/fRThg4dyoABA5g8eXKx3uth61jTFxd7a77beJJLCRmcvpaKXoFZW06z8dgVlg98Auub611rNBoURWHz8Wv4uNpTxUttw/5h0ylmbFCT2ulPO9z5Zrd/0Qe1gGGXAI1xm/WNM+pjcKeyapL0CCx49qXydaHLDHVcbC5djtpG7lZRrYnerbbuEwa1eqi1fEVRa2wXdqnDb6p3zItXl6UO6Uk8r3aUs7K5+dgfdQGE2107kfezXg9rx6qPjis+njefs3sl2Do177jKzdQOetWfVsf07v5JrcFXbqrWXnMfIQOUrQY9F6qPr3Nr7gGN1WaJLHVqXdwC1Mfcjp5qrdvaHsoG53Wwysky/vdo/Yn6+Ln8zc6HGo3MCS/EI86kRO3q6nrP/S+//PIDBXQnO3fu5Ntvvy2147RzH4fnPhKft+Ms83efZ/eZG5y4ksrQBfu5mpLJrtM3WNq/KZuOX+XDhfvRWmno2zyIQU9W5Z//8h6p6vUKVlYmPKLMbYO81a0LCeTO8lQQ94D8iw5orWHAPvXnlHg4tkpN9mWD1TZUp7JqG23KFfCrlf/+QS3hwwvG28pVgzc2qI/2L8aAR2U1Acb9p9Zs3QLUWq5bRfXx+oVdaoK2slJr8XWj4PRmtZaaq9aLaju+ZxCUr6fWTm/15Og7l9vZO/+4XQc3aPWR8bZGfe98jdv/aPKXhW2EEMYKtSjHw5aSkkKdOnX4+uuv+eSTT6hVq9Z916jNtShHUZm+7gQTlsfe87hvXqpL/7l7ydLdnK70w1Z4udgXd3hCCCEKwZTcVCLG5kRHR9OhQwciIyPNHcpD16V2+TvuC6/gSlQjtSb75uzdhiQNcClRbb+du+Msny2PJUenZ/yyw3y7/kSB1xJCCGGZLL4z2bx589izZw87d+68r+MzMzPJzMw0vE9OTi6u0B4KH1d7Pmwfwril+WvVYzo9htZKYzSkK1enaZt5oYE/c3eo0zBuP3Wd3WduAPBCw4q42KttzXq9wpYT16jp72rYJoQQwnJYdI363LlzDBgwgF9//RV7+/t7jDt+/HhcXV0Nrxo1ahRzlMWvzxNBnBrfnkXREbSoVo6navpyclx7wv3dCPVzoUfDigWel5ukAUOSBqg5aiWxcWoP55WHLvPSD9tpN3kjSRlqx6xsnZ5Fey9wOSmjwOvm3FJzF0IIUbwsuo160aJFdOnSBa02r6ORTqdDo9FgZWVFZmam0T7IX6O+cOECNWrUKLFt1PdDr1cI/NC0OdYjq3vxfVR9xi89zLc3e4oP71Cd15oG8u36E4xfFktgWSf+fa85ACsPxuFsb0Plsk60/nI97R7zZcKzpbNjnxBCFDdT2qgt+tF3q1at2L/feFjPK6+8QkhICIMHD86XpAHs7Oyws8ub/D8pqYCxsaXMrb27A8s6EeDpSMdwPwb9rva6bl6tHJU8nZi15bThuNWH45m74yynr6Uatk1de5ynw/1Ysl9dmvDk1VTGLz2MnbUVX/2rDn8aGBlMUkYOv+06x6fPhKGRyS+EEKJYWXSidnZ25rHHHjPa5uTkhKenZ77tj7oZPevy7YaTTHwunMpl1RnD5u04x47T1+lSuzydapXn2boVeGrKJsM5QxcY/xGUkJZNg3FrjLbl1rZznbySl9i3nrxGsJcz5ZwLWBVJCCFEkbDoNmpx/1qH+vDnW40NSRpg5iv1mfVKfTrWVOf5fqy8KyfGtTc6xlTrj14x/Pzid9tp9cU6riRn3uWMgi3ae4GNx67c+0Dg913naPPlBs5eu8OMZkIIUYpZdBt1USjp46iLw43ULFYcjGPIzRp12TK2zOvTiMhJ6wt9zbdbVqG8mwMbjl2hmrcLjwd64Gxvw64z19l28hpPh5enaXBZ7G207D17g2e/2QrA8bHtsNbe/e/FSkOWANCgsge/v9Go0DEKIYSlMCU3SaJ+xB24kIiDrZagcmWIjUui7eSNBR4XVt6V/RcSH+hevq72WGk0XEhIN2x7q3kQTrZarKw09GgYgKtD/iFiuYlao4FT4/OmRr2emkWOTo+Lgw1JGdl4OcsEL0KIkqHUdCYTxe+x8nnTwob4uPBjr3r0nrXL6JiDo9uw49R1XpmljmUP9irDsfgUk++VOwnLraavy5uA5bPlR5jzekMaB5Ut8HxFUYeO2Wit0OsVOk3bRHxSJpXLOhEbl8yG91tQ0dPR5LiEEMKSSaIWRsIruBl+rl/JnWEdauBkZ03zauVY2r8pwd5lSM/WUXPUynznOtpqScvSGW1zd7ThRlr2fd+/x/fbGd6hBisOxtGimhdvPBGIRqMmaYA1hy+z8/QNrLUazl1Xa+axceqkNr/vOoe9jfoYvV/LYFOKLYQQFksefYt8Tl9Nxd5Gi4/rnR8l5z6OzlW/kjvz32zMgQuJfPDHfxy6lMSX3cPpUrsC11OzsNFquJSYQVqWjs7TNhd3EVj1zhMEeztz9HIyqw5d5vWmgdhaS99JIYRlkEff4oFUuo9e4b+82oBtJ6/h5WzPtLXHGdslDFAfpc9743FiLyVTv5I7AB5O6gpRzjenKP3iuXA2Hb9K7YpuPBFcTl1DO8CdhLQspvxbwHKVhfD23L388VZjWn+5AYCMbB19m1fBRqth84lr1KnoZohHCCEsmdSohcU4ey2NJz5fC6g90a+mZD3Q9YLKOXHilnHfHWr6otcrLDsQR1SjAEZ3krH4QgjzKHWrZ4lHg7+Hg+HnymWdCCt/9/XPCxLu72b4+dYkDbDkv0ssOxCn/rz/EldTMvl2/QlSM3MKF7AQQjwEkqiFxdBoNIT4OAPQpXYFfuxV32i/jVadrrSgIVygDvWa3L0W627OT343V1OyqPfJasYvi2Xk4oMFHrPx2BXem79PErkQwqykjVpYlF9ebcjuMzdoE+qNRqNhUXQEvWbu4P021ahc1onRiw8x4dma6BWFv/ZeMFric3DbEAAURaGcs51hxrT6ldzZefpGgfcD+GP3eZ6rW4FlB+JoWNmDugHueLnY0/OHHQA421vTO6IyigKZOTrWH73CKxGV0VrJPOdCiOInbdTC4imKcsfFPz5bHsvX604wsmMNXomobNi+9+wNPl9xhNoV3Xi/TQgrD8bx8ZJDvNe6GgPmxdz1fiE+zizp35Sg21Yks9VakXVzic9xXcJ48ebyohnZOuxt1AVijscncykxg6bB5QpbXCHEI0B6fYtS5W4rdL3buhpd65QnqFwZo+21K7oz5/XHDe9bh/rQOtQHgJTMHL5eewJfV3tSs3QcvmS8wlpsXDIf/PFfvntl3bIO96FL6ixta2PjDRPBNAr0ZOvJawAsH9iUEB+XAmPefvIaIT4uuDpKr3MhxL1JohYlmtZKQxUvZ5PO6dEwgB4NAwzvZ24+xei/Dxkd8+ee83e9xuxtZ3F3tDWaVjU3SQNsP3k9X6LOzNHxy9YzfLLkMFW8yrAoOoIydtY3j7/G1LXHGds5TGZXE0IYkUQtHnmvRFQmqlElDlxMJEevsPLgZb5Zf8LomHLOdlT1LsPm43nJeMq/xylbpuAlPvdfSCRHp+fDhfv5NzaeT7vW5K99F/l730UAjsenMOTP/8jRKbzRLJDuM7YBMOafg3wfVb/AawohHk2SqIUArKw01Lw5fepjfq4kZ2RzISGdFxtUZPXhy7R9zIeWId4cvpREu//lLVxyNSUTO2sr6lVyN0rif+w+zx+782rl787fR2K68VSq//x3CYDlB+MM224fUgYQn5TB9PUneOnxgHyP+IUQpZ8kaiFuY2ttZZhpDTC0bQNU8sw/a1uHMF/sbbVGifp2tybpOa835N3f9xW4SMmpq6n0+XkX8cmZzOhZFy8Xewb+FsOWE9dYGxvPuvdbFLZYQogSSsZRC2ECB1stf75lvCZ2n2aBZOfkdTR7uVGA0f7AcnnJvUElDxoHlTVa/OR2Kw9dJuZcAl2+3sLEFUfYckL9A+D0tbQiKIEQoqSRRC2EieoGeDCsfXUAhrYLIcTHhdefCMRGq+GlxysyuG0I77Wuir2NFQGejvRpGmg4t1k1ddhWNZ/8HeB+iKpn9P5CQjpT19577vO9Z28wYN5eLiXmrfO96/R1+vy8i7jEDKasOUbzz9cSn5S/Bi+EsHzy6FuIQnitaWWerVsB95sLjlT1dmbPR09ib6PFRmtFv5bB9G5SGQ0akjOyKWNnTUUPR15too719nbJW5nMRqthygt1aFXdm6X9mzLo9xjD0p23W3sknkaBnmRm63Gy0/LMN1vZdy4BgKwcPdNfqsuGo1d4+Ud1spakjGy2nbwOwA+bTzG0XfXi+kiEEMVEJjwR4iFITM/GztrKMDFKUkY2Hb7aSINKnozuFGoYppXrwIVEtpy4yrilsXe8ZoiPs1FCL+/mQLNq5Ziz/WyBx/dqXIlRT4cWuE+vV5i69jhbTlxl0JPVaFDZw9QiCiFMIBOeCGFhbp+f3MXehg3vt7jjZC6PlXfF19X+ron69lr3hYT0OyZpUMdx3+rY5WRsra0I8HRi9vYzTFp1FIDXftrJzuGR2Flr71omIcTDIW3UQpjJ3WZcA/AsY0cVr4KHY1UqxKQo52+kM2rxQV7+cQdztp+lzeQNdPl6C5k5OubvyhtKlpSRw9G4FJOvL4QoHpKohbBgs19tyPKBTXG4+ch8SLsQFvZtbDQ96q0ql3ViUrfwAvdtPHaVWVtOs+HoFT5cuB+9AtdTs+g9ayf7LyRipYHqvupsavsvJHIxIZ2hC/7j9NX8Y7sfRGJaNmlZsiKZEPdLHn0LYcF8XO3xcbVnQd/GbD1xjajGlfKt2tXniUBmbDgJQO+ISnStUwE/NwfWxsbz7c3td5M7/ru6rwtNg8tx+FISHy7cb9i/dH8c+0a2BtSpTgf9vo/xXcN4oqrpC48kZ2QTPmYlvq72bB3ayuTzhXgUWXSNevz48dSvXx9nZ2e8vLzo3LkzR44cMXdYQjx01X1d6N3EeGnN8V3DaBzkSXSLKvzv+Vr0bxXMizfnMH880JOna/kBaq/yViFe+a75ZrMgo/cVPRwJK++a77jE9Gx+2aYuJ9p9xjYuJKTz2s+7OHo5OV+7973sO6fOjX4pMYPsWxY5EULcmUUn6vXr1xMdHc22bdtYtWoV2dnZtG7dmtTUon0UJ0RJ9EKDisx5/XFcHWzoVKs8g56sapTIQ/1cmfNaQ1YPasYPverzdY86ONmqj9AfD/TgnSeD8fdwMBzvf4dEDTBq8UGSMvJmV8vK0dP6yw0M/XN/gcffSfIt10i6bUpVIUTBLPrR9/Lly43ez5o1Cy8vL3bv3s0TTzxhpqiEKDkaVylr+Ll9mC/tw3yJT87AydYaO2stnz8bzvM3FwTxd3cwSty30ukVpq87kW/7gr0XGNc1DFutFVY3/0jQ6xXikjJYsOc83er543XLmPG4WyZdSUzPxvMOi5oIIfJYdKK+XWKi+tjMw0PGeApRWF7OeYmzctm86U29XewL7InetU55Fuy5UGCiBqg/djWPB3ry3cv1mLz6KFP+PY5Or07PkJyRw9D2eZOsXEzImz3t1vnPVx+6jLO9NQ0DPQtfMCFKqRKTqPV6PQMHDiQiIoLHHnvsjsdlZmaSmZlpeJ+cXPAMT0II8HLOq9GWd1dr03Nff5zvN56klr8bTauW4+jlZBbsuQCAvY0VWo2G1Ky8tunkjBxWHbrMG7/sYsXBy0bXP3I5/1jvXEkZas/v8zfSeO3nXQAc/aQdttZWpGXl8MJ326ng7sC0F+sUYYmFKHlKTKKOjo7mwIEDbNq06a7HjR8/ntGjRz+kqIQo2TQaDT9E1eNCQjqhfmr7dKMgTxoF5dVsQ/1cyMrRE1jOiYoejmw6dpUhC/K3TecmaQ8nW1qGePHH7vPcSM0iMS2bdUfj+Wb9SQ5fSjIcn1ujPn01b7GRI3HJhFVwZdHei+w7l8C+cwlMeCYHe2srrLUW3aVGiGJTIn7z+/Xrxz///MPatWvvOdXa0KFDSUxMNLwOHTr0kKIUomRqVd2blxtVuuN+G60VLz0eQOOgslRwd+S5ev70fDzgjse/0rgSvRqr1zt9LY3+8/YyYF6MUZIGOHklhePxKfxvzVHDtn3nEwDjNbrrfryKll+sN+rMJsSjxKITtaIo9OvXj4ULF/Lvv/9SuXLle55jZ2eHi4uL4eXsnH+VIiFE4WmtNHzcOa/5qYydNe+3qWZ4H+ztjK+r2g6emJ7N+qNXDPsql3WiY7g6bGzy6mNETlrPztM3DPun/HuM5Ixsdp66btiWmaPn7PU0Ft58/C7Eo8aiE3V0dDSzZ89mzpw5ODs7ExcXR1xcHOnp6fc+WQhRrN5qro7Dnty9FjVuzmgG6hKeHjdXFbvdiw0q4u18557el5MyGbc0lvTs/OOz/4opOFHHJ2eQmCa1bVF6WXQb9fTp0wFo3ry50faZM2fSq1evhx+QEMLg/dbVeLlRAL6uDlxJzuvAWdHD8Y7zmAd5ORn19r6Vq4MNienZLNpbcELeczaBP3ef588950nJzOGTzo+x7EAc364/QVC5Mqx854l7zp8uRElk0Ym6lK/AKUSJZmWlwddV7SleztmOpf2bYmdjZZh05cvu4bzz2z6jc6r7uhimLAVY915zdp6+zoELiVQq68Tovw8ZatOR1b1Zfdi4F/m78/Ou9/TUzYafj8WncOJK6h0XMRGiJLPoR99CiJKjhp8LQeXyEmWX2hUILJc3Tntxvwh8XR14tUllHivvwv+er0Wlsk48V8+f0Z0eI7CccZJ9rl4F3n2yKi721oZ27bvZcUu7thCliUXXqIUQJZv9LWta16zgBoCfmwP/vN0037EVPYyX7qzt70abUB/6tazCxmNX+XvfRQDahvqw6fhVUjKNV+D6cOF+EtKzeKtZEMfiU3j95130jqjMhqNXaB3qTff6FYu4dEI8HJKohRDFZkynULp9u5W+zavc89jybnnTl3o62RqmHtVoNIT65XVWC/Jy4v221Zi34yx+bg7cSMvmqzXHAPhs+RES0rLZeuIaZ66lMXLxQQDWxMZLohYlliRqIUSxqVfJg30jW1PG7t5fNbbWVgxtF8LGY1d56bZx2rfOCV7FqwxB5cowrEMNw7bYS0msPKS2Z8+4w9KeiqKg0Wi4lJhOdo5CRU/HAo8rSHJGNtZWVjjYau99sBBFTBK1EKJYOdvb3PexbzQL4o3blt/MNfvVhuw5e4NO4eXz7fuyey12nLrOtxtOsO1kwW3VCWnZ5OgVIr9Yb5gC9elwPz56qgZaKw2v/7yLugHufNCmmmEWtNTMHBbuvcCE5bEEe5VhQd+I+y6LEEVFErUQokRoElyWJsFlC9znZGdNixAvWoR4sePUdV76YTtZOcbrXR+/ksLny48YzVO+eN9FFt9s+wbYfeYGlTydeLGh+ph81OKDzN99HlCHh+0/n0hYhfxLga47Es+Yvw/Rv1Uwv+08R98WQTQNLvfAZRYCQKOU8jFQ58+fx9/fn3Pnzt1z+lEhROkQl5jBpcR0dpy6zqwtp7mUmHHvk27q1bgSb7eswn/nE3ll1s58+6t6lyG6RRXqV/LA0VaLm6MtlYcu4fZv0tOfdnjQYohSzJTcJDVqIUSp4+Nqj4+rPbUruvPbrnMmnXs9NYuhC/Yb2rxvd/RyCiP+OkhGtg4brRU/RNXLl6SFKEoyjloIUapFBBk/Ln/3yaqGnyd3r8XMV+rTJtTbsC0uKeOOSfrZumrNJzE9m8wcPSmZOczYcBKrAiZEUxSFtUfimbHhhEzeJB6I1KiFEKXa2y2r4Opgw7qj8dTydyO6RRUc7aypF+BOuL8bAC2qebH2SDyvzNyZb+KUcs52THgmjJYh3iiKwoqDcSRn5I3h3nXmBvoC8vD83ef54I//ABi3NJbhHarzWtPABy5Pbu918eiQRC2EKNW8XOx5r0013rtlha9Xm+Rfic/D0Xghke71/GkX5kNVb2f8bo7x1mg0BHuVYc/ZBMNxt85dXs7ZzjDveW6SzvXJksPUrOCGm6MNF26k0yLE665x7zuXwLkbaTxVM29WttTMHDpO2UTdAHc+fy78HiUXpYUkaiGEAKMVv1zsrRnSLgT3AlYBq+7rYpSoc0VU8WRmrwY8P2NrgfsBYs7dYNzSWMP7H3vVo2WId4HHdpqmzmXu6mBDUnoOyw/GGWZnO3k1lU+fqWmYVx1Af7Nab1XQc3hRokmiFkIIMErK4f5uBSZpUJf31OkV7G20nLiSwsZjVwnxcWbic+HqpC3tq7Pkv0u83bIKSw/E8dGiA4Zz996WwN/8ZQ8xI59kbewVnqzhja113vjtXD1/2FFgHBcT0vH3cCQlM4d3foth1aHL2FlbsXzgE1Quq86xPnfHWRbsOc/0l+pStsydlxcVlk0StRBCAE63zDrmc3P60oJUcHfk02dqAnDySgqbjl+lWz1/7G3U8+tX8qB+JQ8AvG5be3vZgTij91k6PX1+3s2m41fp1bgSp66mkp6lY8fpey8wsvP0dYYvOkBsXBKXk9TH7Zk5en7acppRT4eSlaNn6IL9AExcccQQsyh5JFELIQQYddCqfMuqX3cTWK5MvlW/bvVEcDlq+buRmJ7NqaupBR6z6fhVAGZtOX3/wQKDft9X4HatlYbrqVm0/GKdYdu8neco7+bAS48HkJKZw/4LibR7zIek9BzSsnMMy5UKyySJWgghbopuEcT6o1fyzTVeWA62WhZFR3AxIZ3Gn/5r2D7iqRrM3naGkwUkbz9Xey6aMEHL7S4mpLP60GUS0rKNtn+x6ijT159Ap1fIzNHzcqMAlvx3ibQsHWvebWboMCcsj4yjFkKIm95vE8I/bzfFxYT5ye+Hj4s9tfzdCCrnxC+vNuDlRgEMfLIqjwd68NkzNY0WLfmwQ3XDz17OdjSpUvC0qbfq+XgAuQ8Etp68xvC/DhR4XFqWjsybU6v+vPUM11KzSM/WseoO48aLwplrqRy4kFhs138USI1aCCGKmZWVhkXREej1iqFX9tPhfjwdrg69+mf/JTYcvQKAv7sj4RVc2Xc+kQ41fXnnyarEXkpm8uqjbDlxDYB+LapQxasMLUK8OHghkccDPenZKIDWX27IV5MGdQnRCwnpd4zvi5VH6BjuZ9Tz/VaKorDj1HUqejpy4UY6wxcd4KXHA+755EFRFFp9sZ4cvcK695pTqawT11IycXWwMSx8Iu5NErUQQjwkdxo6FVbexZCoK3o48n1UfZYfjKNL7fKUsbOmQWUPfurdgOBhywCoVNaJzrXVVcQa36xxV/RwxMFGS3q2zujawztUp1t9f575egvH4lMM251stYx6OpT3//iPpIwc6ny8ivJuDgyMDKZ5NS+WH4yjYWUPRv99kLjEDE5cScXW2sqw2MmYvw8B6rKjjwd6AmpiPhafgoONlhy9gpOt+l+AT5fF8nLjAHp8v52ejwcwMLIq83aepWvtCvi43rnznpBELYQQZhdWPm9FLjdHGzQaDT1vq63a3FIDLahXur2NlvlvNuKf/y6x6lAcTnbWTHmhNhXcHdFaaejzRCDv35yEZWTHGrQO9aG8mwPWWg3v/KZ2TLuQkG44piC3rkiWpdMz/ObQswBPR7rV82fFwTj+O68+5rbSYDTJzPKDcSw/qPZ6/3nrGaw0GmZtOc1ny4/weKAH016sg2cZO/R6BZ2iGJX3USerZwkhhJnp9QoTVx6hqrezoaZckOUH4oiNS2JAq2CTpxGNOZdA55uTqBz5pC121upwMkVRaDT+X+KSCt+BrSh0CPNl6ou1eWv2HjYcu8LgtiHUDXCnjJ01lcreXy98gMtJGdhZW+HmWPBjfEthSm6SRC2EEI+I33eeI7CcE/VujvPOFRuXxM5T16lf2YMydtZMX3eCX7efNTqmuq8LrzSuxKrDlxnWvjo7Tl2nVXUvNhy7YqiR382derPX8HXh0KUkQF305I+b63/fysvZjgBPR77tWc+oHT0tK4c/dp/nclIGbzWvwuxtZ5iwPJYK7g4s7BuBm4MNFxLSOXkltcApW2POJfDn7vN80LYazkXcgfBeJFHfQhK1EEKY7uetp5m5+TQ30rJwsbdh+kt1CPVzzXecXq8wdulhbK2t6Pl4AF2+3myYgAXUcd2/v9GIEB9nes3cgaeTHRk5OtYdUdvkY0Y8SadpmzlzLe2+4oqs7oW/hyMx5xI4dDHJ0Ivdxd6apFsWSwG1iSBLp+d6ahYfdwrFz80Ba60VlT2d6P3TTo7f0mbfq3ElKrg7sO3kdY7HJ1O5rBPtwnyp4euCnbUVlcs6FWkHuFKXqKdNm8bnn39OXFwc4eHhTJkyhQYNGtzXuZKohRDi4dl3LoH3/9hH/UoeJGfk0KmWH62qG89n/uOmU4z5R+2MdvrTDszZfpZPlx0msFwZ3mwWyPqjV7C30dKtnj9/7j7Pyaup/Bsbb47iGGkT6s2LDQN4IrjsA69gVqoS9W+//cbLL7/MN998Q8OGDZk8eTLz58/nyJEjeHndffUZkEQthBCWJitHz/hlh6kX4EGHmr73PP5qSibNPltLtk7h6Vp+LI65SM0Krkx4tiYBHo68OXs3qw+rifzlRgG0qu7NleRM3puf90heowFPJ1uupmTd9V5aK3WFtLPX00jL0uXbX87Zjs2DWxrmZS+sUpWoGzZsSP369Zk6dSoAer0ef39/3n77bYYMGXLP8yVRCyFEyXfuehq21lZ4u9iTkpmDg43WaPWwG6lZLNx7gS61yxsWVNl3LoGdp68TWd0bZ3trPMvYcTw+mV4zd3L+Rt64cmd7a9YMaoZnGTvDNTNzdGw8epVGQZ6sPnyZn7acxtHWmubVyhXJuuKlJlFnZWXh6OjIH3/8QefOnQ3bo6KiSEhI4K+//rrnNSRRCyGEuFVGto7rqVmcv5GOj4s9Ntaahz7fuSm5yaLHUV+9ehWdToe3t3H7hre3N7GxsQWek5mZSWZmXkeG5OTkYo1RCCFEyWJvo8XPzaHEzG9e6kaUjx8/HldXV8OrRo0a5g5JCCGEKDSLTtRly5ZFq9Vy+bLxhPGXL1/Gx8enwHOGDh1KYmKi4XXo0KGHEaoQQghRLCw6Udva2lK3bl3WrFlj2KbX61mzZg2NGjUq8Bw7OztcXFwML2dn54cVrhBCCFHkLLqNGmDQoEFERUVRr149GjRowOTJk0lNTeWVV14xd2hCCCFEsbP4RN29e3euXLnCiBEjiIuLo1atWixfvjxfBzMhhBCiNLL4RA3Qr18/+vXrV6hz9Xp1erlLly4VZUhCCCFEoeXmpNwcdTclIlE/iNyOaPc75agQQgjxsFy+fJmKFSve9RiLnvCkKOTk5LB37168vb2xsnqwvnPJycnUqFGDQ4cOSSe1+ySfmenkMzOdfGamk8/MdEX5men1ei5fvkzt2rWxtr57nbnUJ+qilJSUhKurK4mJibi4uJg7nBJBPjPTyWdmOvnMTCefmenM9ZlZ9PAsIYQQ4lEniVoIIYSwYJKoTWBnZ8fIkSOxs7MzdyglhnxmppPPzHTymZlOPjPTmeszkzZqIYQQwoJJjVoIIYSwYJKohRBCCAsmiVoIIYSwYJKoTTBt2jQqVaqEvb09DRs2ZMeOHeYOyWKNHz+e+vXr4+zsjJeXF507d+bIkSPmDqvE+PTTT9FoNAwcONDcoVi0Cxcu8NJLL+Hp6YmDgwNhYWHs2rXL3GFZLJ1Ox0cffUTlypVxcHAgKCiIjz/+GOmqZGzDhg107NgRPz8/NBoNixYtMtqvKAojRozA19cXBwcHIiMjOXbsWLHFI4n6Pv32228MGjSIkSNHsmfPHsLDw2nTpg3x8fHmDs0irV+/nujoaLZt28aqVavIzs6mdevWpKammjs0i7dz506+/fZbatasae5QLNqNGzeIiIjAxsaGZcuWcejQIb744gvc3d3NHZrFmjBhAtOnT2fq1KkcPnyYCRMm8NlnnzFlyhRzh2ZRUlNTCQ8PZ9q0aQXu/+yzz/jqq6/45ptv2L59O05OTrRp04aMjIziCUgR96VBgwZKdHS04b1Op1P8/PyU8ePHmzGqkiM+Pl4BlPXr15s7FIuWnJysBAcHK6tWrVKaNWumDBgwwNwhWazBgwcrTZo0MXcYJUqHDh2U3r17G23r2rWr0qNHDzNFZPkAZeHChYb3er1e8fHxUT7//HPDtoSEBMXOzk6ZO3duscQgNer7kJWVxe7du4mMjDRss7KyIjIykq1bt5oxspIjMTERAA8PDzNHYtmio6Pp0KGD0e+aKNjixYupV68ezz33HF5eXtSuXZvvvvvO3GFZtMaNG7NmzRqOHj0KwL59+9i0aRPt2rUzc2Qlx6lTp4iLizP6f9TV1ZWGDRsWWz4o9atnFYWrV6+i0+nyrYHt7e1NbGysmaIqOfR6PQMHDiQiIoLHHnvM3OFYrHnz5rFnzx527txp7lBKhJMnTzJ9+nQGDRrEhx9+yM6dO+nfvz+2trZERUWZOzyLNGTIEJKSkggJCUGr1aLT6Rg7diw9evQwd2glRlxcHECB+SB3X1GTRC2KXXR0NAcOHGDTpk3mDsVinTt3jgEDBrBq1Srs7e3NHU6JoNfrqVevHuPGjQOgdu3aHDhwgG+++UYS9R38/vvv/Prrr8yZM4fQ0FBiYmIYOHAgfn5+8plZMHn0fR/Kli2LVqs1rG2d6/Lly/j4+JgpqpKhX79+/PPPP6xdu5YKFSqYOxyLtXv3buLj46lTpw7W1tZYW1uzfv16vvrqK6ytrdHpdOYO0eL4+vpSo0YNo23Vq1fn7NmzZorI8r3//vsMGTKE559/nrCwMHr27Mk777zD+PHjzR1aiZH7nf8w84Ek6vtga2tL3bp1WbNmjWGbXq9nzZo1NGrUyIyRWS5FUejXrx8LFy7k33//pXLlyuYOyaK1atWK/fv3ExMTY3jVq1ePHj16EBMTg1arNXeIFiciIiLfkL+jR48SEBBgpogsX1paGlZWxl/7Wq0WvV5vpohKnsqVK+Pj42OUD5KSkti+fXux5QN59H2fBg0aRFRUFPXq1aNBgwZMnjyZ1NRUXnnlFXOHZpGio6OZM2cOf/31F87Ozoa2G1dXVxwcHMwcneVxdnbO137v5OSEp6entOvfwTvvvEPjxo0ZN24c3bp1Y8eOHcyYMYMZM2aYOzSL1bFjR8aOHUvFihUJDQ1l7969TJo0id69e5s7NIuSkpLC8ePHDe9PnTpFTEwMHh4eVKxYkYEDB/LJJ58QHBxM5cqV+eijj/Dz86Nz587FE1Cx9CUvpaZMmaJUrFhRsbW1VRo0aKBs27bN3CFZLKDA18yZM80dWokhw7Pu7e+//1Yee+wxxc7OTgkJCVFmzJhh7pAsWlJSkjJgwAClYsWKir29vRIYGKgMGzZMyczMNHdoFmXt2rUFfn9FRUUpiqIO0froo48Ub29vxc7OTmnVqpVy5MiRYotHVs8SQgghLJi0UQshhBAWTBK1EEIIYcEkUQshhBAWTBK1EEIIYcEkUQshhBAWTBK1EEIIYcEkUQshhBAWTBK1EEIIYcEkUQshipxGo2HRokXmDkOIUkEStRClTK9evdBoNPlebdu2NXdoQohCkEU5hCiF2rZty8yZM4222dnZmSkaIcSDkBq1EKWQnZ0dPj4+Ri93d3dAfSw9ffp02rVrh4ODA4GBgfzxxx9G5+/fv5+WLVvi4OCAp6cnffr0ISUlxeiYH3/8kdDQUOzs7PD19aVfv35G+69evUqXLl1wdHQkODiYxYsXG/bduHGDHj16UK5cORwcHAgODs73h4UQQiWJWohH0EcffcQzzzzDvn376NGjB88//zyHDx8GIDU1lTZt2uDu7s7OnTuZP38+q1evNkrE06dPJzo6mj59+rB//34WL15MlSpVjO4xevRounXrxn///Uf79u3p0aMH169fN9z/0KFDLFu2jMOHDzN9+nTKli378D4AIUqSYluXSwhhFlFRUYpWq1WcnJyMXmPHjlUURV2C9M033zQ6p2HDhspbb72lKIqizJgxQ3F3d1dSUlIM+5csWaJYWVkpcXFxiqIoip+fnzJs2LA7xgAow4cPN7xPSUlRAGXZsmWKoihKx44dlVdeeaVoCixEKSdt1EKUQi1atGD69OlG2zw8PAw/N2rUyGhfo0aNiImJAeDw4cOEh4fj5ORk2B8REYFer+fIkSNoNBouXrxIq1at7hpDzZo1DT87OTnh4uJCfHw8AG+99RbPPPMMe/bsoXXr1nTu3JnGjRsXqqxClHaSqIUohZycnPI9ii4qDg4O93WcjY2N0XuNRoNerwegXbt2nDlzhqVLl7Jq1SpatWpFdHQ0EydOLPJ4hSjppI1aiEfQtm3b8r2vXr06ANWrV2ffvn2kpqYa9m/evBkrKyuqVauGs7MzlSpVYs2aNQ8UQ7ly5YiKimL27NlMnjyZGTNmPND1hCitpEYtRCmUmZlJXFyc0TZra2tDh6358+dTr149mjRpwq+//sqOHTv44YcfAOjRowcjR44kKiqKUaNGceXKFd5++2169uyJt7c3AKNGjeLNN9/Ey8uLdu3akZyczObNm3n77bfvK74RI0ZQt25dQkNDyczM5J9//jH8oSCEMCaJWohSaPny5fj6+hptq1atGrGxsYDaI3vevHn07dsXX19f5s6dS40aNQBwdHRkxYoVDBgwgPr16+Po6MgzzzzDpEmTDNeKiooiIyODL7/8kvfee4+yZcvy7LPP3nd8tra2DB06lNOnT+Pg4EDTpk2ZN29eEZRciNJHoyiKYu4ghBAPj0ajYeHChXTu3NncoQgh7oO0UQshhBAWTBK1EEIIYcGkjVqIR4y0dglRskiNWgghhLBgkqiFEEIICyaJWgghhLBgkqiFEEIICyaJWgghhLBgkqiFEEIICyaJWgghhLBgkqiFEEIICyaJWgghhLBg/we6W6tILfb2kQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(\n",
    "    epochs_seen: torch.Tensor,\n",
    "    tokens_seen: list[int] | torch.Tensor,\n",
    "    train_losses: list[float],\n",
    "    val_losses: list[float],\n",
    "    output_path: str = \"loss-plot.pdf\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plota as perdas (treino e validação) em função das épocas e adiciona um segundo eixo\n",
    "    x para a quantidade de tokens vistos.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    epochs_seen : torch.Tensor\n",
    "        Valores no eixo x representando as épocas correspondentes às medições.\n",
    "    tokens_seen : list[int] | torch.Tensor\n",
    "        Quantidade acumulada de tokens processados nas medições.\n",
    "    train_losses : list[float]\n",
    "        Lista de perdas de treino.\n",
    "    val_losses : list[float]\n",
    "        Lista de perdas de validação.\n",
    "    output_path : str, default = \"loss-plot.pdf\"\n",
    "        Caminho do arquivo de saída para salvar o gráfico.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plota as losses de treino e validação em função das épocas\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(\n",
    "        MaxNLocator(integer=True)\n",
    "    )  # Mostra apenas inteiros no eixo x\n",
    "\n",
    "    # Cria um segundo eixo x para tokens vistos\n",
    "    ax2 = ax1.twiny()  # Cria um segundo eixo x compartilhando o mesmo eixo y\n",
    "    ax2.plot(\n",
    "        tokens_seen, train_losses, alpha=0\n",
    "    )  # Plot invisível apenas para alinhar os ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Ajusta o layout para caber bem\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e48e7-62f4-4dfb-a199-d0734f876a58",
   "metadata": {},
   "source": [
    "## Realiza a inferência\n",
    "\n",
    "O texto ainda parece aleatório, mas já se tornou mais coerente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fea9893-03da-46cc-8bc5-4234f527019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto de saída:\n",
      "Uma noite destas, vindo da cidade para o marque é justamente casa escura e o metal para que os que\n",
      "estás estréa, dizer, sem quinito. Levantou-me que se livrou. O gente, que\n",
      "faziam\n"
     ]
    }
   ],
   "source": [
    "# Define o dispositivo para inferência (CPU)\n",
    "inference_device = torch.device(\"cpu\")\n",
    "\n",
    "# Move o modelo para o dispositivo de inferência e coloca em modo de avaliação\n",
    "model.to(inference_device)\n",
    "model.eval()\n",
    "\n",
    "# Inicializa o tokenizer (encoding compatível com GPT-2)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Gera texto a partir de um prompt inicial\n",
    "prompt = \"Uma noite destas, vindo da cidade para o\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(prompt, tokenizer).to(inference_device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "\n",
    "# Decodifica e imprime o resultado\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(f\"Texto de saída:\\n{output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acd998-454f-42d6-a4fb-126464907e1e",
   "metadata": {},
   "source": [
    "### Mais uma inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "124958f7-85cb-49eb-9441-541adf05b077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto de saída:\n",
      "Quando voltámos, á noite, viemos por allição,\n",
      "olhos de tão que me acudiu estava assim lhe das inicia dos desvadores a lettra, ainda que me não engano passagem á\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "# Gera texto a partir de um prompt inicial\n",
    "prompt = \"Quando voltámos, á noite, viemos por alli\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(prompt, tokenizer).to(inference_device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "\n",
    "# Decodifica e imprime o resultado\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(f\"Texto de saída:\\n{output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24effdec-1112-4ec6-869e-3cd19ff92ed6",
   "metadata": {},
   "source": [
    "## Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3522cc5f-273e-4393-856e-4edf39de87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../models/my_gpt.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0dd5b6-5034-4fb9-be1c-f2f9a4454956",
   "metadata": {},
   "source": [
    "### Testando se o modelo abre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d003f19-ef82-4a0f-9ab1-8fbf991561a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = GPTModel(GPT_CONFIG_124M)\n",
    "device = get_device()\n",
    "print(\"Device:\", device)\n",
    "\n",
    "new_model.load_state_dict(\n",
    "    torch.load(\"../../models/my_gpt.pth\", map_location=device, weights_only=True)\n",
    ")\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb865dd2-93b4-4b24-ac26-eeb46328336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto de saída:\n",
      "Quando voltámos, á noite, viemos por allição,\n",
      "olhos de tão que me acudiu estava assim lhe das inicia dos desvadores a lettra, ainda que me não engano passagem á\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "# Gera texto a partir de um prompt inicial\n",
    "prompt = \"Quando voltámos, á noite, viemos por alli\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=new_model,\n",
    "    idx=text_to_token_ids(prompt, tokenizer).to(inference_device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "\n",
    "# Decodifica e imprime o resultado\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(f\"Texto de saída:\\n{output_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
